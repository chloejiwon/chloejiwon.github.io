I"¥2<blockquote>
  <p>ì´ë²ˆì—” 2019ë…„ë„ì— publishëœ ë…¼ë¬¸ BERTë¥¼ ì½ì–´ë³´ì•˜ë‹¤. ì•„ë¬´ë˜ë„ attention ì— ëŒ€í•œ ì§€ì‹ì´ ê±°ì˜ ì „ë¬´í•˜ë‹¤ë³´ë‹ˆ, í”¼ìƒì ìœ¼ë¡  ì´í•´í–ˆìœ¼ë‚˜ ê¹Šìˆ™í•œ ë©”ì»¤ë‹ˆì¦˜ì€ ì´í•´í•˜ì§€ ëª»í•œ ê²ƒ ê°™ë‹¤. (ë…¼ë¬¸ì´ ì‰½ê²Œ ì“°ì—¬ìˆì–´ì„œ ë§ì •ì´ì§€..) í•˜ì—¬íŠ¼, ë‹¤ìŒ ë…¼ë¬¸ìœ¼ë¡œ attention is all you needë¥¼ ì½ì–´ì„œ ë‹¤ì‹œ ì‹±í¬ë¥¼ ë§ì¶°ë´ì•¼ê² ë‹¤.</p>
</blockquote>

<h2 id="abstract">Abstract</h2>

<p>BERT : New language representation model, <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentation from <strong>T</strong>ransformers</p>

<p>ë‹¤ë¥¸ Language representation modelê³¼ëŠ” ë‹¬ë¦¬, BERTëŠ” ëª¨ë“  layerì—ì„œ unlabeled textë¥¼ ê°€ì§€ê³  left, right ì–‘ ì˜† contextë¥¼ ê³µë™ìœ¼ë¡œ ì¡°ì ˆí•˜ë©° deep bidirectional representationì„ pre-trainí•œë‹¤.</p>

<p>ë”°ë¼ì„œ, pre-trained BERT ëª¨ë¸ì€ ì¶”ê°€ë¡œ output layer í•˜ë‚˜ë§Œ ë”í•´ì„œ fine-tuneí•˜ë©´ ë‹¤ì–‘í•œ taskì— ë‹¤ state-of-the-art ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‹¤.</p>

<h2 id="introduction">Introduction</h2>

<p>Language model pre-trainingì€ ë§ì€ NLP taskì— íš¨ê³¼ì ì´ë‹¤.</p>

<p>pre-trained language representationì„ downstream task(ë³¸ ë¬¸ì œ)ì— ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ë‘ ê°€ì§€ê°€ ìˆë‹¤.</p>

<p><img src="/assets/images/ml-research-2-1.png" alt="ml-1" style="zoom:50%;" /></p>

<ul>
  <li>
    <p><strong>feature-based</strong></p>

    <p><code class="language-plaintext highlighter-rouge">ELMo</code> - pre-trained representationì„ additional featureë¡œ í¬í•¨ì‹œí‚¤ëŠ” task-specific architecture ì‚¬ìš©</p>
  </li>
  <li>
    <p><strong>fine-tuning</strong></p>

    <p><code class="language-plaintext highlighter-rouge">GPT</code>(Generative Pre-trained Transformer) - pre-trained parameterë¥¼ ì „ë¶€ fine-tuning ì‹œí‚´</p>
  </li>
</ul>

<p>ë‘ ë°©ë²• ëª¨ë‘ pre-trainingë•Œ ê°™ì€ objective functionì„ ê°€ì§€ê³  í•™ìŠµì‹œí‚¤ê³ , ë¬´ì—‡ë³´ë‹¤ <strong><code class="language-plaintext highlighter-rouge">unidirectional language model</code></strong> ì„ general language representationì„ í•™ìŠµí• ë•Œ ì‚¬ìš©í•¨.</p>

<p>â‡’ pre-trained representation ì‚¬ìš©ì˜ íš¨ê³¼(íŠ¹íˆ fine-tuning approachì—ì„œ)ë¥¼ ê°ì†Œì‹œí‚´. ëª¨ë¸ì´ unidirectionalì´ê¸° ë•Œë¬¸ì— pre-trainingí• ë•Œ ì‚¬ìš©ë  ëª¨ë¸ì˜ architectureì˜ ì„ íƒì˜ í­ì„ ì¢íŒë‹¤.</p>

<p>ex) GPT ì˜ ê²½ìš°, ëª¨ë“  tokenì´ ê·¸ ì „ token ì—ë§Œ ì˜í–¥ë°›ì„ ìˆ˜ ìˆëŠ” left-to-right architectureë¥¼ ì‚¬ìš©í–ˆë‹¤. question-answering task ì™€ ê°™ì´ ì–‘ ë°©í–¥ì—ì„œì˜ context ê°€ ì¤‘ìš”í•œ ë¬¸ì œì—ì„œ ì¢‹ì§€ ëª»í•˜ë‹¤.</p>

<p><strong>In this paper, fine-tuning based approachë¥¼ ì¢€ ë” ë°œì „ì‹œí‚¤ëŠ” BERTë¥¼ ì œì•ˆí•œë‹¤.</strong></p>

<p>unidirectionalí•˜ê¸° ë•Œë¬¸ì— ë°œìƒí•˜ëŠ” ì œí•œì„ <code class="language-plaintext highlighter-rouge">Masked Language Model (MLM)</code> pre-training objective ë¥¼ ì„¤ì •í•¨ìœ¼ë¡œì¨ ê·¹ë³µí•œë‹¤.</p>

<ul>
  <li>
    <p><strong>Masked Language Model</strong></p>

    <p>inputì—ì„œ ì–´ë–¤ tokenë“¤ì„ ëœë¤í•˜ê²Œ maskí•˜ê³ , objectiveëŠ” ì´ masked wordì—ì„œ ì˜¤ì§ contextë¡œë§Œ original vocabulary idë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤.</p>

    <p>â‡’ left-right modelê³¼ ë‹¬ë¦¬ left, right contextë¥¼ fuseí•  ìˆ˜ ìˆë„ë¡ í•´ì¤Œ</p>

    <p>â‡’ ë”°ë¼ì„œ, deep bidirectional Transformerë¥¼ pre-trainì‹œí‚¬ ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤€ë‹¤</p>
  </li>
</ul>

<p>MLM ì— ì¶”ê°€ë¡œ, next sentence prediction taskë¥¼ ì‚¬ìš©í•´ì„œ text-pair representationì„ pre-train ì‹œí‚¨ë‹¤.</p>

<h3 id="contributions">Contributions</h3>

<ul>
  <li>language representationì—ì„œ bidirectional pre-trainingì˜ ì¤‘ìš”ì„±ì„ ë³´ì—¬ì¤€ë‹¤</li>
  <li>pre-trained representationì´ ì—”ì§€ë‹ˆì–´ë§ ìˆ˜ê³ ê°€ ë§ì´ ë“¤ì–´ê°€ëŠ” task-specfic architectureì˜ í•„ìš”ë¥¼ ì¤„ì—¬ì¤€ë‹¤ëŠ” ê±¸ ë³´ì—¬ì¤€ë‹¤ (ì™œëƒë©´ BERTë¡œ Fine-tuningë§Œ í•˜ë©´ sentence-level &amp; token-level tasksì— SOTA ê°€ëŠ¥í•˜ë‹ˆê¹Œ)</li>
  <li>BERTëŠ” 11 NLP taskì—ì„œ S-O-T-A ê¸°ë¡</li>
</ul>

<h2 id="related-work">Related Work</h2>

<h3 id="1-unsupervised-feature-based-approaches">1. Unsupervised Feature-based Approaches</h3>

<p>Pre-trained word embeddings, sentence embeddings, paragraph embeddings</p>

<p><strong>ELMo</strong> - left-to-right ê³¼ right-to-left ëª¨ë¸ì—ì„œ <em>context-sensitive</em> featureë¥¼ ì¶”ì¶œí•œë‹¤. ê·¸ë˜ì„œ contextual representation of each tokenì€ left-to-right &amp; right-to-left representationì„ í•©ì¹œ ê²ƒ. (contextual word embedding + ê¸°ì¡´ì— ìˆë˜ task-specific architectureë¥¼ ì˜ í†µí•©í•œ ì•Œê³ ë¦¬ì¦˜) ì„±ëŠ¥ì´ ì¢‹ë‹¤.</p>

<h3 id="2-unsupervised-fine-tuning-approaches">2. Unsupervised Fine-tuning Approaches</h3>

<p>ì²˜ìŒì—” feature-based ì ‘ê·¼ë²•ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ unlabeled textë¡œë¶€í„° word embedding parameterë¥¼ pre-trainì‹œì¼°ë‹¤.</p>

<p>pre-trained from unlabeled text â€”&gt; fine-tuned for a supervised downstream task</p>

<p>ì´ ë°©ì‹ì˜ ì¢‹ì€ ì ì€ ë°”ë‹¥ë¶€í„° í•™ìŠµì‹œí‚¬ íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ê°€ ì–¼ë§ˆ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤.</p>

<p>OpenAI GPTëŠ” sentence-level taskì—ì„œ ë§ì´ state-of-the-art ê²°ê³¼ë¥¼ ëƒˆë‹¤.</p>

<h3 id="3-transfer-learning-from-supervised-data">3. Transfer Learning from Supervised Data</h3>

<p>large datasetì„ ê°€ì§€ê³  ìˆëŠ” supervised taskë¡œë¶€í„° íš¨ìœ¨ì ìœ¼ë¡œ transferí•˜ëŠ” ë°©ì‹ì„ ë³´ì—¬ì£¼ëŠ” ì—°êµ¬ë“¤ë„ ìˆë‹¤. (Ex, machine translation, natural language inference)</p>

<p>Computer visionì—ì„œë„ image netê°€ì§€ê³  pre-trainedëœ ëª¨ë¸ì„ fine-tuneí•´ì„œ ì‚¬ìš©í•˜ê¸°ë„ í•œë‹¤. (íš¨ìœ¨ì ì´ë‹ˆê¹Œ)</p>

<h2 id="bert">BERT</h2>

<ol>
  <li>pre-training 2) fine-tuning ì´ ë‘ê°€ì§€ ìŠ¤í…ì„ ê±°ì³ ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">pre-training</code> â‡’ ë‹¤ì–‘í•œ pre-training taskì˜ unlabeled dataë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">fine-tuning</code> â‡’ pre-trained parameterê°’ìœ¼ë¡œ ì´ˆê¸°í™”ì‹œí‚¤ê³ , downstream taskì˜ labeled dataë¥¼ ì‚¬ìš©í•´ì„œ fine-tuningí•œë‹¤.</li>
</ol>

<p>downstream task ë§ˆë‹¤ ë‹¤ë¥¸ fine-tuned modelì„ ê°€ì§„ë‹¤. ì—¬ê¸°ì„œëŠ” question-answering taskë¥¼ ê°€ì§€ê³  ì„¤ëª…í•  ê²ƒì´ë‹¤.</p>

<p>BERTì˜ íŠ¹ì§• ì¤‘ í•˜ë‚˜ëŠ” ë‹¤ì–‘í•œ taskì— í†µì¼ëœ í•˜ë‚˜ì˜ architectureë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒ</p>

<p>pre-trained architecture ê³¼ final downstream architectureê°„ì˜ ì°¨ì´ê°€ ê±°ì˜ ì—†ë‹¤</p>

<ul>
  <li>
    <p><strong>Model Architecture</strong></p>

    <p>multi-layer bidirectional Transformer encoder (decoderëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)</p>

    <p>layerìˆ˜(Transformer Block) : L</p>

    <p>hidden size : H</p>

    <p>self-attention head : A</p>
  </li>
  <li>
    <p><strong>Input/Output Representation</strong></p>

    <p><img src="/assets/images/ml-research-2-2.png" alt="ml-1" /></p>

    <p>BERTëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ ì„¸ ê°€ì§€ embedding ê°’ì„ í•©ì³ì„œ inputìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.</p>

    <p>inputì€ sentence í•˜ë‚˜ë‚˜, pair of sentenceê°€ ì˜¬ê±°ê³ , ì´ê±¸ one token sentenceë¡œ í‘œí˜„í•´ì„œ ì‚¬ìš©</p>

    <p>ì—¬ê¸°ì„œ <strong><em>sentence</em></strong>ëŠ” ì§„ì§œ ë¬¸ì¥ì´ ì•„ë‹ˆë¼ BERTì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” token sequence(í•œ ë¬¸ì¥ o ë‘ ë¬¸ì¥ì„ í•©ì³ë†“ì€ê±°)ë¥¼ ì˜ë¯¸í•œë‹¤</p>

    <p>WordPiece embedding with 30000 token vocabulary ì„ ì‚¬ìš©í–ˆë‹¤.</p>

    <p>ë¬¸ì¥ì˜ ì²«ë²ˆì§¸ tokenì€ ë¬´ì¡°ê±´ [CLS] (ë¬¸ì¥ì˜ ì‹œì‘ì„ ì•Œë¦¬ê¸° ìœ„í•´), ë‘ ë¬¸ì¥ì„ í•©ì³ë†“ì€ inputì—ì„œ ë‘ ë¬¸ì¥ì„ ë¬¸ë§¥ì ìœ¼ë¡œ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ì¤‘ê°„ì— [SEP] tokenì„ ë„£ê³ , í•™ìŠµëœ embeddingì„ ê° tokenì— ë”í•œë‹¤. (ê° tokenì´ sentence A ì— ì†í•˜ëŠ”ì§€, Bì— ì†í•˜ëŠ”ì§€ ì•Œë ¤ì£¼ëŠ”)</p>

    <p><img src="/assets/images/ml-research-2-3.png" alt="ml-1" /></p>
  </li>
</ul>

<h3 id="pre-training">Pre-training</h3>

<p>2ê°œì˜ unsupervised taskë¡œ pre-trainì‹œí‚¬ ê²ƒì´ë‹¤.</p>

<ul>
  <li>
    <p><strong>Task#1 : Masked LM(=Cloze task)</strong></p>

    <p><img src="/assets/images/ml-research-2-4.png" alt="ml-1" /></p>

    <p>input tokenì„ randomí•˜ê²Œ 15%(ì—¬ê¸°ì„œëŠ”)ë¥¼ maskí•˜ê³ , ì „ì²´ inputì„ ì˜ˆì¸¡í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ maskëœ tokenë§Œì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ trainí•œë‹¤.</p>

    <p>ë‹¨ì ì€ [MASK] tokenì´ fine-tuningë•ŒëŠ” ì—†ìœ¼ë‹ˆê¹Œ pre-trainingê³¼ fine-tuningê°„ì˜ mismatchë¥¼ ë°œìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì´ë‹¤.</p>

    <p>â‡’ ì´ê±¸ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ, â€œmaskedâ€ wordë¥¼ ê·¸ëƒ¥ [MASK] tokenìœ¼ë¡œ ë¬´ì¡°ê±´ êµì²´í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, training dataë¥¼ generateí•  ë•Œ random ìœ¼ë¡œ maskì‹œí‚¬ tokenë“¤ì´ ì„ íƒë˜ë©´,</p>

    <p>(i) 80% of timeë§Œ [MASK] Tokenìœ¼ë¡œ êµì²´í•œë‹¤</p>

    <p>(ii) 10%ëŠ” random tokenìœ¼ë¡œ ë°”ê¾¼ë‹¤</p>

    <p>(iii) 10%ëŠ” ê·¸ëƒ¥ ê·¸ëŒ€ë¡œ ë‘”ë‹¤.</p>
  </li>
  <li>
    <p><strong>Task#2 : Next Sentence Prediction (NSP)</strong></p>

    <p>Question Answering(QA) ë‚˜ Natural Language Inference(NLI)ê°™ì€ downstream taskëŠ” ë‘ sentenceì˜ ê´€ê³„ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì—ì„œ ì¶œë°œí•œë‹¤. (language modelingìœ¼ë¡œ ë°”ë¡œ ë³´ì´ì§€ëŠ” ì•ŠìŒ)</p>

    <p>ê´€ê³„ë¥¼ ì•Œê¸° ìœ„í•´ì„œ,  monolingual corpusì—ì„œ ìƒì„±ëœ <em>binarized</em> next sentence prediction taskë¡œ pre-trainí•œë‹¤.</p>

    <p>ì¢€ ë” ìì„¸íˆ ì–´ë–»ê²Œ pre-trainí•˜ëƒë©´, sentence A, Bê°€ ìˆì„ë•Œ (i) 50%ëŠ” Bê°€ Aë‹¤ìŒ ì˜¨ë‹¤(labeled as IsNext). (ii) 50%ëŠ” random sentence(labeled as NotNext)</p>

    <p>Prior workë‘ ë‹¤ë¥¸ ì ì€ prior workëŠ” sentence embeddingë§Œ downstream taskì— transferë˜ëŠ”ë°, BERTëŠ” ëª¨ë“  parameterë¥¼ ì „ë¶€ transferí•œë‹¤.</p>
  </li>
  <li>
    <p><strong>pre-training data</strong></p>

    <p>BooksCorpus(800M words) &amp; English Wikipedia (2500M words) ì‚¬ìš©</p>
  </li>
</ul>

<h3 id="fine-tuning">Fine-tuning</h3>

<p>ë³´í†µ text pairì— ëŒ€í•œ ëª¨ë¸ì„ fine-tuningí•˜ëŠ” ë°©ì‹ì€ 1) ë…ë¦½ì ìœ¼ë¡œ text pairë¥¼ encodingí•˜ê³  2) bidirectional cross attentionì— ì ìš©í•˜ë‹¤.</p>

<p>í•˜ì§€ë§Œ BERTëŠ” ì´ ë‘ ë‹¨ê³„ë¥¼ self-attention mechanismì„ í†µí•´ í•˜ë‚˜ë¡œ í•©ì³¤ë‹¤.</p>

<p>pre-trainingì— ë¹„í•´ êµ‰ì¥íˆ ì €ë ´í•˜ë‹¤. single Cloud TPU ì‚¬ìš©í•´ì„œ ìµœëŒ€ 1ì‹œê°„, GPUë¡œëŠ” ëª‡ì‹œê°„ ì •ë„ë§Œ í•™ìŠµí•˜ë©´ Fine-tuningëœë‹¤.</p>

<h2 id="ablation-studies">Ablation Studies</h2>

<h3 id="effect-of-pre-training-tasks">Effect of Pre-training Tasks</h3>

<p><img src="/assets/images/ml-research-2-5.png" alt="ml-1" /></p>

<ul>
  <li>No NSP â‡’ NSPì˜ ì¤‘ìš”ì„±</li>
  <li>LTR(MLMëŒ€ì‹ ì— Left-to-Right LM, left-context-only model) &amp; No NSP â‡’ Bidirectional Representationì˜ ì¤‘ìš”ì„±</li>
</ul>

<p>it would be possible to train ê° LTR, RTL model and each tokenì„ ì € ë‘ ëª¨ë¸ì˜ concatenationìœ¼ë¡œ ì‚¬ìš© (ELMoì²˜ëŸ¼)</p>

<p>ê·¼ë°, ì´ë ‡ê²Œ í•˜ë©´ <strong>(a)</strong> single bidirectional-modelë³´ë‹¤ ë‘ë°° ë” ë¹„ì‹¸ê³ , <strong>(b)</strong> QAê°™ì€ taskì— non-intuitiveí•˜ë‹¤. (RTL modelì€ questionì— ë‹µì„ ëª»í•˜ë‹ˆê¹Œ) ê·¸ë¦¬ê³  <strong>(c)</strong> ëª¨ë“  layerì—ì„œ left, right contextë°–ì— ì‚¬ìš© ëª»í•˜ë‹ˆê¹Œ deep bidirectional modelë³´ë‹¤ ë³„ë¡œ ì¢‹ì§€ ì•Šë‹¤.</p>

<h3 id="effect-of-model-size">Effect of Model Size</h3>

<p>fine-tuning task accuracyë¥¼ í†µí•´ì„œ model sizeì˜ ì˜í–¥ì„ ì¡°ì‚¬í–ˆë‹¤.</p>

<p>BERT modelì˜ ë ˆì´ì–´ ìˆ˜, hidden units, attention headsë¥¼ ë‹¤ë¥´ê²Œ í•´ì„œ ì‹¤í—˜í–ˆë‹¤. (ë‹¤ë¥¸ hyperparameterí•˜ê³  training procedureëŠ” ë˜‘ê°™ì´ ë‘ê³ )</p>

<p><img src="/assets/images/ml-research-2-6.png" alt="ml-1" /></p>

<ul>
  <li>larger modelì´ accuracy ê°€ ì›”ë“±íˆ ì¢‹ì•˜ë‹¤.</li>
  <li>BERT(base)ëŠ” 110M parameter, BERT(large) ëŠ” 340M parameter</li>
  <li>LM perlexity*ë¥¼ ë¹„êµí•´ë³´ë©´ ëª¨ë¸ ì‚¬ì´ì¦ˆê°€ í¬ë©´ í´ ìˆ˜ë¡ large-scale task(machine translation, language modeling)ì— ì¢‹ë‹¤. ê·¼ë° small taskë„ ëª¨ë¸ ì‚¬ì´ì¦ˆê°€ í´ìˆ˜ë¡ í¬ê²Œ ì„±ëŠ¥ì´ ì¢‹ì•„ì§„ë‹¤.</li>
</ul>

<p><em>*perplexity</em> : ì–¸ì–´ ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë‚´ë¶€ í‰ê°€ ì§€í‘œ. ë‚®ì„ ìˆ˜ë¡ ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ë‹¤ëŠ”ê±¸ ë§í•œë‹¤.</p>

<h3 id="feature-based-approach-with-bert">Feature-based Approach with Bert</h3>

<p>ì§€ê¸ˆê¹Œì§€ ëª¨ë“  ê²°ê³¼ëŠ” fine-tuning approachë¥¼ ì‚¬ìš©í•´ì„œ ì‹¤í—˜ëœ ê²°ê³¼ì´ë‹¤. (simple classification layerê°€ pre-trained modelì— ë”í•´ì§€ê³ , ëª¨ë“  pre-trained model parameterê°€ ì ìš©ë˜ì–´ì„œ downstream taskì— ëŒ€í•´ fine-tune)</p>

<p>Feature-based approach(pre-trained modelì—ì„œ fixed featureê°€ ì¶”ì¶œë˜ëŠ”)ë„ ì¥ì ì´ ìˆë‹¤.</p>

<p>(1) ëª¨ë“  taskê°€ Transformer encoder êµ¬ì¡°ë¡œ ì˜ í‘œí˜„ë˜ëŠ”ê²Œ ì•„ë‹ˆë¼ì„œ, task-specific architectureê°€ í•„ìš”í•  ìˆ˜ ìˆë‹¤.</p>

<p>(2) ë¹„ì‹¼ training dataì˜ representationì„ ê³„ì‚°ì„ ë¨¼ì € í•˜ê³ , ì´ representationì„ ì‚¬ìš©í•œ cheaper modelì—ì„œ ì—¬ëŸ¬ ì‹¤í—˜ì„ í•˜ëŠ”ê²Œ computational costì…ì¥ì—ì„œ í›¨ì”¬ ì´ë“ì´ë‹¤.</p>

<p><img src="/assets/images/ml-research-2-7.png" alt="ml-1" /></p>

<p>ê²°ê³¼ì ìœ¼ë¡œ, BERTê°€ fine tuningê³¼ feature based approach ëª¨ë‘ì—ì„œ ì„±ëŠ¥ì´ ì¢‹ë‹¤.</p>

<h2 id="conclusion">Conclusion</h2>

<p>ìš°ë¦¬ ì—°êµ¬ì˜ ê°€ì¥ í° ì„±ì·¨ëŠ” NLP taskì— ì˜ ì ìš©ë  ìˆ˜ ìˆëŠ” pre-trained modelë¥¼ deep bidirectional architectureë¡œ ì´ë£¨ì—ˆë‹¤ëŠ” ê²ƒì— ìˆë‹¤.</p>
:ET