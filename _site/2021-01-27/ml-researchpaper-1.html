<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations | Tale</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations" />
<meta name="author" content="Chester How" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="ë¬´ì‹œë¬´ì‹œí•œ ML ë…¼ë¬¸ ì½ê¸°" />
<meta property="og:description" content="ë¬´ì‹œë¬´ì‹œí•œ ML ë…¼ë¬¸ ì½ê¸°" />
<link rel="canonical" href="https://chesterhow.github.io/tale/2021-01-27/ml-researchpaper-1" />
<meta property="og:url" content="https://chesterhow.github.io/tale/2021-01-27/ml-researchpaper-1" />
<meta property="og:site_name" content="Tale" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-27T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations" />
<script type="application/ld+json">
{"url":"https://chesterhow.github.io/tale/2021-01-27/ml-researchpaper-1","mainEntityOfPage":{"@type":"WebPage","@id":"https://chesterhow.github.io/tale/2021-01-27/ml-researchpaper-1"},"author":{"@type":"Person","name":"Chester How"},"description":"ë¬´ì‹œë¬´ì‹œí•œ ML ë…¼ë¬¸ ì½ê¸°","@type":"BlogPosting","headline":"Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations","dateModified":"2021-01-28T07:42:00+09:00","datePublished":"2021-01-27T00:00:00+09:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/tale/assets/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">

  <!-- Favicon -->
  <link rel="icon" type="image/png" sizes="32x32" href="/tale/assets/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/tale/assets/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/tale/assets/apple-touch-icon.png">

  <!-- RSS -->
  <link type="application/atom+xml" rel="alternate" href="https://chesterhow.github.io/tale/feed.xml" title="Tale" />

  <!-- Google Analytics-->
  
</head>


  <body>

    <nav class="nav">
  <div class="nav-container">
    <a href="/tale/">
      <h2 class="nav-title">Tale</h2>
    </a>
    <ul>
      <li><a href="/tale/">Posts</a></li>
      <li><a href="/tale/tags">Tags</a></li>
      <li><a href="/tale/about">About</a></li>
    </ul>
  </div>
</nav>


    <main>
      <div class="post">
  <div class="post-info">
    <span>Written by</span>
    
        Chester How
    

    
      <br>
      <span>on&nbsp;</span><time datetime="2021-01-27 00:00:00 +0900">January 27, 2021</time>
    
  </div>

  <h1 class="post-title">Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations</h1>
  <div class="post-line"></div>

  <blockquote>
  <p>2019ë…„ Top 10 ë…¼ë¬¸ì— ë½‘íŒ Common Assumptions in the unsupervised learning of disentangled representations ë…¼ë¬¸ì„ ì‚´í´ë³´ì. ì´ë²ˆ ë…¼ë¬¸ì€ ì œëª© ê·¸ëŒ€ë¡œ Common Assumptionì— ëŒ€í•´ ë„ì „í•˜ëŠ” ë…¼ë¬¸ì´ë‹¤. ê·¸ë˜ì„œ ì´ê±¸ ì´í•´í•˜ë ¤ë©´ ì• ì´ˆì— ê·¸ëŸ¼ ë‚´ê°€ Common Assumptionì´ ìˆì„ ì •ë„ë¡œ ë°°ê²½ ì§€ì‹ì´ ìˆì–´ì•¼ í–ˆê¸° ë•Œë¬¸ì—(â€¦) ë°°ê²½ ì§€ì‹ì„ (ë…¼ë¬¸ì„ ì´í•´í•  ë§Œí¼ë§Œ) ê°„ë‹¨íˆ ì •ë¦¬í•´ë³´ì•˜ë‹¤. ì—¬ê¸°ì„œ ğŸ¥ ë¡œ ì¤‘ê°„ ì¤‘ê°„  ë‚´ ì˜ê²¬, ì°¸ê²¬ë“¤ì„ ì ì–´ë‘ì—ˆë‹¤. ì‹œë„ëŸ¬ìš°ë©´ ì‚´í¬ì‹œ ë¬´ì‹œí•´ì£¼ì„¸ìš”.</p>
</blockquote>

<h1 id="ë°°ê²½-ì§€ì‹">ë°°ê²½ ì§€ì‹</h1>

<h3 id="representation-learning">representation learning</h3>

<p>representationì€ ì–´ë–¤ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ë‹¤ë¥¸ ì°¨ì›ì˜ ë°ì´í„° í¬ì¸íŠ¸ë¡œ ë°”ê¾¸ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.</p>

<p>(x1, x2 ..)  â†’ (z1, z2, â€¦)</p>

<p>ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë‹¤ë¥¸ ì°¨ì›ì— <strong>í‘œí˜„</strong> í•œë‹¤ëŠ” ëœ»ìœ¼ë¡œ ë°›ì•„ë“¤ì´ë©´ ëœë‹¤. ê·¸ë¦¬ê³  <code class="language-plaintext highlighter-rouge">learning</code>ì´ë‹ˆê¹Œ ê·¸ í‘œí˜„í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë‹¤ë¥¸ ë°ì´í„° í¬ì¸íŠ¸ë¡œ mappingí•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµí•œë‹¤ê³  ë³´ë©´ ë˜ê² ë‹¤.</p>

<p><img src="../assets/images/ml-research-1.png" alt="MLë„í‘œ" style="zoom:40%;" /></p>

<h3 id="disentanglement">Disentanglement</h3>

<p>Transforming from an uninterpretable space with entagled features to eigen spaces where features are independent.</p>

<p><em>í–‰ë ¬ì˜ ê³ ìœ  ë²¡í„°(eigen vector)ë“¤ì´ í˜•ì„±í•˜ëŠ” ë¶€ë¶„ ê³µê°„ = ê³ ìœ  ê³µê°„(eigenspace)</em></p>

<p>ì¡°ê¸ˆ ë” í’€ì–´ ì„¤ëª…í•˜ìë©´, ì•„ë˜ ê·¸ë¦¼ì—ì„œ ë³´ë“¯ì´ xë¥¼ ë§Œë“¤ë•Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë…ë¦½ì ì¸ ìš”ì¸ì¸ z1, z2, â€¦. ê°€ ìˆë‹¤ê³  ì¹˜ì. ê·¸ë˜ì„œ xê°€ ì£¼ì–´ì¡Œì„ë•Œ ê·¸ì˜ í‘œí˜„ì¸ r(x)ë¥¼ ë§Œë“¤ë•Œ (â† r(x)ë§Œë“¤ë•Œ zë“¤ì´ ì˜í–¥ì„ ë¼ì¹˜ëŠ”ê±´ë°) í•œ ìš”ì†Œê°€ ë°”ë€Œë©´ ë”± í•œ representation vectorë§Œ ë°”ë€ŒëŠ” ê±¸ ì˜ë¯¸í•œë‹¤.</p>

<p>ë‹¤ë¥¸ representation vectorê¹Œì§€ ë°”ê¿”ë²„ë¦¬ë©´, zê°€ entagledë˜ì–´ ìˆë‹¤ëŠ” ê±¸ ì˜ë¯¸í•œë‹¤.</p>

<p><img src="../assets/images/ml-research-2.png" alt="disentanglement" style="zoom:50%;" /></p>

<h3 id="disentagled-representation">Disentagled Representation</h3>

<p>ì–´ë–¤ data(ì´ë¯¸ì§€)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” latent variableì´ ì—¬ëŸ¬ ê°œë¡œ ë¶„ë¦¬ ë˜ì–´ ê°ê° ë‹¤ë¥¸ ë°ì´í„°(ì´ë¯¸ì§€)ì˜ íŠ¹ì„±ì— ê´€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” ê²ƒì„ ì˜ë¯¸</p>

<h3 id="latent-variable">latent variable</h3>

<p>that describes original data</p>

<p>ì ì¬ ë³€ìˆ˜. Factor Analysis ë¥¼ í•  ë•Œ ë§ì€ ìˆ˜ì˜ ë³€ìˆ˜ë“¤ì„ ì†Œìˆ˜ì˜ ëª‡ ê°œì˜ ì ì¬ëœ ë³€ìˆ˜ë¡œ ì°¾ì•„ë‚´ëŠ”ë°, ê·¸ ë³€ìˆ˜ê°€ latent variable.</p>

<h3 id="autoencoder">Autoencoder</h3>

<p>for (Nonlinear) Dimensionality Reduction, Feature extraction, Representation learning</p>

<p><img src="../assets/images/ml-research-3.png" alt="autoencoder" style="zoom:50%;" /></p>

<h3 id="vae-variational-auto-encoder">VAE (variational auto encoder)</h3>

<p>ìƒì„±ëª¨ë¸ì„ ë‹¤ë£¨ëŠ” ê¸°ìˆ  ì¤‘ í•˜ë‚˜. GANê³¼ ê°™ì´ ê°ê´‘ë°›ê³  ìˆìŒ.</p>

<p><img src="../assets/images/ml-research-4.png" alt="vae" style="zoom:50%;" /></p>

<h3 id="downstream-task">downstream task</h3>

<p>ì‹¤ì œë¡œ, êµ¬ì²´ì ìœ¼ë¡œ í’€ê³  ì‹¶ì€ ë¬¸ì œ(task)</p>

<p>ìš”ìƒˆëŠ” ëŒ€ë¶€ë¶„ pretrain í•˜ê³  ê·¸ ëª¨ë¸ì— ìš°ë¦¬ê°€ í’€ê³  ì‹¶ì–´í•˜ëŠ” downstream task ë¥¼ ë„£ì–´ì„œ fine tuningì„ í•œë‹¤.</p>

<hr />

<h1 id="abstract">Abstract</h1>

<p>unsupervised learning of disentagled representationì˜ key idea :  real-worldì˜ ë°ì´í„°ê°€ ì†Œìˆ˜ì˜ explanatory factor(unsupervised learning ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë§Œë“¤ì–´ì§€ëŠ”)ë¡œ ìƒì„±ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ.</p>

<p>ì—¬ê¸°ì„œ ë„˜ì–´ê°€ê¸° ì „, priorê³¼ posteriorì˜ ê°œë…ì— ëŒ€í•´ ì§šê³  ë„˜ì–´ê°€ì.</p>

<ul>
  <li><strong>prior</strong> : z</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>posterior(=likelihood)</strong> : P(x</td>
          <td>z)</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p><strong>prior</strong>ëŠ” ì–´ë–¤ ì¤‘ìš”í•œ factor(autoencoder ì…ì¥ì—ì„œ ë³´ë©´ latent variable), <strong>posterior</strong>ëŠ” prior(=z)ë“¤ì´ ì£¼ì–´ì¡Œì„ë•Œ, xë¥¼ zì˜ ê²°í•©ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ë‹¤.</p>

<p>in this paper, ì´ ë¶„ì•¼ì˜ ìµœê·¼ ì§„í–‰ ìƒí™©ì„ ì‚´í´ë³´ê³ , ì¼ë°˜ì ì¸ ê°€ì •ì— challengeí•˜ê² ë‹¤.</p>

<h1 id="introduction">Introduction</h1>

<p>In representation learning, real-world observations xëŠ” ì´ë ‡ê²Œ 2-stepìœ¼ë¡œ ìƒì„±ëœë‹¤ê³  ìƒê°í•œë‹¤.</p>

<ol>
  <li>multivariate latent random variable zê°€ P(z) ë¶„í¬ë¡œë¶€í„° ìƒ˜í”Œëœë‹¤.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>P(x</td>
          <td>z)ë¡œë¶€í„° xê°€ ìƒ˜í”Œëœë‹¤.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ol>

<p>ê·¸ë‹ˆê¹Œ, real-worldì—ì„œëŠ” ìš°ë¦¬ê°€ ëª¨ë¥´ëŠ” ìš”ì¸ì¸ zê°€ ì´ë¯¸ ìˆëŠ”ë°, ê·¸ zë¡œë¶€í„° x(ì‹¤ì„¸ê³„ì— ì¡´ì¬í•˜ëŠ” ê³ ì°¨ì›ì˜ ë°ì´í„°)ë¥¼ generateí•  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•˜ëŠ” ê²ƒì´ë‹¤.</p>

<p>â‡’ ì¦‰, ê³ ì°¨ì›ì˜ ë°ì´í„° xê°€ ì €ì°¨ì›ì˜ ë°ì´í„° z(semantically meaningful latent variable)ë¡œ í‘œí˜„ì´ ëœë‹¤.</p>

<p>ê·¸ëŸ¬ë©´, ì—¬ê¸°ì„œ ëª©í‘œì™€ ì™œë¥¼ í•œë²ˆ ë” ì§šì–´ë³´ì.</p>

<ul>
  <li><strong>Representation learningì˜ ëª©í‘œ</strong> : useful transformation r(x)ë¥¼ ì°¾ëŠ” ê²ƒ. ì™œ? Classifierë‚˜ ë‹¤ë¥¸ ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“¤ ë•Œ, ì–´ë–¤ xì— ëŒ€í•´ ìœ ìš©í•œ informationì„ ì˜ ì¶”ì¶œí•˜ê¸° ìœ„í•´ì„œ.</li>
  <li><strong>ê·¸ë¦¬ê³  disentaglementê°€ ì—¬ê¸°ì„œ ì™œ ì¤‘ìš”í•˜ëƒ?</strong> dataì—ì„œ informativeí•˜ê³  êµ¬ë¶„ë˜ëŠ” factorë¥¼ ë‚˜ëˆ ì•¼ ì¢€ ë” ì˜ ìƒì„±ë  ìˆ˜ ìˆì–´ ë³´ì¸ë‹¤.</li>
</ul>

<h3 id="ì´-ë…¼ë¬¸ì—ì„œ-ë§í•˜ê³ ì-í•˜ëŠ”-ì£¼ì¥ë“¤ì˜-summary-ì‚¬ì‹¤-ì´ê²Œ-ì „ë¶€ë‹¤">ì´ ë…¼ë¬¸ì—ì„œ ë§í•˜ê³ ì í•˜ëŠ” ì£¼ì¥ë“¤ì˜ summary (ì‚¬ì‹¤ ì´ê²Œ ì „ë¶€ë‹¤â€¦)</h3>

<ul>
  <li>
    <p>Unsupervised learning of disentangled representationì€ learning approach(ëª¨ë¸,..)ì™€ datasetì—  inductive biasì—†ì´ëŠ” ë¶ˆê°€ëŠ¥í•˜ë‹¤. (ì´ê±¸ ì´ë¡ ì ìœ¼ë¡œ ì¦ëª…í•  ê²ƒì´ë‹¤.)</p>
  </li>
  <li>
    <p>ê·¸ë˜ì„œ current approachë“¤ê³¼ inductive biasì— ëŒ€í•´ ì¡°ì‚¬í•˜ê³  ì‹¤í—˜í–ˆë‹¤.</p>
  </li>
  <li>
    <p>ìš°ë¦¬ê°€ í•œ disentaglement_lib ë„ ë°°í¬í•˜ê³ , trained modelë„ ì—¬ëŸ¬ê°œ ë°°í¬í–ˆë‹¤.</p>
  </li>
  <li>
    <p>ìš°ë¦¬ëŠ” ì´ë ‡ê²Œ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¶„ì„í–ˆë‹¤.</p>

    <p>(i) ëª¨ë“  methodê°€</p>

    <p>(ii) random seedì™€ hyperparameterê°€ model choiceë³´ë‹¤ ë” ì¤‘ìš”í•˜ë‹¤. ì¦‰, reliableí•œ modelì´ ì—†ë‹¤.</p>

    <p>(iii) disentaglementê°€ downstream tasksì— ìœ ìš©í•œì§€ ê²€ì¦í•  ê¸¸ì´ ì—†ë‹¤.</p>
  </li>
  <li>
    <p>ê·¸ë˜ì„œ further researchì—ê²ŒëŠ” followingsê°€ ìˆìŒ ì¢‹ê² ë‹¤.</p>

    <p>(i) inductive biasì™€ supervisionì˜ ì—­í• ì´ ëª…í™•í•´ì•¼ í•œë‹¤.: model selectionì´ Key questionìœ¼ë¡œ ë‚¨ê² ì§€.</p>

    <p>(ii) distenglementì˜ ê°œë…ì´ ì‹¤ì§ˆì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì´ë“ì„ ê°€ì ¸ì˜¤ëŠ”ì§€ë¥¼ ì¦ëª…ë˜ì–´ì•¼ í•œë‹¤.</p>

    <p>(iii) ì‹¤í—˜ë“¤ì€ ëª¨ë‘ ì¬ìƒì‚°í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì—ì„œ ì‹œí–‰ë˜ì–´ì•¼í•œë‹¤.</p>
  </li>
</ul>

<h1 id="impossibility-result">Impossibility result</h1>

<p>ì–´ë–¤ generative modelë„ unsupervised disentanglement learningì´ ê°€ëŠ¥í•œê°€? ë¥¼ ì¦ëª…í•´ë³´ì.</p>

<p>â‡’ ë‹µì€ Noì´ê³  ì•„ë˜ theoremì„ ì´ìš©í•  ê²ƒì´ë‹¤.</p>

<p><img src="../assets/images/ml-research-5.png" alt="Theorem" style="zoom:70%;" /></p>

<table>
  <tbody>
    <tr>
      <td>(ì¦ëª…) P(x</td>
      <td>z) ê°€ generative modelì´ë¼ê³  í•˜ì. ê·¸ë¦¬ê³  zì— ëŒ€í•´ ì™„ë²½í•˜ê²Œ disentagledëœ representation r(x)ë¥¼ ì°¾ì•„ëƒˆë‹¤ê³  ê°€ì •í•˜ì.</td>
    </tr>
  </tbody>
</table>

<p>ê·¼ë° Theorem1ì— ì˜í•´ì„œ, equivalentí•œ generative modelì¸ z_hat = f(z) ê°€ ìˆì„ ê²ƒì´ë‹¤.</p>

<p>ì´ z_hatì€ ê·¼ë° z, r(x)ì™€ ì™„ì „íˆ entagledë˜ì–´ ìˆë‹¤.</p>

<p>ì™œëƒí•˜ë©´, Theorem 1ì—ì„œ ë§í–ˆë“¯ì´, fì˜ ë¯¸ë¶„ê°’ì´ ëŒ€ì²´ë¡œ 0ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— (=ì¦‰,  ujê°€ fiì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ëœ» = disentagleë˜ë ¤ë©´ ujëŠ” fiì— ì˜í–¥ì„ ë¯¸ì¹˜ë©´ ì•ˆë¨).</p>

<p>ê·¸ëŸ¬ë‹ˆê¹Œ, zì˜ í•œ ì°¨ì›ì„ ë³€í™”ì‹œí‚¤ë©´ z_hatì˜ ëª¨ë“  ì°¨ì›ì´ ë‹¤ ì˜í–¥ì„ ë°›ì•„ì„œ ë³€í™”í•œë‹¤. (=entangled) ê·¸ë¦¬ê³  p(z) = p(z_hat) ì´ë‹ˆê¹Œ ë‘ generative modelì€ xì— ëŒ€í•´ ê°™ì€ marginal distributionì„ ê°€ì§„ë‹¤.</p>

<p>distenglement methodëŠ” ê´€ì¸¡ê°’ì¸ xì—ë§Œ ì ‘ê·¼í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ë‘ ìƒì„± ëª¨ë¸ ì¤‘ ì–´ë–¤ê²Œ ì œëŒ€ë¡œ disentagledëœ ì•„ì´ì¸ì§€ êµ¬ë³„ í•  ìˆ˜ ê°€ ì—†ë‹¤. (ë‘˜ ì¤‘ í•˜ë‚˜ëŠ” ë¬´ì¡°ê±´ entagledë˜ì–´ ìˆë‹¤.)</p>

<p>â‡’ fê°€ infinite familyì´ë‹ˆê¹Œ ê·¸ ë¬´í•œëŒ€ì¸ entaglement ì‚¬ì´ì—ì„œ ì§„ì§œ disentagledëœ ìƒì„±ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤.</p>

<p>â†’ <strong>ì¦‰, inductive bias ì™€ supervision ì—†ì´ëŠ” unsupervised disentaglement learningì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤.</strong></p>

<h1 id="experiment-design">experiment design</h1>

<p>ê¸°ë³¸ì ìœ¼ë¡œ VAE ë¥¼ ì‚¬ìš©í•˜ê³ , loss ëŠ” VAE loss + regularizer</p>

<p>Gaussian encoder, Bernoulli decoder, latent dimensionì€ 10ìœ¼ë¡œ ê³ ì •í•´ì„œ êµ¬ì„±</p>

<p>ê·¸ë¦¬ê³  inductive biasê°€ í•„ìš”í•˜ë‹¤ëŠ” ê±¸ ë³´ì—¬ì£¼ê¸° ìœ„í•´, ì—¬ëŸ¬ hyperparmeter setì„ ì¨ì„œ ì‹¤í—˜ì„ í•  ê²ƒì´ë‹¤.</p>

<h1 id="key-experimental-results">Key experimental results</h1>

<h3 id="1-can-current-methods-enforce-a-uncorrelated-aggregated-posterior-and-representation">1) Can current methods enforce a uncorrelated aggregated posterior and representation?</h3>

<p>ì§ˆë¬¸ì˜ ëœ»ì€, í˜„ì¬ ë°©ì‹ë“¤ì´ aggregated posteriorì™€ representationì´ ê°ê° uncorrelate í•˜ê²Œ í•´ì£¼ëƒ?</p>

<p>ë³´í†µì˜ methodì—ì„œ, Gaussian Encoderì—ì„œ ë‚˜ì˜¨ valueë¥¼ sampleí•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼ mean vectorë¥¼ representationìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.</p>

<p>mean vectorë¡œ í•™ìŠµ ì§„í–‰í–ˆì„ ë•Œì™€, ê·¸ ë¶„í¬ì—ì„œ sampleì„ ì–»ì–´ ì§„í–‰í–ˆì„ë•Œì˜ ê²°ê³¼ë¥¼ ì‚´í´ë³´ë©´ ì•„ë˜ í‘œì™€ ê°™ë‹¤. ì—¬ê¸°ì„œ valueëŠ” total correlationì´ë‹¤.</p>

<p>ì™¼ìª½(sample)ì€ regularization strengthê°€ ê°•í•´ì§ˆìˆ˜ë¡(=ì¼ë°˜í™”ì˜ ì¤‘ìš”ë„ê°€ ê°•í•´ì§ˆìˆ˜ë¡) valueê°€ ê°ì†Œí•˜ê³  ì˜¤ë¥¸ìª½(mean)ì€ ë°˜ëŒ€ë‹¤.</p>

<p>ğŸ¥ : ì—¬ê¸°ì„œ regularization strength â†” value ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì–´ë–»ê²Œ í•´ì„í•˜ëŠ” ê±°ì§€? ì¼ë°˜í™”ê°€ ì˜ ë˜ë©´, ë‹¹ì—°íˆ correlation scoreë„ ë‚®ì•„ì§€ëŠ” ê²Œ ì¼ë°˜ì ì´ë¼ê³  ë³´ëŠ”ê±´ê°€?</p>

<p>â‡’ ì•„! ê·¸ê²Œ ì•„ë‹ˆë¼, regularzation strengthëŠ” hyper parameter ì´ë‹ˆê¹Œ hyperparam choiceì— ë”°ë¼ correlation scoreê°€ ë‹¬ë¼ì§„ë‹¤ê³  ë³´ëŠ”ê²Œ ë§ë‹¤.</p>

<p><img src="../assets/images/ml-research-6.png" alt="ì‹¤í—˜ê²°ê³¼1" style="zoom:67%;" /></p>

<ul>
  <li><strong>Implications</strong></li>
</ul>

<p>í˜„ ë°©ì‹ë“¤ì€ aggregated posteriorê°€ uncorrelateí•œ ë°©í–¥ìœ¼ë¡œ ì‹œí–‰ë˜ëŠ”ê±´ ë§ì§€ë§Œ, ê·¸ë ‡ë‹¤ê³  í•´ì„œ mean representationì˜ dimensionì´ uncorrelateí•˜ë‹¤ëŠ” ëœ»ì€ ì•„ë‹ˆë‹¤.</p>

<h3 id="2-how-much-do-the-disentanglement-metrics-agree">2) How much do the disentanglement metrics agree?</h3>

<p>disentanglementì˜ í™•ì‹¤í•œ ì •ì˜ê°€ ì—†ê¸° ë•Œë¬¸ì— disentaglementì˜ metric scoreê°€ ì–¼ë§ˆë‚˜ í†µì¼ëœ ê²°ê³¼ë¥¼ ë³´ì´ëŠ”ê°€ë¥¼ ì‹¤í—˜í•´ë´¤ë‹¤.</p>

<p>ì•„ë˜ figureëŠ” Noisy-dSprites datasetì— ëŒ€í•œ ê° metricì˜ correlationì„ matrixë¡œ ë³´ì¸ê²ƒì´ë‹¤.</p>

<p>Modularityì •ë„ ë¹¼ë†“ê³ ëŠ” metricë“¤ì´ ì„œë¡œ ì—„ì²­ correlateë˜ì–´ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>ğŸ¥ : ê·¸ë‹ˆê¹Œ correlateë˜ì–´ ìˆë‹¤ëŠ” ê±°ëŠ”, ëŒ€ì¶© metric scoreê°€ í†µì¼ëœ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ê³  ìˆë‹¤ëŠ” ê±°ê² ì§€? Modularityê°™ì€ ì•  ë¹¼ë†“ê³¤ disentanglement ê°€ í™•ì‹¤íˆ ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠëŠ”ë°ë„ ë¶ˆêµ¬í•˜ê³  (= ê·¸ëŸ¬ë‹ˆê¹Œ ë‹¹ì—°íˆ ê·¸ê±¸ ì¸¡ì •í•  metricë„ í™•ì‹¤í•˜ê²Œ ìˆëŠ”ê²Œ ì•„ë‹Œë°) ëŒ€ë¶€ë¶„ ë¹„ìŠ·í•œ ê²°ê³¼ë¥¼ ë³´ì´ë”ë¼.</p>

<p><img src="../assets/images/ml-research-7.png" alt="ì‹¤í—˜ê²°ê³¼2" style="zoom:67%;" /></p>

<ul>
  <li><strong>Implications</strong></li>
</ul>

<p>Modularityë¥¼ ì œì™¸í•œ ëª¨ë“  disentaglement metricsê°€ datasetì— ë”°ë¼ ì •ë„ëŠ” ë‹¤ë¥´ì§€ë§Œ, ì„œë¡œ correlateë˜ì–´ ìˆë‹¤.</p>

<h3 id="3-how-important-are-different-models-and-hyperparameters-for-disentaglement">3) How important are different models and hyperparameters for disentaglement?</h3>

<p>ìš°ë¦¬ê°€ ì‹¤í—˜í•œ ì´ëŸ° methodë“¤ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” Disentanglementë¥¼ ì˜í•˜ê¸° ìœ„í•´ í•˜ëŠ” ê±°ë‹ˆê¹Œ, disentanglement ì„±ëŠ¥ì´ model choice, hyperparameter selection, randomness(=random seed)ì— ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ë§ì´ ë°›ë‚˜ë¥¼ ì‹¤í—˜í•´ë´¤ë‹¤.</p>

<p><img src="../assets/images/ml-research-8.png" alt="ì‹¤í—˜ê²°ê³¼3" style="zoom:67%;" /></p>

<p>ì™¼ìª½ í‘œë¥¼ ë³´ë©´, Modelì— ë”°ë¥¸(hyperparameter, ì—¬ê¸°ì„  regularization strength)ì„ ë‹¬ë¦¬í•´ì„œ ì‹¤í—˜í•œ) score í‘œì¸ë°, ê°™ì€ ëª¨ë¸ì´ì–´ë„ ì—„ì²­ ë¶„í¬ê°€ 0.95 ìƒìœ„ë¶€í„° 0.60ì •ë„ì˜ í•˜ìœ„ê¹Œì§€ ê±¸ì³ì ¸ ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ. varianceê°€ í¬ë‹¤! hyperparmeterì— ì—„ì²­ ì˜í–¥ì„ ë§ì´ ë°›ëŠ” ë‹¤ëŠ” ê±¸ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>ì˜¤ë¥¸ìª½ í‘œëŠ” regularization strengthë¥¼ ë‹¤ë¥´ê²Œ í•´ì„œ í•œ modelì— ëŒ€í•´ random seedë¥¼ ë‹¤ë¥´ê²Œ í•˜ì—¬ ì‹¤í—˜í•œ ê²°ê³¼. ì™¼ìª½ë³´ë‹¤ëŠ” ë‚«ì§€ë§Œ random seedì—ë„ scoreê°€ ì˜í–¥ì„ ë°›ëŠ” ë‹¤ëŠ” ê±¸ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>scoreë¡œ ë”°ì¡Œì„ë•Œ, good run with a bad hyperparmeter &gt; bad run with a good hyperparmeter</p>

<ul>
  <li><strong>Implication</strong></li>
</ul>

<p>unsupervised learningì˜ disentanglement scoreì€ randomness(in the form of random seed)ì™€ hyperparmeter choice(in the form of the regularization strength)ì— ì˜í–¥ì„ ë§‰ëŒ€í•˜ê²Œ ë°›ëŠ”ë‹¤.</p>

<h3 id="4-are-there-reliable-recipes-for-model-selection">4) Are there reliable recipes for model selection?</h3>

<p>ì•ì„œ hyperparameterì™€ model choiceê°€ ì—„ì²­ ì¤‘ìš”í•œ ê²ƒì„ ì•Œì•˜ìœ¼ë‹ˆ, ê·¸ëŸ¼ ì–´ë–»ê²Œ ì–´ë–»ê²Œ ì¢‹ì€ hyperparameterë¥¼ ê³ ë¥´ë‚˜? ì´ ì—°êµ¬ì—ì„œëŠ” (1) good learning modelê³¼ (2) regularization strength corresponding to that loss function ì´ ë‘ê°€ì§€ë¥¼ ì§‘ì¤‘í•´ì„œ ì‚´í´ë³¸ë‹¤.</p>

<ul>
  <li><strong>General recipes for hyperparameter selection</strong></li>
</ul>

<p><img src="../assets/images/ml-research-9.png" alt="MLë„í‘œ" style="zoom:67%;" /></p>

<p>ì´ í‘œë¥¼ Regularization strengthê°€ ë‹¬ë¼ì§ˆ ë•Œë§ˆë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” model ì´ ê³„ì† ë°”ë€ŒëŠ” ê±¸ ì•Œ ìˆ˜ ìˆë‹¤. (â€¦) ì¼ê´€ë˜ê²Œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” modelì´ ì—†ê³ , regularization strengthë¥¼ ê³ ë¥¼ë•Œë„ ì„±ëŠ¥ì„ ìµœëŒ€ë¡œ ëŒì–´ì˜¬ë¦¬ëŠ” ê·¸ëŸ° ì „ëµì´ ì—†ë‹¤.</p>

<ul>
  <li><strong>Model selection based on unsupervised scores</strong></li>
</ul>

<p>ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•´ë³´ì. Hyperparmeterë¥¼ unsupervised score(ex, reconstruction error, KL divergence between the prior and the approximate posterior, ë“±ë“±) ë¥¼ ë³´ê³  ê³ ë¥´ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¬ë ¤ë©´ ìµœì¢… ëª©í‘œëŠ” disentanglement scoreë¥¼ ë†’ì´ëŠ” ê²ƒì´ë‹ˆ disentanglement metricì´ë‘ unsupervised scoreë‘ correlateí•˜ê³  ìˆìŒ unsupervised scoreê¸°ë°˜ìœ¼ë¡œ ê²°ì •ì„ ë‚´ë ¤ë„ ë˜ê³˜ì§€? í•˜ì§€ë§Œ í‘œë¥¼ ë³´ë©´, correlationì´ ê±°ì˜ ì—†ì–´ì„œ unsupervised scoreë¥¼ ì“°ê¸°ì— ì ì ˆì¹˜ ì•Šë‹¤ëŠ” ê²°ë¡ ì„ ë‚´ë ¸ë‹¤.</p>

<p><img src="../assets/images/ml-research-10.png" alt="MLë„í‘œ" style="zoom:67%;" /></p>

<ul>
  <li><strong>Hyperparameter selection based on transfer</strong></li>
</ul>

<p>ğŸ¥ : ì—¬ê¸°ê¹Œì§€ ì˜¤ë‹¤ë‹ˆâ€¦</p>

<p>ì•ì˜ ë‘ ë°©ë²• ë‹¤ ì ì ˆì¹˜ ì•Šì•„ì„œ, ë§ˆì§€ë§‰ìœ¼ë¡œ transferring good settings across data sets ì´ ì „ëµì„ ì·¨í•´ë´¤ë‹¤. í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ì¢‹ì€ hyperparameter settingì€ labelì´ ìˆëŠ” datasetì—ì„œ ì¶”ë¡ ë˜ì–´ ìƒˆë¡œìš´ datasetì— ì ìš©ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.</p>

<p>ìš°ë¦° ì´ transfer based approach to hyperparameter selection vs random model selection ë¹„êµë¥¼ ì´ë ‡ê²Œ í–ˆë‹¤.</p>

<ol>
  <li>50ê°œ random seedì¤‘ í•œ sampleê³¼ random disentanglement metric, random datasetì„ ë½‘ëŠ”ë‹¤. ê·¸ë¦¬ê³  ìš”ê²ƒë“¤ë¡œ ì„±ëŠ¥ì„ ìµœëŒ€ë¡œ ëŒì–´ì˜¬ë¦¬ëŠ” hyperparameter settingì„ ì°¾ëŠ”ë‹¤.</li>
  <li>randomly selected modelê³¼ ìš°ë¦¬ê°€ 1)ì—ì„œ ê³ ë¥¸ hyperparameter settingì„ ë¹„êµí•œë‹¤.</li>
  <li>transfer strategyê°€ random model selectionë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¼ ë•Œë¥¼ 10000 trial ì¤‘ ëª‡ í¼ì„¼íŠ¸ì¸ì§€ í™•ì¸í•  ê²ƒì´ë‹¤.</li>
</ol>

<p><strong>ê²°ê³¼</strong> â‡’ same metric, same dataset (+ different random seed)ë¥¼ ê³ ë¥´ë©´, 80.7% ë‚˜ì˜¨ë‹¤. same metricì„ ì „ì²´ datasetì— transferí•œë‹¤ê³  í•˜ë©´ 59.9%, metricê³¼ dataset ì „ì²´ë¥¼ transferí•˜ë©´ 54.9%ë¡œ ë–¨ì–´ì§„ë‹¤.</p>

<ul>
  <li><strong>Implications</strong></li>
</ul>

<p>Unsupervised model selectionì€ unsolved problemì´ë‹¤. good hyperparameterë¥¼ transferí•˜ëŠ” ë°©ë²•ì€ ë³„ë¡œ ì¢‹ì€ ë°©ë²•ì´ ì•„ë‹Œ ê²ƒ ê°™ë‹¤. ì™œëƒí•˜ë©´ good or bad random seedë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ì—†ê¸° ë•Œë¬¸ì´ë‹¤.</p>

<h3 id="5-are-these-disentagled-representations-useful-for-downstream-tasks-in-terms-of-the-sample-complexity-of-learning">5) Are these disentagled representations useful for downstream tasks in terms of the sample complexity of learning?</h3>

<p>ê·¸ëŸ¬ë©´ ì§„ì§œë¡œ disentangled representationì´ downstream taskì— ìœ ìš©í•˜ëƒ?</p>

<ul>
  <li><strong>Implications</strong></li>
</ul>

<p>ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” interpretability ë‚˜ fairness ì˜ë¯¸ë¥¼ ê°€ì§„ usefulnessë¥¼ í™•ì¸í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì—, ì´ê±¸ ê³ ë ¤í•˜ë©´ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ì¶”ë¡ í•  ìˆ˜ë„ ìˆë‹¤.</p>

<p>í•˜ì§€ë§Œ, ì—¬ëŸ¬ different datasetì— disentangeld representation score ì™€ downstream task performance ì˜ correlationì„ ë”°ì§€ë©´ ê±°ì˜ ê´€ê³„ê°€ ì—†ëŠ” ê±¸ë¡œ ë‚˜ì˜¨ë‹¤. ê·¸ë‹ˆê¹Œ, disentanglementë•Œë¬¸ì— downstream taskì— ì¢‹ì€ ì„±ëŠ¥ì´ ë‚˜ì˜¨ë‹¤ê³  ë§í•  ìˆ˜ ì—†ë‹¤.</p>

<h1 id="conclusion">Conclusion</h1>

<p>ì´ ë…¼ë¬¸ì—ì„  ì´ë¡ ì ìœ¼ë¡œ unsupervised learning of disentanged representationì´ inductive bias ì—†ì´ëŠ” ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ê±¸ ì¦ëª…í–ˆë‹¤.</p>

<p>ê·¸ë¦¬ê³ , ëŒ€ê·œëª¨ ì‹¤í—˜ (6 state-of-the-art disentanglement methodsì™€ metricë¡œ)ì„ ì§„í–‰í•˜ë©° ì´ëŸ° ê²°ë¡ ì„ ë‚´ë ¸ë‹¤.</p>

<ol>
  <li>posteriorë¥¼ factorizingí•˜ëŠ” ê²ƒì´ ê¼­ representationì˜ ì°¨ì›ì´ uncorrelatedë˜ì–´ìˆë‹¤ëŠ”ê±¸ ì•”ì‹œí•˜ì§€ ì•ŠëŠ”ë‹¤.</li>
  <li>Random seedì™€ hyperparmeterê°€ model selectionë³´ë‹¤ ì˜í–¥ì„ ë§ì´ ë¯¸ì¹˜ê³ , tuningí• ë• supervisionì´ í•„ìš”í•˜ë‹¤.</li>
  <li>disentanglementê°€ ì˜ ëœë‹¤ê³  í•´ì„œ downstream taskì˜ í•™ìŠµì´ ì˜ëœë‹¤ëŠ” ê±¸ ì˜ë¯¸í•˜ì§€ ì•ŠëŠ”ë‹¤.</li>
</ol>

<p>ê·¸ë˜ì„œ ì•ìœ¼ë¡œ ìˆì„ ì—°êµ¬ì—ëŠ” ë‹¤ìŒì˜ ê²ƒë“¤ì„ ê¸°ëŒ€í•œë‹¤.</p>

<p>(i) inductive biasì™€ supervisionì˜ ì—­í• ì´ ëª…í™•í•´ì•¼ í•œë‹¤.: model selectionì´ Key questionìœ¼ë¡œ ë‚¨ê² ì§€.</p>

<p>(ii) distenglementì˜ ê°œë…ì´ ì‹¤ì§ˆì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì´ë“ì„ ê°€ì ¸ì˜¤ëŠ”ì§€ë¥¼ ì¦ëª…ë˜ì–´ì•¼ í•œë‹¤.</p>

<p>(iii) ì‹¤í—˜ë“¤ì€ ëª¨ë‘ ì¬ìƒì‚°í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì—ì„œ ì‹œí–‰ë˜ì–´ì•¼í•œë‹¤.</p>

<h1 id="reference">reference</h1>

<p>[ë°°ê²½ ì§€ì‹]ì—ì„œ ì‚¬ìš©ëœ ì´ë¯¸ì§€ë¥¼ ì œì™¸í•œ ëª¨ë“  ì´ë¯¸ì§€ëŠ” <a href="https://arxiv.org/pdf/1811.12359.pdf">Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations ë…¼ë¬¸</a>  ì—ì„œ ê°€ì ¸ì™”ë‹¤.</p>

<p>[ë°°ê²½ ì§€ì‹]ì˜ ì²«ë²ˆì§¸ ì´ë¯¸ì§€(ë„í‘œ)ë¥¼ ì œì™¸í•˜ê³  ë‚´ê°€ ê·¸ë¦° ê·¸ë¦¼ì´ë‹¤.</p>

</div>



<div class="pagination">
  
  
    <a href="/tale/2020-09-22/ibm-cloud-review" class="right arrow">&#8594;</a>
  

  <a href="#" class="top">Top</a>
</div>
    </main>

    <footer>
  <span>
    &copy; <time datetime="2021-02-19 20:56:09 +0900">2021</time> Chester How. Made with Jekyll using the <a href="https://github.com/chesterhow/tale/">Tale</a> theme.
  </span>
</footer>

  </body>
</html>
