<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.17.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - JJIONI NOTES</title>
<meta name="description" content="ë¬´ì‹œë¬´ì‹œí•œ ML ë…¼ë¬¸ ì½ê¸° ">


  <meta name="author" content="JJIONI">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="JJIONI NOTES">
<meta property="og:title" content="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding">
<meta property="og:url" content="http://localhost:4000/machinelearning/ml-researchpaper-2/">


  <meta property="og:description" content="ë¬´ì‹œë¬´ì‹œí•œ ML ë…¼ë¬¸ ì½ê¸° ">







  <meta property="article:published_time" content="2021-02-03T00:00:00+00:00">



  <meta property="article:modified_time" content="2021-02-03T22:42:00+00:00">



  

  


<link rel="canonical" href="http://localhost:4000/machinelearning/ml-researchpaper-2/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "JJIONI",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="JJIONI NOTES Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          JJIONI NOTES
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/" >About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/projects/" >Projects</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/" >Category</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/jjioni_logo.png" alt="JJIONI" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">JJIONI</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p><strong>software development engineer</strong></p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Somewhere</span>
        </li>
      

      
        
          
            <li><a href="mailto:chloe326o.o@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
          
        
          
            <li><a href="https://chloejiwon.github.io" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i> Website</a></li>
          
        
          
            <li><a href="https://github.com/chloejiwon" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding">
    <meta itemprop="description" content="ë¬´ì‹œë¬´ì‹œí•œ ML ë…¼ë¬¸ ì½ê¸°">
    <meta itemprop="datePublished" content="2021-02-03T00:00:00+00:00">
    <meta itemprop="dateModified" content="2021-02-03T22:42:00+00:00">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  5 minute read

</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <blockquote>
  <p>ì´ë²ˆì—” 2019ë…„ë„ì— publishëœ ë…¼ë¬¸ BERTë¥¼ ì½ì–´ë³´ì•˜ë‹¤. ì•„ë¬´ë˜ë„ attention ì— ëŒ€í•œ ì§€ì‹ì´ ê±°ì˜ ì „ë¬´í•˜ë‹¤ë³´ë‹ˆ, í”¼ìƒì ìœ¼ë¡  ì´í•´í–ˆìœ¼ë‚˜ ê¹Šìˆ™í•œ ë©”ì»¤ë‹ˆì¦˜ì€ ì´í•´í•˜ì§€ ëª»í•œ ê²ƒ ê°™ë‹¤. (ë…¼ë¬¸ì´ ì‰½ê²Œ ì“°ì—¬ìˆì–´ì„œ ë§ì •ì´ì§€..) í•˜ì—¬íŠ¼, ë‹¤ìŒ ë…¼ë¬¸ìœ¼ë¡œ attention is all you needë¥¼ ì½ì–´ì„œ ë‹¤ì‹œ ì‹±í¬ë¥¼ ë§ì¶°ë´ì•¼ê² ë‹¤.</p>
</blockquote>

<h2 id="abstract">Abstract</h2>

<p>BERT : New language representation model, <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentation from <strong>T</strong>ransformers</p>

<p>ë‹¤ë¥¸ Language representation modelê³¼ëŠ” ë‹¬ë¦¬, BERTëŠ” ëª¨ë“  layerì—ì„œ unlabeled textë¥¼ ê°€ì§€ê³  left, right ì–‘ ì˜† contextë¥¼ ê³µë™ìœ¼ë¡œ ì¡°ì ˆí•˜ë©° deep bidirectional representationì„ pre-trainí•œë‹¤.</p>

<p>ë”°ë¼ì„œ, pre-trained BERT ëª¨ë¸ì€ ì¶”ê°€ë¡œ output layer í•˜ë‚˜ë§Œ ë”í•´ì„œ fine-tuneí•˜ë©´ ë‹¤ì–‘í•œ taskì— ë‹¤ state-of-the-art ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‹¤.</p>

<h2 id="introduction">Introduction</h2>

<p>Language model pre-trainingì€ ë§ì€ NLP taskì— íš¨ê³¼ì ì´ë‹¤.</p>

<p>pre-trained language representationì„ downstream task(ë³¸ ë¬¸ì œ)ì— ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ë‘ ê°€ì§€ê°€ ìˆë‹¤.</p>

<p><img src="/assets/images/ml-research-2-1.png" alt="ml-1" style="zoom:50%;" /></p>

<ul>
  <li>
    <p><strong>feature-based</strong></p>

    <p><code class="language-plaintext highlighter-rouge">ELMo</code> - pre-trained representationì„ additional featureë¡œ í¬í•¨ì‹œí‚¤ëŠ” task-specific architecture ì‚¬ìš©</p>
  </li>
  <li>
    <p><strong>fine-tuning</strong></p>

    <p><code class="language-plaintext highlighter-rouge">GPT</code>(Generative Pre-trained Transformer) - pre-trained parameterë¥¼ ì „ë¶€ fine-tuning ì‹œí‚´</p>
  </li>
</ul>

<p>ë‘ ë°©ë²• ëª¨ë‘ pre-trainingë•Œ ê°™ì€ objective functionì„ ê°€ì§€ê³  í•™ìŠµì‹œí‚¤ê³ , ë¬´ì—‡ë³´ë‹¤ <strong><code class="language-plaintext highlighter-rouge">unidirectional language model</code></strong> ì„ general language representationì„ í•™ìŠµí• ë•Œ ì‚¬ìš©í•¨.</p>

<p>â‡’ pre-trained representation ì‚¬ìš©ì˜ íš¨ê³¼(íŠ¹íˆ fine-tuning approachì—ì„œ)ë¥¼ ê°ì†Œì‹œí‚´. ëª¨ë¸ì´ unidirectionalì´ê¸° ë•Œë¬¸ì— pre-trainingí• ë•Œ ì‚¬ìš©ë  ëª¨ë¸ì˜ architectureì˜ ì„ íƒì˜ í­ì„ ì¢íŒë‹¤.</p>

<p>ex) GPT ì˜ ê²½ìš°, ëª¨ë“  tokenì´ ê·¸ ì „ token ì—ë§Œ ì˜í–¥ë°›ì„ ìˆ˜ ìˆëŠ” left-to-right architectureë¥¼ ì‚¬ìš©í–ˆë‹¤. question-answering task ì™€ ê°™ì´ ì–‘ ë°©í–¥ì—ì„œì˜ context ê°€ ì¤‘ìš”í•œ ë¬¸ì œì—ì„œ ì¢‹ì§€ ëª»í•˜ë‹¤.</p>

<p><strong>In this paper, fine-tuning based approachë¥¼ ì¢€ ë” ë°œì „ì‹œí‚¤ëŠ” BERTë¥¼ ì œì•ˆí•œë‹¤.</strong></p>

<p>unidirectionalí•˜ê¸° ë•Œë¬¸ì— ë°œìƒí•˜ëŠ” ì œí•œì„ <code class="language-plaintext highlighter-rouge">Masked Language Model (MLM)</code> pre-training objective ë¥¼ ì„¤ì •í•¨ìœ¼ë¡œì¨ ê·¹ë³µí•œë‹¤.</p>

<ul>
  <li>
    <p><strong>Masked Language Model</strong></p>

    <p>inputì—ì„œ ì–´ë–¤ tokenë“¤ì„ ëœë¤í•˜ê²Œ maskí•˜ê³ , objectiveëŠ” ì´ masked wordì—ì„œ ì˜¤ì§ contextë¡œë§Œ original vocabulary idë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤.</p>

    <p>â‡’ left-right modelê³¼ ë‹¬ë¦¬ left, right contextë¥¼ fuseí•  ìˆ˜ ìˆë„ë¡ í•´ì¤Œ</p>

    <p>â‡’ ë”°ë¼ì„œ, deep bidirectional Transformerë¥¼ pre-trainì‹œí‚¬ ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤€ë‹¤</p>
  </li>
</ul>

<p>MLM ì— ì¶”ê°€ë¡œ, next sentence prediction taskë¥¼ ì‚¬ìš©í•´ì„œ text-pair representationì„ pre-train ì‹œí‚¨ë‹¤.</p>

<h3 id="contributions">Contributions</h3>

<ul>
  <li>language representationì—ì„œ bidirectional pre-trainingì˜ ì¤‘ìš”ì„±ì„ ë³´ì—¬ì¤€ë‹¤</li>
  <li>pre-trained representationì´ ì—”ì§€ë‹ˆì–´ë§ ìˆ˜ê³ ê°€ ë§ì´ ë“¤ì–´ê°€ëŠ” task-specfic architectureì˜ í•„ìš”ë¥¼ ì¤„ì—¬ì¤€ë‹¤ëŠ” ê±¸ ë³´ì—¬ì¤€ë‹¤ (ì™œëƒë©´ BERTë¡œ Fine-tuningë§Œ í•˜ë©´ sentence-level &amp; token-level tasksì— SOTA ê°€ëŠ¥í•˜ë‹ˆê¹Œ)</li>
  <li>BERTëŠ” 11 NLP taskì—ì„œ S-O-T-A ê¸°ë¡</li>
</ul>

<h2 id="related-work">Related Work</h2>

<h3 id="1-unsupervised-feature-based-approaches">1. Unsupervised Feature-based Approaches</h3>

<p>Pre-trained word embeddings, sentence embeddings, paragraph embeddings</p>

<p><strong>ELMo</strong> - left-to-right ê³¼ right-to-left ëª¨ë¸ì—ì„œ <em>context-sensitive</em> featureë¥¼ ì¶”ì¶œí•œë‹¤. ê·¸ë˜ì„œ contextual representation of each tokenì€ left-to-right &amp; right-to-left representationì„ í•©ì¹œ ê²ƒ. (contextual word embedding + ê¸°ì¡´ì— ìˆë˜ task-specific architectureë¥¼ ì˜ í†µí•©í•œ ì•Œê³ ë¦¬ì¦˜) ì„±ëŠ¥ì´ ì¢‹ë‹¤.</p>

<h3 id="2-unsupervised-fine-tuning-approaches">2. Unsupervised Fine-tuning Approaches</h3>

<p>ì²˜ìŒì—” feature-based ì ‘ê·¼ë²•ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ unlabeled textë¡œë¶€í„° word embedding parameterë¥¼ pre-trainì‹œì¼°ë‹¤.</p>

<p>pre-trained from unlabeled text â€”&gt; fine-tuned for a supervised downstream task</p>

<p>ì´ ë°©ì‹ì˜ ì¢‹ì€ ì ì€ ë°”ë‹¥ë¶€í„° í•™ìŠµì‹œí‚¬ íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ê°€ ì–¼ë§ˆ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤.</p>

<p>OpenAI GPTëŠ” sentence-level taskì—ì„œ ë§ì´ state-of-the-art ê²°ê³¼ë¥¼ ëƒˆë‹¤.</p>

<h3 id="3-transfer-learning-from-supervised-data">3. Transfer Learning from Supervised Data</h3>

<p>large datasetì„ ê°€ì§€ê³  ìˆëŠ” supervised taskë¡œë¶€í„° íš¨ìœ¨ì ìœ¼ë¡œ transferí•˜ëŠ” ë°©ì‹ì„ ë³´ì—¬ì£¼ëŠ” ì—°êµ¬ë“¤ë„ ìˆë‹¤. (Ex, machine translation, natural language inference)</p>

<p>Computer visionì—ì„œë„ image netê°€ì§€ê³  pre-trainedëœ ëª¨ë¸ì„ fine-tuneí•´ì„œ ì‚¬ìš©í•˜ê¸°ë„ í•œë‹¤. (íš¨ìœ¨ì ì´ë‹ˆê¹Œ)</p>

<h2 id="bert">BERT</h2>

<ol>
  <li>pre-training 2) fine-tuning ì´ ë‘ê°€ì§€ ìŠ¤í…ì„ ê±°ì³ ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">pre-training</code> â‡’ ë‹¤ì–‘í•œ pre-training taskì˜ unlabeled dataë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤.</li>
  <li><code class="language-plaintext highlighter-rouge">fine-tuning</code> â‡’ pre-trained parameterê°’ìœ¼ë¡œ ì´ˆê¸°í™”ì‹œí‚¤ê³ , downstream taskì˜ labeled dataë¥¼ ì‚¬ìš©í•´ì„œ fine-tuningí•œë‹¤.</li>
</ol>

<p>downstream task ë§ˆë‹¤ ë‹¤ë¥¸ fine-tuned modelì„ ê°€ì§„ë‹¤. ì—¬ê¸°ì„œëŠ” question-answering taskë¥¼ ê°€ì§€ê³  ì„¤ëª…í•  ê²ƒì´ë‹¤.</p>

<p>BERTì˜ íŠ¹ì§• ì¤‘ í•˜ë‚˜ëŠ” ë‹¤ì–‘í•œ taskì— í†µì¼ëœ í•˜ë‚˜ì˜ architectureë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒ</p>

<p>pre-trained architecture ê³¼ final downstream architectureê°„ì˜ ì°¨ì´ê°€ ê±°ì˜ ì—†ë‹¤</p>

<ul>
  <li>
    <p><strong>Model Architecture</strong></p>

    <p>multi-layer bidirectional Transformer encoder (decoderëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)</p>

    <p>layerìˆ˜(Transformer Block) : L</p>

    <p>hidden size : H</p>

    <p>self-attention head : A</p>
  </li>
  <li>
    <p><strong>Input/Output Representation</strong></p>

    <p><img src="/assets/images/ml-research-2-2.png" alt="ml-1" /></p>

    <p>BERTëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ ì„¸ ê°€ì§€ embedding ê°’ì„ í•©ì³ì„œ inputìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.</p>

    <p>inputì€ sentence í•˜ë‚˜ë‚˜, pair of sentenceê°€ ì˜¬ê±°ê³ , ì´ê±¸ one token sentenceë¡œ í‘œí˜„í•´ì„œ ì‚¬ìš©</p>

    <p>ì—¬ê¸°ì„œ <strong><em>sentence</em></strong>ëŠ” ì§„ì§œ ë¬¸ì¥ì´ ì•„ë‹ˆë¼ BERTì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” token sequence(í•œ ë¬¸ì¥ o ë‘ ë¬¸ì¥ì„ í•©ì³ë†“ì€ê±°)ë¥¼ ì˜ë¯¸í•œë‹¤</p>

    <p>WordPiece embedding with 30000 token vocabulary ì„ ì‚¬ìš©í–ˆë‹¤.</p>

    <p>ë¬¸ì¥ì˜ ì²«ë²ˆì§¸ tokenì€ ë¬´ì¡°ê±´ [CLS] (ë¬¸ì¥ì˜ ì‹œì‘ì„ ì•Œë¦¬ê¸° ìœ„í•´), ë‘ ë¬¸ì¥ì„ í•©ì³ë†“ì€ inputì—ì„œ ë‘ ë¬¸ì¥ì„ ë¬¸ë§¥ì ìœ¼ë¡œ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ì¤‘ê°„ì— [SEP] tokenì„ ë„£ê³ , í•™ìŠµëœ embeddingì„ ê° tokenì— ë”í•œë‹¤. (ê° tokenì´ sentence A ì— ì†í•˜ëŠ”ì§€, Bì— ì†í•˜ëŠ”ì§€ ì•Œë ¤ì£¼ëŠ”)</p>

    <p><img src="/assets/images/ml-research-2-3.png" alt="ml-1" /></p>
  </li>
</ul>

<h3 id="pre-training">Pre-training</h3>

<p>2ê°œì˜ unsupervised taskë¡œ pre-trainì‹œí‚¬ ê²ƒì´ë‹¤.</p>

<ul>
  <li>
    <p><strong>Task#1 : Masked LM(=Cloze task)</strong></p>

    <p><img src="/assets/images/ml-research-2-4.png" alt="ml-1" /></p>

    <p>input tokenì„ randomí•˜ê²Œ 15%(ì—¬ê¸°ì„œëŠ”)ë¥¼ maskí•˜ê³ , ì „ì²´ inputì„ ì˜ˆì¸¡í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ maskëœ tokenë§Œì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ trainí•œë‹¤.</p>

    <p>ë‹¨ì ì€ [MASK] tokenì´ fine-tuningë•ŒëŠ” ì—†ìœ¼ë‹ˆê¹Œ pre-trainingê³¼ fine-tuningê°„ì˜ mismatchë¥¼ ë°œìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì´ë‹¤.</p>

    <p>â‡’ ì´ê±¸ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ, â€œmaskedâ€ wordë¥¼ ê·¸ëƒ¥ [MASK] tokenìœ¼ë¡œ ë¬´ì¡°ê±´ êµì²´í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, training dataë¥¼ generateí•  ë•Œ random ìœ¼ë¡œ maskì‹œí‚¬ tokenë“¤ì´ ì„ íƒë˜ë©´,</p>

    <p>(i) 80% of timeë§Œ [MASK] Tokenìœ¼ë¡œ êµì²´í•œë‹¤</p>

    <p>(ii) 10%ëŠ” random tokenìœ¼ë¡œ ë°”ê¾¼ë‹¤</p>

    <p>(iii) 10%ëŠ” ê·¸ëƒ¥ ê·¸ëŒ€ë¡œ ë‘”ë‹¤.</p>
  </li>
  <li>
    <p><strong>Task#2 : Next Sentence Prediction (NSP)</strong></p>

    <p>Question Answering(QA) ë‚˜ Natural Language Inference(NLI)ê°™ì€ downstream taskëŠ” ë‘ sentenceì˜ ê´€ê³„ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì—ì„œ ì¶œë°œí•œë‹¤. (language modelingìœ¼ë¡œ ë°”ë¡œ ë³´ì´ì§€ëŠ” ì•ŠìŒ)</p>

    <p>ê´€ê³„ë¥¼ ì•Œê¸° ìœ„í•´ì„œ,  monolingual corpusì—ì„œ ìƒì„±ëœ <em>binarized</em> next sentence prediction taskë¡œ pre-trainí•œë‹¤.</p>

    <p>ì¢€ ë” ìì„¸íˆ ì–´ë–»ê²Œ pre-trainí•˜ëƒë©´, sentence A, Bê°€ ìˆì„ë•Œ (i) 50%ëŠ” Bê°€ Aë‹¤ìŒ ì˜¨ë‹¤(labeled as IsNext). (ii) 50%ëŠ” random sentence(labeled as NotNext)</p>

    <p>Prior workë‘ ë‹¤ë¥¸ ì ì€ prior workëŠ” sentence embeddingë§Œ downstream taskì— transferë˜ëŠ”ë°, BERTëŠ” ëª¨ë“  parameterë¥¼ ì „ë¶€ transferí•œë‹¤.</p>
  </li>
  <li>
    <p><strong>pre-training data</strong></p>

    <p>BooksCorpus(800M words) &amp; English Wikipedia (2500M words) ì‚¬ìš©</p>
  </li>
</ul>

<h3 id="fine-tuning">Fine-tuning</h3>

<p>ë³´í†µ text pairì— ëŒ€í•œ ëª¨ë¸ì„ fine-tuningí•˜ëŠ” ë°©ì‹ì€ 1) ë…ë¦½ì ìœ¼ë¡œ text pairë¥¼ encodingí•˜ê³  2) bidirectional cross attentionì— ì ìš©í•˜ë‹¤.</p>

<p>í•˜ì§€ë§Œ BERTëŠ” ì´ ë‘ ë‹¨ê³„ë¥¼ self-attention mechanismì„ í†µí•´ í•˜ë‚˜ë¡œ í•©ì³¤ë‹¤.</p>

<p>pre-trainingì— ë¹„í•´ êµ‰ì¥íˆ ì €ë ´í•˜ë‹¤. single Cloud TPU ì‚¬ìš©í•´ì„œ ìµœëŒ€ 1ì‹œê°„, GPUë¡œëŠ” ëª‡ì‹œê°„ ì •ë„ë§Œ í•™ìŠµí•˜ë©´ Fine-tuningëœë‹¤.</p>

<h2 id="ablation-studies">Ablation Studies</h2>

<h3 id="effect-of-pre-training-tasks">Effect of Pre-training Tasks</h3>

<p><img src="/assets/images/ml-research-2-5.png" alt="ml-1" /></p>

<ul>
  <li>No NSP â‡’ NSPì˜ ì¤‘ìš”ì„±</li>
  <li>LTR(MLMëŒ€ì‹ ì— Left-to-Right LM, left-context-only model) &amp; No NSP â‡’ Bidirectional Representationì˜ ì¤‘ìš”ì„±</li>
</ul>

<p>it would be possible to train ê° LTR, RTL model and each tokenì„ ì € ë‘ ëª¨ë¸ì˜ concatenationìœ¼ë¡œ ì‚¬ìš© (ELMoì²˜ëŸ¼)</p>

<p>ê·¼ë°, ì´ë ‡ê²Œ í•˜ë©´ <strong>(a)</strong> single bidirectional-modelë³´ë‹¤ ë‘ë°° ë” ë¹„ì‹¸ê³ , <strong>(b)</strong> QAê°™ì€ taskì— non-intuitiveí•˜ë‹¤. (RTL modelì€ questionì— ë‹µì„ ëª»í•˜ë‹ˆê¹Œ) ê·¸ë¦¬ê³  <strong>(c)</strong> ëª¨ë“  layerì—ì„œ left, right contextë°–ì— ì‚¬ìš© ëª»í•˜ë‹ˆê¹Œ deep bidirectional modelë³´ë‹¤ ë³„ë¡œ ì¢‹ì§€ ì•Šë‹¤.</p>

<h3 id="effect-of-model-size">Effect of Model Size</h3>

<p>fine-tuning task accuracyë¥¼ í†µí•´ì„œ model sizeì˜ ì˜í–¥ì„ ì¡°ì‚¬í–ˆë‹¤.</p>

<p>BERT modelì˜ ë ˆì´ì–´ ìˆ˜, hidden units, attention headsë¥¼ ë‹¤ë¥´ê²Œ í•´ì„œ ì‹¤í—˜í–ˆë‹¤. (ë‹¤ë¥¸ hyperparameterí•˜ê³  training procedureëŠ” ë˜‘ê°™ì´ ë‘ê³ )</p>

<p><img src="/assets/images/ml-research-2-6.png" alt="ml-1" /></p>

<ul>
  <li>larger modelì´ accuracy ê°€ ì›”ë“±íˆ ì¢‹ì•˜ë‹¤.</li>
  <li>BERT(base)ëŠ” 110M parameter, BERT(large) ëŠ” 340M parameter</li>
  <li>LM perlexity*ë¥¼ ë¹„êµí•´ë³´ë©´ ëª¨ë¸ ì‚¬ì´ì¦ˆê°€ í¬ë©´ í´ ìˆ˜ë¡ large-scale task(machine translation, language modeling)ì— ì¢‹ë‹¤. ê·¼ë° small taskë„ ëª¨ë¸ ì‚¬ì´ì¦ˆê°€ í´ìˆ˜ë¡ í¬ê²Œ ì„±ëŠ¥ì´ ì¢‹ì•„ì§„ë‹¤.</li>
</ul>

<p><em>*perplexity</em> : ì–¸ì–´ ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë‚´ë¶€ í‰ê°€ ì§€í‘œ. ë‚®ì„ ìˆ˜ë¡ ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ë‹¤ëŠ”ê±¸ ë§í•œë‹¤.</p>

<h3 id="feature-based-approach-with-bert">Feature-based Approach with Bert</h3>

<p>ì§€ê¸ˆê¹Œì§€ ëª¨ë“  ê²°ê³¼ëŠ” fine-tuning approachë¥¼ ì‚¬ìš©í•´ì„œ ì‹¤í—˜ëœ ê²°ê³¼ì´ë‹¤. (simple classification layerê°€ pre-trained modelì— ë”í•´ì§€ê³ , ëª¨ë“  pre-trained model parameterê°€ ì ìš©ë˜ì–´ì„œ downstream taskì— ëŒ€í•´ fine-tune)</p>

<p>Feature-based approach(pre-trained modelì—ì„œ fixed featureê°€ ì¶”ì¶œë˜ëŠ”)ë„ ì¥ì ì´ ìˆë‹¤.</p>

<p>(1) ëª¨ë“  taskê°€ Transformer encoder êµ¬ì¡°ë¡œ ì˜ í‘œí˜„ë˜ëŠ”ê²Œ ì•„ë‹ˆë¼ì„œ, task-specific architectureê°€ í•„ìš”í•  ìˆ˜ ìˆë‹¤.</p>

<p>(2) ë¹„ì‹¼ training dataì˜ representationì„ ê³„ì‚°ì„ ë¨¼ì € í•˜ê³ , ì´ representationì„ ì‚¬ìš©í•œ cheaper modelì—ì„œ ì—¬ëŸ¬ ì‹¤í—˜ì„ í•˜ëŠ”ê²Œ computational costì…ì¥ì—ì„œ í›¨ì”¬ ì´ë“ì´ë‹¤.</p>

<p><img src="/assets/images/ml-research-2-7.png" alt="ml-1" /></p>

<p>ê²°ê³¼ì ìœ¼ë¡œ, BERTê°€ fine tuningê³¼ feature based approach ëª¨ë‘ì—ì„œ ì„±ëŠ¥ì´ ì¢‹ë‹¤.</p>

<h2 id="conclusion">Conclusion</h2>

<p>ìš°ë¦¬ ì—°êµ¬ì˜ ê°€ì¥ í° ì„±ì·¨ëŠ” NLP taskì— ì˜ ì ìš©ë  ìˆ˜ ìˆëŠ” pre-trained modelë¥¼ deep bidirectional architectureë¡œ ì´ë£¨ì—ˆë‹¤ëŠ” ê²ƒì— ìˆë‹¤.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/" class="page__taxonomy-item" rel="tag"></a><span class="sep">, </span>
    
      
      
      <a href="/tags/#%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D" class="page__taxonomy-item" rel="tag">ë¨¸ì‹ ëŸ¬ë‹</a><span class="sep">, </span>
    
      
      
      <a href="/tags/" class="page__taxonomy-item" rel="tag"></a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#machinelearning" class="page__taxonomy-item" rel="tag">MachineLearning</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-02-03">February 3, 2021</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=BERT%3A+Pre-training+of+Deep+Bidirectional+Transformers+for+Language+Understanding%20http%3A%2F%2Flocalhost%3A4000%2Fmachinelearning%2Fml-researchpaper-2%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fmachinelearning%2Fml-researchpaper-2%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fmachinelearning%2Fml-researchpaper-2%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/machinelearning/ml-researchpaper-1/" class="pagination--pager" title="Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machinelearning/ml-researchpaper-1/" rel="permalink">Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  7 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">ë¬´ì‹œë¬´ì‹œí•œ ML ë…¼ë¬¸ ì½ê¸°
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/ibm_clouders/ibm-cloud-review/" rel="permalink">â˜IBM Clouders í™œë™ í›„ê¸°(20/7 - 20/9)
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">IBM Clouders í™œë™ì„ ëŒì•„ë³´ë©°â€¦
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/deeplearning/deeplearning-cnn-1/" rel="permalink">ğŸˆ Deep learning - CNN (1)
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">ë”¥ëŸ¬ë‹ CNN ê¸°ì´ˆ ê°œë… ë½€ê°œê¸°
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/python/python-thread-process/" rel="permalink">pythonì˜ processì™€ thread
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">GILì´ ë­ì•¼!
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://chloejiwon.github.com/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 JJIONI. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>










  </body>
</html>
