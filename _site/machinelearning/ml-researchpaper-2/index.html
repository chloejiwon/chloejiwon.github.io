<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.17.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - JJIONI NOTES</title>
<meta name="description" content="무시무시한 ML 논문 읽기 ">


  <meta name="author" content="JJIONI">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="JJIONI NOTES">
<meta property="og:title" content="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding">
<meta property="og:url" content="http://localhost:4000/machinelearning/ml-researchpaper-2/">


  <meta property="og:description" content="무시무시한 ML 논문 읽기 ">







  <meta property="article:published_time" content="2021-02-03T00:00:00+00:00">



  <meta property="article:modified_time" content="2021-02-03T22:42:00+00:00">



  

  


<link rel="canonical" href="http://localhost:4000/machinelearning/ml-researchpaper-2/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "JJIONI",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="JJIONI NOTES Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          JJIONI NOTES
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/" >About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/projects/" >Projects</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/" >Category</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/jjioni_logo.png" alt="JJIONI" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">JJIONI</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p><strong>software development engineer</strong></p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Somewhere</span>
        </li>
      

      
        
          
            <li><a href="mailto:chloe326o.o@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
          
        
          
            <li><a href="https://chloejiwon.github.io" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i> Website</a></li>
          
        
          
            <li><a href="https://github.com/chloejiwon" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding">
    <meta itemprop="description" content="무시무시한 ML 논문 읽기">
    <meta itemprop="datePublished" content="2021-02-03T00:00:00+00:00">
    <meta itemprop="dateModified" content="2021-02-03T22:42:00+00:00">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  5 minute read

</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <blockquote>
  <p>이번엔 2019년도에 publish된 논문 BERT를 읽어보았다. 아무래도 attention 에 대한 지식이 거의 전무하다보니, 피상적으론 이해했으나 깊숙한 메커니즘은 이해하지 못한 것 같다. (논문이 쉽게 쓰여있어서 망정이지..) 하여튼, 다음 논문으로 attention is all you need를 읽어서 다시 싱크를 맞춰봐야겠다.</p>
</blockquote>

<h2 id="abstract">Abstract</h2>

<p>BERT : New language representation model, <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentation from <strong>T</strong>ransformers</p>

<p>다른 Language representation model과는 달리, BERT는 모든 layer에서 unlabeled text를 가지고 left, right 양 옆 context를 공동으로 조절하며 deep bidirectional representation을 pre-train한다.</p>

<p>따라서, pre-trained BERT 모델은 추가로 output layer 하나만 더해서 fine-tune하면 다양한 task에 다 state-of-the-art 성능을 낼 수 있다.</p>

<h2 id="introduction">Introduction</h2>

<p>Language model pre-training은 많은 NLP task에 효과적이다.</p>

<p>pre-trained language representation을 downstream task(본 문제)에 사용하는 방법은 두 가지가 있다.</p>

<p><img src="/assets/images/ml-research-2-1.png" alt="ml-1" style="zoom:50%;" /></p>

<ul>
  <li>
    <p><strong>feature-based</strong></p>

    <p><code class="language-plaintext highlighter-rouge">ELMo</code> - pre-trained representation을 additional feature로 포함시키는 task-specific architecture 사용</p>
  </li>
  <li>
    <p><strong>fine-tuning</strong></p>

    <p><code class="language-plaintext highlighter-rouge">GPT</code>(Generative Pre-trained Transformer) - pre-trained parameter를 전부 fine-tuning 시킴</p>
  </li>
</ul>

<p>두 방법 모두 pre-training때 같은 objective function을 가지고 학습시키고, 무엇보다 <strong><code class="language-plaintext highlighter-rouge">unidirectional language model</code></strong> 을 general language representation을 학습할때 사용함.</p>

<p>⇒ pre-trained representation 사용의 효과(특히 fine-tuning approach에서)를 감소시킴. 모델이 unidirectional이기 때문에 pre-training할때 사용될 모델의 architecture의 선택의 폭을 좁힌다.</p>

<p>ex) GPT 의 경우, 모든 token이 그 전 token 에만 영향받을 수 있는 left-to-right architecture를 사용했다. question-answering task 와 같이 양 방향에서의 context 가 중요한 문제에서 좋지 못하다.</p>

<p><strong>In this paper, fine-tuning based approach를 좀 더 발전시키는 BERT를 제안한다.</strong></p>

<p>unidirectional하기 때문에 발생하는 제한을 <code class="language-plaintext highlighter-rouge">Masked Language Model (MLM)</code> pre-training objective 를 설정함으로써 극복한다.</p>

<ul>
  <li>
    <p><strong>Masked Language Model</strong></p>

    <p>input에서 어떤 token들을 랜덤하게 mask하고, objective는 이 masked word에서 오직 context로만 original vocabulary id를 예측하는 것이다.</p>

    <p>⇒ left-right model과 달리 left, right context를 fuse할 수 있도록 해줌</p>

    <p>⇒ 따라서, deep bidirectional Transformer를 pre-train시킬 수 있도록 도와준다</p>
  </li>
</ul>

<p>MLM 에 추가로, next sentence prediction task를 사용해서 text-pair representation을 pre-train 시킨다.</p>

<h3 id="contributions">Contributions</h3>

<ul>
  <li>language representation에서 bidirectional pre-training의 중요성을 보여준다</li>
  <li>pre-trained representation이 엔지니어링 수고가 많이 들어가는 task-specfic architecture의 필요를 줄여준다는 걸 보여준다 (왜냐면 BERT로 Fine-tuning만 하면 sentence-level &amp; token-level tasks에 SOTA 가능하니까)</li>
  <li>BERT는 11 NLP task에서 S-O-T-A 기록</li>
</ul>

<h2 id="related-work">Related Work</h2>

<h3 id="1-unsupervised-feature-based-approaches">1. Unsupervised Feature-based Approaches</h3>

<p>Pre-trained word embeddings, sentence embeddings, paragraph embeddings</p>

<p><strong>ELMo</strong> - left-to-right 과 right-to-left 모델에서 <em>context-sensitive</em> feature를 추출한다. 그래서 contextual representation of each token은 left-to-right &amp; right-to-left representation을 합친 것. (contextual word embedding + 기존에 있던 task-specific architecture를 잘 통합한 알고리즘) 성능이 좋다.</p>

<h3 id="2-unsupervised-fine-tuning-approaches">2. Unsupervised Fine-tuning Approaches</h3>

<p>처음엔 feature-based 접근법과 마찬가지로 unlabeled text로부터 word embedding parameter를 pre-train시켰다.</p>

<p>pre-trained from unlabeled text —&gt; fine-tuned for a supervised downstream task</p>

<p>이 방식의 좋은 점은 바닥부터 학습시킬 파라미터의 수가 얼마 없다는 것이다.</p>

<p>OpenAI GPT는 sentence-level task에서 많이 state-of-the-art 결과를 냈다.</p>

<h3 id="3-transfer-learning-from-supervised-data">3. Transfer Learning from Supervised Data</h3>

<p>large dataset을 가지고 있는 supervised task로부터 효율적으로 transfer하는 방식을 보여주는 연구들도 있다. (Ex, machine translation, natural language inference)</p>

<p>Computer vision에서도 image net가지고 pre-trained된 모델을 fine-tune해서 사용하기도 한다. (효율적이니까)</p>

<h2 id="bert">BERT</h2>

<ol>
  <li>pre-training 2) fine-tuning 이 두가지 스텝을 거쳐 모델을 사용한다.</li>
  <li><code class="language-plaintext highlighter-rouge">pre-training</code> ⇒ 다양한 pre-training task의 unlabeled data로 모델을 학습시킨다.</li>
  <li><code class="language-plaintext highlighter-rouge">fine-tuning</code> ⇒ pre-trained parameter값으로 초기화시키고, downstream task의 labeled data를 사용해서 fine-tuning한다.</li>
</ol>

<p>downstream task 마다 다른 fine-tuned model을 가진다. 여기서는 question-answering task를 가지고 설명할 것이다.</p>

<p>BERT의 특징 중 하나는 다양한 task에 통일된 하나의 architecture를 사용한다는 것</p>

<p>pre-trained architecture 과 final downstream architecture간의 차이가 거의 없다</p>

<ul>
  <li>
    <p><strong>Model Architecture</strong></p>

    <p>multi-layer bidirectional Transformer encoder (decoder는 사용하지 않음)</p>

    <p>layer수(Transformer Block) : L</p>

    <p>hidden size : H</p>

    <p>self-attention head : A</p>
  </li>
  <li>
    <p><strong>Input/Output Representation</strong></p>

    <p><img src="/assets/images/ml-research-2-2.png" alt="ml-1" /></p>

    <p>BERT는 위 그림과 같이 세 가지 embedding 값을 합쳐서 input으로 사용한다.</p>

    <p>input은 sentence 하나나, pair of sentence가 올거고, 이걸 one token sentence로 표현해서 사용</p>

    <p>여기서 <strong><em>sentence</em></strong>는 진짜 문장이 아니라 BERT의 입력값으로 사용되는 token sequence(한 문장 o 두 문장을 합쳐놓은거)를 의미한다</p>

    <p>WordPiece embedding with 30000 token vocabulary 을 사용했다.</p>

    <p>문장의 첫번째 token은 무조건 [CLS] (문장의 시작을 알리기 위해), 두 문장을 합쳐놓은 input에서 두 문장을 문맥적으로 구분하기 위해 중간에 [SEP] token을 넣고, 학습된 embedding을 각 token에 더한다. (각 token이 sentence A 에 속하는지, B에 속하는지 알려주는)</p>

    <p><img src="/assets/images/ml-research-2-3.png" alt="ml-1" /></p>
  </li>
</ul>

<h3 id="pre-training">Pre-training</h3>

<p>2개의 unsupervised task로 pre-train시킬 것이다.</p>

<ul>
  <li>
    <p><strong>Task#1 : Masked LM(=Cloze task)</strong></p>

    <p><img src="/assets/images/ml-research-2-4.png" alt="ml-1" /></p>

    <p>input token을 random하게 15%(여기서는)를 mask하고, 전체 input을 예측하는게 아니라 mask된 token만을 예측하는 방식으로 train한다.</p>

    <p>단점은 [MASK] token이 fine-tuning때는 없으니까 pre-training과 fine-tuning간의 mismatch를 발생시킨다는 것이다.</p>

    <p>⇒ 이걸 해결하기 위해서, “masked” word를 그냥 [MASK] token으로 무조건 교체하는게 아니라, training data를 generate할 때 random 으로 mask시킬 token들이 선택되면,</p>

    <p>(i) 80% of time만 [MASK] Token으로 교체한다</p>

    <p>(ii) 10%는 random token으로 바꾼다</p>

    <p>(iii) 10%는 그냥 그대로 둔다.</p>
  </li>
  <li>
    <p><strong>Task#2 : Next Sentence Prediction (NSP)</strong></p>

    <p>Question Answering(QA) 나 Natural Language Inference(NLI)같은 downstream task는 두 sentence의 관계를 이해하는 것에서 출발한다. (language modeling으로 바로 보이지는 않음)</p>

    <p>관계를 알기 위해서,  monolingual corpus에서 생성된 <em>binarized</em> next sentence prediction task로 pre-train한다.</p>

    <p>좀 더 자세히 어떻게 pre-train하냐면, sentence A, B가 있을때 (i) 50%는 B가 A다음 온다(labeled as IsNext). (ii) 50%는 random sentence(labeled as NotNext)</p>

    <p>Prior work랑 다른 점은 prior work는 sentence embedding만 downstream task에 transfer되는데, BERT는 모든 parameter를 전부 transfer한다.</p>
  </li>
  <li>
    <p><strong>pre-training data</strong></p>

    <p>BooksCorpus(800M words) &amp; English Wikipedia (2500M words) 사용</p>
  </li>
</ul>

<h3 id="fine-tuning">Fine-tuning</h3>

<p>보통 text pair에 대한 모델을 fine-tuning하는 방식은 1) 독립적으로 text pair를 encoding하고 2) bidirectional cross attention에 적용하다.</p>

<p>하지만 BERT는 이 두 단계를 self-attention mechanism을 통해 하나로 합쳤다.</p>

<p>pre-training에 비해 굉장히 저렴하다. single Cloud TPU 사용해서 최대 1시간, GPU로는 몇시간 정도만 학습하면 Fine-tuning된다.</p>

<h2 id="ablation-studies">Ablation Studies</h2>

<h3 id="effect-of-pre-training-tasks">Effect of Pre-training Tasks</h3>

<p><img src="/assets/images/ml-research-2-5.png" alt="ml-1" /></p>

<ul>
  <li>No NSP ⇒ NSP의 중요성</li>
  <li>LTR(MLM대신에 Left-to-Right LM, left-context-only model) &amp; No NSP ⇒ Bidirectional Representation의 중요성</li>
</ul>

<p>it would be possible to train 각 LTR, RTL model and each token을 저 두 모델의 concatenation으로 사용 (ELMo처럼)</p>

<p>근데, 이렇게 하면 <strong>(a)</strong> single bidirectional-model보다 두배 더 비싸고, <strong>(b)</strong> QA같은 task에 non-intuitive하다. (RTL model은 question에 답을 못하니까) 그리고 <strong>(c)</strong> 모든 layer에서 left, right context밖에 사용 못하니까 deep bidirectional model보다 별로 좋지 않다.</p>

<h3 id="effect-of-model-size">Effect of Model Size</h3>

<p>fine-tuning task accuracy를 통해서 model size의 영향을 조사했다.</p>

<p>BERT model의 레이어 수, hidden units, attention heads를 다르게 해서 실험했다. (다른 hyperparameter하고 training procedure는 똑같이 두고)</p>

<p><img src="/assets/images/ml-research-2-6.png" alt="ml-1" /></p>

<ul>
  <li>larger model이 accuracy 가 월등히 좋았다.</li>
  <li>BERT(base)는 110M parameter, BERT(large) 는 340M parameter</li>
  <li>LM perlexity*를 비교해보면 모델 사이즈가 크면 클 수록 large-scale task(machine translation, language modeling)에 좋다. 근데 small task도 모델 사이즈가 클수록 크게 성능이 좋아진다.</li>
</ul>

<p><em>*perplexity</em> : 언어 모델을 평가하기 위한 내부 평가 지표. 낮을 수록 언어 모델의 성능이 좋다는걸 말한다.</p>

<h3 id="feature-based-approach-with-bert">Feature-based Approach with Bert</h3>

<p>지금까지 모든 결과는 fine-tuning approach를 사용해서 실험된 결과이다. (simple classification layer가 pre-trained model에 더해지고, 모든 pre-trained model parameter가 적용되어서 downstream task에 대해 fine-tune)</p>

<p>Feature-based approach(pre-trained model에서 fixed feature가 추출되는)도 장점이 있다.</p>

<p>(1) 모든 task가 Transformer encoder 구조로 잘 표현되는게 아니라서, task-specific architecture가 필요할 수 있다.</p>

<p>(2) 비싼 training data의 representation을 계산을 먼저 하고, 이 representation을 사용한 cheaper model에서 여러 실험을 하는게 computational cost입장에서 훨씬 이득이다.</p>

<p><img src="/assets/images/ml-research-2-7.png" alt="ml-1" /></p>

<p>결과적으로, BERT가 fine tuning과 feature based approach 모두에서 성능이 좋다.</p>

<h2 id="conclusion">Conclusion</h2>

<p>우리 연구의 가장 큰 성취는 NLP task에 잘 적용될 수 있는 pre-trained model를 deep bidirectional architecture로 이루었다는 것에 있다.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/" class="page__taxonomy-item" rel="tag"></a><span class="sep">, </span>
    
      
      
      <a href="/tags/#%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D" class="page__taxonomy-item" rel="tag">머신러닝</a><span class="sep">, </span>
    
      
      
      <a href="/tags/" class="page__taxonomy-item" rel="tag"></a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#machinelearning" class="page__taxonomy-item" rel="tag">MachineLearning</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2021-02-03">February 3, 2021</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=BERT%3A+Pre-training+of+Deep+Bidirectional+Transformers+for+Language+Understanding%20http%3A%2F%2Flocalhost%3A4000%2Fmachinelearning%2Fml-researchpaper-2%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fmachinelearning%2Fml-researchpaper-2%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fmachinelearning%2Fml-researchpaper-2%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/machinelearning/ml-researchpaper-1/" class="pagination--pager" title="Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machinelearning/ml-researchpaper-1/" rel="permalink">Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  7 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">무시무시한 ML 논문 읽기
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/ibm_clouders/ibm-cloud-review/" rel="permalink">☁IBM Clouders 활동 후기(20/7 - 20/9)
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">IBM Clouders 활동을 돌아보며…
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/deeplearning/deeplearning-cnn-1/" rel="permalink">🐈 Deep learning - CNN (1)
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">딥러닝 CNN 기초 개념 뽀개기
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/python/python-thread-process/" rel="permalink">python의 process와 thread
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">GIL이 뭐야!
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://chloejiwon.github.com/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 JJIONI. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>










  </body>
</html>
