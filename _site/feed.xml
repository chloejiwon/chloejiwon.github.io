<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-02-03T01:48:01+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">JJIONI NOTES</title><subtitle>NOTES, ...</subtitle><author><name>JJIONI</name></author><entry><title type="html">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title><link href="http://localhost:4000/machinelearning/ml-researchpaper-2/" rel="alternate" type="text/html" title="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" /><published>2021-02-03T00:00:00+00:00</published><updated>2021-02-03T22:42:00+00:00</updated><id>http://localhost:4000/machinelearning/ml-researchpaper-2</id><content type="html" xml:base="http://localhost:4000/machinelearning/ml-researchpaper-2/">&lt;blockquote&gt;
  &lt;p&gt;이번엔 2019년도에 publish된 논문 BERT를 읽어보았다. 아무래도 attention 에 대한 지식이 거의 전무하다보니, 피상적으론 이해했으나 깊숙한 메커니즘은 이해하지 못한 것 같다. (논문이 쉽게 쓰여있어서 망정이지..) 하여튼, 다음 논문으로 attention is all you need를 읽어서 다시 싱크를 맞춰봐야겠다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;BERT : New language representation model, &lt;strong&gt;B&lt;/strong&gt;idirectional &lt;strong&gt;E&lt;/strong&gt;ncoder &lt;strong&gt;R&lt;/strong&gt;epresentation from &lt;strong&gt;T&lt;/strong&gt;ransformers&lt;/p&gt;

&lt;p&gt;다른 Language representation model과는 달리, BERT는 모든 layer에서 unlabeled text를 가지고 left, right 양 옆 context를 공동으로 조절하며 deep bidirectional representation을 pre-train한다.&lt;/p&gt;

&lt;p&gt;따라서, pre-trained BERT 모델은 추가로 output layer 하나만 더해서 fine-tune하면 다양한 task에 다 state-of-the-art 성능을 낼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Language model pre-training은 많은 NLP task에 효과적이다.&lt;/p&gt;

&lt;p&gt;pre-trained language representation을 downstream task(본 문제)에 사용하는 방법은 두 가지가 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-2-1.png&quot; alt=&quot;ml-1&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;feature-based&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ELMo&lt;/code&gt; - pre-trained representation을 additional feature로 포함시키는 task-specific architecture 사용&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;fine-tuning&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPT&lt;/code&gt;(Generative Pre-trained Transformer) - pre-trained parameter를 전부 fine-tuning 시킴&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;두 방법 모두 pre-training때 같은 objective function을 가지고 학습시키고, 무엇보다 &lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unidirectional language model&lt;/code&gt;&lt;/strong&gt; 을 general language representation을 학습할때 사용함.&lt;/p&gt;

&lt;p&gt;⇒ pre-trained representation 사용의 효과(특히 fine-tuning approach에서)를 감소시킴. 모델이 unidirectional이기 때문에 pre-training할때 사용될 모델의 architecture의 선택의 폭을 좁힌다.&lt;/p&gt;

&lt;p&gt;ex) GPT 의 경우, 모든 token이 그 전 token 에만 영향받을 수 있는 left-to-right architecture를 사용했다. question-answering task 와 같이 양 방향에서의 context 가 중요한 문제에서 좋지 못하다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In this paper, fine-tuning based approach를 좀 더 발전시키는 BERT를 제안한다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;unidirectional하기 때문에 발생하는 제한을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Masked Language Model (MLM)&lt;/code&gt; pre-training objective 를 설정함으로써 극복한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Masked Language Model&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;input에서 어떤 token들을 랜덤하게 mask하고, objective는 이 masked word에서 오직 context로만 original vocabulary id를 예측하는 것이다.&lt;/p&gt;

    &lt;p&gt;⇒ left-right model과 달리 left, right context를 fuse할 수 있도록 해줌&lt;/p&gt;

    &lt;p&gt;⇒ 따라서, deep bidirectional Transformer를 pre-train시킬 수 있도록 도와준다&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;MLM 에 추가로, next sentence prediction task를 사용해서 text-pair representation을 pre-train 시킨다.&lt;/p&gt;

&lt;h3 id=&quot;contributions&quot;&gt;Contributions&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;language representation에서 bidirectional pre-training의 중요성을 보여준다&lt;/li&gt;
  &lt;li&gt;pre-trained representation이 엔지니어링 수고가 많이 들어가는 task-specfic architecture의 필요를 줄여준다는 걸 보여준다 (왜냐면 BERT로 Fine-tuning만 하면 sentence-level &amp;amp; token-level tasks에 SOTA 가능하니까)&lt;/li&gt;
  &lt;li&gt;BERT는 11 NLP task에서 S-O-T-A 기록&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt;

&lt;h3 id=&quot;1-unsupervised-feature-based-approaches&quot;&gt;1. Unsupervised Feature-based Approaches&lt;/h3&gt;

&lt;p&gt;Pre-trained word embeddings, sentence embeddings, paragraph embeddings&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ELMo&lt;/strong&gt; - left-to-right 과 right-to-left 모델에서 &lt;em&gt;context-sensitive&lt;/em&gt; feature를 추출한다. 그래서 contextual representation of each token은 left-to-right &amp;amp; right-to-left representation을 합친 것. (contextual word embedding + 기존에 있던 task-specific architecture를 잘 통합한 알고리즘) 성능이 좋다.&lt;/p&gt;

&lt;h3 id=&quot;2-unsupervised-fine-tuning-approaches&quot;&gt;2. Unsupervised Fine-tuning Approaches&lt;/h3&gt;

&lt;p&gt;처음엔 feature-based 접근법과 마찬가지로 unlabeled text로부터 word embedding parameter를 pre-train시켰다.&lt;/p&gt;

&lt;p&gt;pre-trained from unlabeled text —&amp;gt; fine-tuned for a supervised downstream task&lt;/p&gt;

&lt;p&gt;이 방식의 좋은 점은 바닥부터 학습시킬 파라미터의 수가 얼마 없다는 것이다.&lt;/p&gt;

&lt;p&gt;OpenAI GPT는 sentence-level task에서 많이 state-of-the-art 결과를 냈다.&lt;/p&gt;

&lt;h3 id=&quot;3-transfer-learning-from-supervised-data&quot;&gt;3. Transfer Learning from Supervised Data&lt;/h3&gt;

&lt;p&gt;large dataset을 가지고 있는 supervised task로부터 효율적으로 transfer하는 방식을 보여주는 연구들도 있다. (Ex, machine translation, natural language inference)&lt;/p&gt;

&lt;p&gt;Computer vision에서도 image net가지고 pre-trained된 모델을 fine-tune해서 사용하기도 한다. (효율적이니까)&lt;/p&gt;

&lt;h2 id=&quot;bert&quot;&gt;BERT&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;pre-training 2) fine-tuning 이 두가지 스텝을 거쳐 모델을 사용한다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pre-training&lt;/code&gt; ⇒ 다양한 pre-training task의 unlabeled data로 모델을 학습시킨다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fine-tuning&lt;/code&gt; ⇒ pre-trained parameter값으로 초기화시키고, downstream task의 labeled data를 사용해서 fine-tuning한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;downstream task 마다 다른 fine-tuned model을 가진다. 여기서는 question-answering task를 가지고 설명할 것이다.&lt;/p&gt;

&lt;p&gt;BERT의 특징 중 하나는 다양한 task에 통일된 하나의 architecture를 사용한다는 것&lt;/p&gt;

&lt;p&gt;pre-trained architecture 과 final downstream architecture간의 차이가 거의 없다&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Model Architecture&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;multi-layer bidirectional Transformer encoder (decoder는 사용하지 않음)&lt;/p&gt;

    &lt;p&gt;layer수(Transformer Block) : L&lt;/p&gt;

    &lt;p&gt;hidden size : H&lt;/p&gt;

    &lt;p&gt;self-attention head : A&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Input/Output Representation&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-2-2.png&quot; alt=&quot;ml-1&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;BERT는 위 그림과 같이 세 가지 embedding 값을 합쳐서 input으로 사용한다.&lt;/p&gt;

    &lt;p&gt;input은 sentence 하나나, pair of sentence가 올거고, 이걸 one token sentence로 표현해서 사용&lt;/p&gt;

    &lt;p&gt;여기서 &lt;strong&gt;&lt;em&gt;sentence&lt;/em&gt;&lt;/strong&gt;는 진짜 문장이 아니라 BERT의 입력값으로 사용되는 token sequence(한 문장 o 두 문장을 합쳐놓은거)를 의미한다&lt;/p&gt;

    &lt;p&gt;WordPiece embedding with 30000 token vocabulary 을 사용했다.&lt;/p&gt;

    &lt;p&gt;문장의 첫번째 token은 무조건 [CLS] (문장의 시작을 알리기 위해), 두 문장을 합쳐놓은 input에서 두 문장을 문맥적으로 구분하기 위해 중간에 [SEP] token을 넣고, 학습된 embedding을 각 token에 더한다. (각 token이 sentence A 에 속하는지, B에 속하는지 알려주는)&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-2-3.png&quot; alt=&quot;ml-1&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pre-training&quot;&gt;Pre-training&lt;/h3&gt;

&lt;p&gt;2개의 unsupervised task로 pre-train시킬 것이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Task#1 : Masked LM(=Cloze task)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-2-4.png&quot; alt=&quot;ml-1&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;input token을 random하게 15%(여기서는)를 mask하고, 전체 input을 예측하는게 아니라 mask된 token만을 예측하는 방식으로 train한다.&lt;/p&gt;

    &lt;p&gt;단점은 [MASK] token이 fine-tuning때는 없으니까 pre-training과 fine-tuning간의 mismatch를 발생시킨다는 것이다.&lt;/p&gt;

    &lt;p&gt;⇒ 이걸 해결하기 위해서, “masked” word를 그냥 [MASK] token으로 무조건 교체하는게 아니라, training data를 generate할 때 random 으로 mask시킬 token들이 선택되면,&lt;/p&gt;

    &lt;p&gt;(i) 80% of time만 [MASK] Token으로 교체한다&lt;/p&gt;

    &lt;p&gt;(ii) 10%는 random token으로 바꾼다&lt;/p&gt;

    &lt;p&gt;(iii) 10%는 그냥 그대로 둔다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Task#2 : Next Sentence Prediction (NSP)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Question Answering(QA) 나 Natural Language Inference(NLI)같은 downstream task는 두 sentence의 관계를 이해하는 것에서 출발한다. (language modeling으로 바로 보이지는 않음)&lt;/p&gt;

    &lt;p&gt;관계를 알기 위해서,  monolingual corpus에서 생성된 &lt;em&gt;binarized&lt;/em&gt; next sentence prediction task로 pre-train한다.&lt;/p&gt;

    &lt;p&gt;좀 더 자세히 어떻게 pre-train하냐면, sentence A, B가 있을때 (i) 50%는 B가 A다음 온다(labeled as IsNext). (ii) 50%는 random sentence(labeled as NotNext)&lt;/p&gt;

    &lt;p&gt;Prior work랑 다른 점은 prior work는 sentence embedding만 downstream task에 transfer되는데, BERT는 모든 parameter를 전부 transfer한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;pre-training data&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;BooksCorpus(800M words) &amp;amp; English Wikipedia (2500M words) 사용&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fine-tuning&quot;&gt;Fine-tuning&lt;/h3&gt;

&lt;p&gt;보통 text pair에 대한 모델을 fine-tuning하는 방식은 1) 독립적으로 text pair를 encoding하고 2) bidirectional cross attention에 적용하다.&lt;/p&gt;

&lt;p&gt;하지만 BERT는 이 두 단계를 self-attention mechanism을 통해 하나로 합쳤다.&lt;/p&gt;

&lt;p&gt;pre-training에 비해 굉장히 저렴하다. single Cloud TPU 사용해서 최대 1시간, GPU로는 몇시간 정도만 학습하면 Fine-tuning된다.&lt;/p&gt;

&lt;h2 id=&quot;ablation-studies&quot;&gt;Ablation Studies&lt;/h2&gt;

&lt;h3 id=&quot;effect-of-pre-training-tasks&quot;&gt;Effect of Pre-training Tasks&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-2-5.png&quot; alt=&quot;ml-1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;No NSP ⇒ NSP의 중요성&lt;/li&gt;
  &lt;li&gt;LTR(MLM대신에 Left-to-Right LM, left-context-only model) &amp;amp; No NSP ⇒ Bidirectional Representation의 중요성&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;it would be possible to train 각 LTR, RTL model and each token을 저 두 모델의 concatenation으로 사용 (ELMo처럼)&lt;/p&gt;

&lt;p&gt;근데, 이렇게 하면 &lt;strong&gt;(a)&lt;/strong&gt; single bidirectional-model보다 두배 더 비싸고, &lt;strong&gt;(b)&lt;/strong&gt; QA같은 task에 non-intuitive하다. (RTL model은 question에 답을 못하니까) 그리고 &lt;strong&gt;(c)&lt;/strong&gt; 모든 layer에서 left, right context밖에 사용 못하니까 deep bidirectional model보다 별로 좋지 않다.&lt;/p&gt;

&lt;h3 id=&quot;effect-of-model-size&quot;&gt;Effect of Model Size&lt;/h3&gt;

&lt;p&gt;fine-tuning task accuracy를 통해서 model size의 영향을 조사했다.&lt;/p&gt;

&lt;p&gt;BERT model의 레이어 수, hidden units, attention heads를 다르게 해서 실험했다. (다른 hyperparameter하고 training procedure는 똑같이 두고)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-2-6.png&quot; alt=&quot;ml-1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;larger model이 accuracy 가 월등히 좋았다.&lt;/li&gt;
  &lt;li&gt;BERT(base)는 110M parameter, BERT(large) 는 340M parameter&lt;/li&gt;
  &lt;li&gt;LM perlexity*를 비교해보면 모델 사이즈가 크면 클 수록 large-scale task(machine translation, language modeling)에 좋다. 근데 small task도 모델 사이즈가 클수록 크게 성능이 좋아진다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;*perplexity&lt;/em&gt; : 언어 모델을 평가하기 위한 내부 평가 지표. 낮을 수록 언어 모델의 성능이 좋다는걸 말한다.&lt;/p&gt;

&lt;h3 id=&quot;feature-based-approach-with-bert&quot;&gt;Feature-based Approach with Bert&lt;/h3&gt;

&lt;p&gt;지금까지 모든 결과는 fine-tuning approach를 사용해서 실험된 결과이다. (simple classification layer가 pre-trained model에 더해지고, 모든 pre-trained model parameter가 적용되어서 downstream task에 대해 fine-tune)&lt;/p&gt;

&lt;p&gt;Feature-based approach(pre-trained model에서 fixed feature가 추출되는)도 장점이 있다.&lt;/p&gt;

&lt;p&gt;(1) 모든 task가 Transformer encoder 구조로 잘 표현되는게 아니라서, task-specific architecture가 필요할 수 있다.&lt;/p&gt;

&lt;p&gt;(2) 비싼 training data의 representation을 계산을 먼저 하고, 이 representation을 사용한 cheaper model에서 여러 실험을 하는게 computational cost입장에서 훨씬 이득이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-2-7.png&quot; alt=&quot;ml-1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결과적으로, BERT가 fine tuning과 feature based approach 모두에서 성능이 좋다.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;우리 연구의 가장 큰 성취는 NLP task에 잘 적용될 수 있는 pre-trained model를 deep bidirectional architecture로 이루었다는 것에 있다.&lt;/p&gt;</content><author><name>JJIONI</name></author><category term="머신러닝, 논문" /><summary type="html">무시무시한 ML 논문 읽기</summary></entry><entry><title type="html">Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations</title><link href="http://localhost:4000/machinelearning/ml-researchpaper-1/" rel="alternate" type="text/html" title="Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations" /><published>2021-01-27T00:00:00+00:00</published><updated>2021-01-27T22:42:00+00:00</updated><id>http://localhost:4000/machinelearning/ml-researchpaper-1</id><content type="html" xml:base="http://localhost:4000/machinelearning/ml-researchpaper-1/">&lt;blockquote&gt;
  &lt;p&gt;2019년 Top 10 논문에 뽑힌 Common Assumptions in the unsupervised learning of disentangled representations 논문을 살펴보자. 이번 논문은 제목 그대로 Common Assumption에 대해 도전하는 논문이다. 그래서 이걸 이해하려면 애초에 그럼 내가 Common Assumption이 있을 정도로 배경 지식이 있어야 했기 때문에(…) 배경 지식을 (논문을 이해할 만큼만) 간단히 정리해보았다. 여기서 🥝 로 중간 중간  내 의견, 참견들을 적어두었다. 시끄러우면 살포시 무시해주세요.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;배경-지식&quot;&gt;배경 지식&lt;/h1&gt;

&lt;h3 id=&quot;representation-learning&quot;&gt;representation learning&lt;/h3&gt;

&lt;p&gt;representation은 어떤 데이터 포인트를 다른 차원의 데이터 포인트로 바꾸는 것을 의미한다.&lt;/p&gt;

&lt;p&gt;(x1, x2 ..)  → (z1, z2, …)&lt;/p&gt;

&lt;p&gt;주어진 데이터를 다른 차원에 &lt;strong&gt;표현&lt;/strong&gt; 한다는 뜻으로 받아들이면 된다. 그리고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;learning&lt;/code&gt;이니까 그 표현하는 방법을 학습하는 것이다. 주어진 데이터를 다른 데이터 포인트로 mapping하는 모델을 학습한다고 보면 되겠다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-1.png&quot; alt=&quot;ML도표&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;disentanglement&quot;&gt;Disentanglement&lt;/h3&gt;

&lt;p&gt;Transforming from an uninterpretable space with entagled features to eigen spaces where features are independent.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;행렬의 고유 벡터(eigen vector)들이 형성하는 부분 공간 = 고유 공간(eigenspace)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;조금 더 풀어 설명하자면, 아래 그림에서 보듯이 x를 만들때 영향을 미치는 독립적인 요인인 z1, z2, …. 가 있다고 치자. 그래서 x가 주어졌을때 그의 표현인 r(x)를 만들때 (← r(x)만들때 z들이 영향을 끼치는건데) 한 요소가 바뀌면 딱 한 representation vector만 바뀌는 걸 의미한다.&lt;/p&gt;

&lt;p&gt;다른 representation vector까지 바꿔버리면, z가 entagled되어 있다는 걸 의미한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-2.png&quot; alt=&quot;disentanglement&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;disentagled-representation&quot;&gt;Disentagled Representation&lt;/h3&gt;

&lt;p&gt;어떤 data(이미지)를 나타내는 latent variable이 여러 개로 분리 되어 각각 다른 데이터(이미지)의 특성에 관한 정보를 담고 있는 것을 의미&lt;/p&gt;

&lt;h3 id=&quot;latent-variable&quot;&gt;latent variable&lt;/h3&gt;

&lt;p&gt;that describes original data&lt;/p&gt;

&lt;p&gt;잠재 변수. Factor Analysis 를 할 때 많은 수의 변수들을 소수의 몇 개의 잠재된 변수로 찾아내는데, 그 변수가 latent variable.&lt;/p&gt;

&lt;h3 id=&quot;autoencoder&quot;&gt;Autoencoder&lt;/h3&gt;

&lt;p&gt;for (Nonlinear) Dimensionality Reduction, Feature extraction, Representation learning&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-3.png&quot; alt=&quot;autoencoder&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;vae-variational-auto-encoder&quot;&gt;VAE (variational auto encoder)&lt;/h3&gt;

&lt;p&gt;생성모델을 다루는 기술 중 하나. GAN과 같이 각광받고 있음.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-4.png&quot; alt=&quot;vae&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;downstream-task&quot;&gt;downstream task&lt;/h3&gt;

&lt;p&gt;실제로, 구체적으로 풀고 싶은 문제(task)&lt;/p&gt;

&lt;p&gt;요새는 대부분 pretrain 하고 그 모델에 우리가 풀고 싶어하는 downstream task 를 넣어서 fine tuning을 한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;unsupervised learning of disentagled representation의 key idea :  real-world의 데이터가 소수의 explanatory factor(unsupervised learning 알고리즘으로 만들어지는)로 생성될 수 있다는 것.&lt;/p&gt;

&lt;p&gt;여기서 넘어가기 전, prior과 posterior의 개념에 대해 짚고 넘어가자.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;prior&lt;/strong&gt; : z&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;posterior(=likelihood)&lt;/strong&gt; : P(x&lt;/td&gt;
          &lt;td&gt;z)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;prior&lt;/strong&gt;는 어떤 중요한 factor(autoencoder 입장에서 보면 latent variable), &lt;strong&gt;posterior&lt;/strong&gt;는 prior(=z)들이 주어졌을때, x를 z의 결합으로 표현하는 것이다.&lt;/p&gt;

&lt;p&gt;in this paper, 이 분야의 최근 진행 상황을 살펴보고, 일반적인 가정에 challenge하겠다.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;In representation learning, real-world observations x는 이렇게 2-step으로 생성된다고 생각한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;multivariate latent random variable z가 P(z) 분포로부터 샘플된다.&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;P(x&lt;/td&gt;
          &lt;td&gt;z)로부터 x가 샘플된다.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;그니까, real-world에서는 우리가 모르는 요인인 z가 이미 있는데, 그 z로부터 x(실세계에 존재하는 고차원의 데이터)를 generate할 수 있다고 생각하는 것이다.&lt;/p&gt;

&lt;p&gt;⇒ 즉, 고차원의 데이터 x가 저차원의 데이터 z(semantically meaningful latent variable)로 표현이 된다.&lt;/p&gt;

&lt;p&gt;그러면, 여기서 목표와 왜를 한번 더 짚어보자.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Representation learning의 목표&lt;/strong&gt; : useful transformation r(x)를 찾는 것. 왜? Classifier나 다른 예측 모델을 만들 때, 어떤 x에 대해 유용한 information을 잘 추출하기 위해서.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;그리고 disentaglement가 여기서 왜 중요하냐?&lt;/strong&gt; data에서 informative하고 구분되는 factor를 나눠야 좀 더 잘 생성될 수 있어 보인다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;이-논문에서-말하고자-하는-주장들의-summary-사실-이게-전부다&quot;&gt;이 논문에서 말하고자 하는 주장들의 summary (사실 이게 전부다…)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Unsupervised learning of disentangled representation은 learning approach(모델,..)와 dataset에  inductive bias없이는 불가능하다. (이걸 이론적으로 증명할 것이다.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;그래서 current approach들과 inductive bias에 대해 조사하고 실험했다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;우리가 한 disentaglement_lib 도 배포하고, trained model도 여러개 배포했다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;우리는 이렇게 실험 결과를 분석했다.&lt;/p&gt;

    &lt;p&gt;(i) 모든 method가&lt;/p&gt;

    &lt;p&gt;(ii) random seed와 hyperparameter가 model choice보다 더 중요하다. 즉, reliable한 model이 없다.&lt;/p&gt;

    &lt;p&gt;(iii) disentaglement가 downstream tasks에 유용한지 검증할 길이 없다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;그래서 further research에게는 followings가 있음 좋겠다.&lt;/p&gt;

    &lt;p&gt;(i) inductive bias와 supervision의 역할이 명확해야 한다.: model selection이 Key question으로 남겠지.&lt;/p&gt;

    &lt;p&gt;(ii) distenglement의 개념이 실질적으로 얼마나 이득을 가져오는지를 증명되어야 한다.&lt;/p&gt;

    &lt;p&gt;(iii) 실험들은 모두 재생산할 수 있는 환경에서 시행되어야한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;impossibility-result&quot;&gt;Impossibility result&lt;/h1&gt;

&lt;p&gt;어떤 generative model도 unsupervised disentanglement learning이 가능한가? 를 증명해보자.&lt;/p&gt;

&lt;p&gt;⇒ 답은 No이고 아래 theorem을 이용할 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-5.png&quot; alt=&quot;Theorem&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;(증명) P(x&lt;/td&gt;
      &lt;td&gt;z) 가 generative model이라고 하자. 그리고 z에 대해 완벽하게 disentagled된 representation r(x)를 찾아냈다고 가정하자.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;근데 Theorem1에 의해서, equivalent한 generative model인 z_hat = f(z) 가 있을 것이다.&lt;/p&gt;

&lt;p&gt;이 z_hat은 근데 z, r(x)와 완전히 entagled되어 있다.&lt;/p&gt;

&lt;p&gt;왜냐하면, Theorem 1에서 말했듯이, f의 미분값이 대체로 0이 아니기 때문에 (=즉,  uj가 fi에 영향을 미친다는 뜻 = disentagle되려면 uj는 fi에 영향을 미치면 안됨).&lt;/p&gt;

&lt;p&gt;그러니까, z의 한 차원을 변화시키면 z_hat의 모든 차원이 다 영향을 받아서 변화한다. (=entangled) 그리고 p(z) = p(z_hat) 이니까 두 generative model은 x에 대해 같은 marginal distribution을 가진다.&lt;/p&gt;

&lt;p&gt;distenglement method는 관측값인 x에만 접근할 수 있기 때문에, 두 생성 모델 중 어떤게 제대로 disentagled된 아이인지 구별 할 수 가 없다. (둘 중 하나는 무조건 entagled되어 있다.)&lt;/p&gt;

&lt;p&gt;⇒ f가 infinite family이니까 그 무한대인 entaglement 사이에서 진짜 disentagled된 생성모델을 찾을 수 없다.&lt;/p&gt;

&lt;p&gt;→ &lt;strong&gt;즉, inductive bias 와 supervision 없이는 unsupervised disentaglement learning이 불가능하다.&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;experiment-design&quot;&gt;experiment design&lt;/h1&gt;

&lt;p&gt;기본적으로 VAE 를 사용하고, loss 는 VAE loss + regularizer&lt;/p&gt;

&lt;p&gt;Gaussian encoder, Bernoulli decoder, latent dimension은 10으로 고정해서 구성&lt;/p&gt;

&lt;p&gt;그리고 inductive bias가 필요하다는 걸 보여주기 위해, 여러 hyperparmeter set을 써서 실험을 할 것이다.&lt;/p&gt;

&lt;h1 id=&quot;key-experimental-results&quot;&gt;Key experimental results&lt;/h1&gt;

&lt;h3 id=&quot;1-can-current-methods-enforce-a-uncorrelated-aggregated-posterior-and-representation&quot;&gt;1) Can current methods enforce a uncorrelated aggregated posterior and representation?&lt;/h3&gt;

&lt;p&gt;질문의 뜻은, 현재 방식들이 aggregated posterior와 representation이 각각 uncorrelate 하게 해주냐?&lt;/p&gt;

&lt;p&gt;보통의 method에서, Gaussian Encoder에서 나온 value를 sample해서 사용하는 게 아니라 mean vector를 representation으로 사용한다.&lt;/p&gt;

&lt;p&gt;mean vector로 학습 진행했을 때와, 그 분포에서 sample을 얻어 진행했을때의 결과를 살펴보면 아래 표와 같다. 여기서 value는 total correlation이다.&lt;/p&gt;

&lt;p&gt;왼쪽(sample)은 regularization strength가 강해질수록(=일반화의 중요도가 강해질수록) value가 감소하고 오른쪽(mean)은 반대다.&lt;/p&gt;

&lt;p&gt;🥝 : 여기서 regularization strength ↔ value 사이의 관계를 어떻게 해석하는 거지? 일반화가 잘 되면, 당연히 correlation score도 낮아지는 게 일반적이라고 보는건가?&lt;/p&gt;

&lt;p&gt;⇒ 아! 그게 아니라, regularzation strength는 hyper parameter 이니까 hyperparam choice에 따라 correlation score가 달라진다고 보는게 맞다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-6.png&quot; alt=&quot;실험결과1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Implications&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;현 방식들은 aggregated posterior가 uncorrelate한 방향으로 시행되는건 맞지만, 그렇다고 해서 mean representation의 dimension이 uncorrelate하다는 뜻은 아니다.&lt;/p&gt;

&lt;h3 id=&quot;2-how-much-do-the-disentanglement-metrics-agree&quot;&gt;2) How much do the disentanglement metrics agree?&lt;/h3&gt;

&lt;p&gt;disentanglement의 확실한 정의가 없기 때문에 disentaglement의 metric score가 얼마나 통일된 결과를 보이는가를 실험해봤다.&lt;/p&gt;

&lt;p&gt;아래 figure는 Noisy-dSprites dataset에 대한 각 metric의 correlation을 matrix로 보인것이다.&lt;/p&gt;

&lt;p&gt;Modularity정도 빼놓고는 metric들이 서로 엄청 correlate되어 있음을 알 수 있다.&lt;/p&gt;

&lt;p&gt;🥝 : 그니까 correlate되어 있다는 거는, 대충 metric score가 통일된 결과를 보여주고 있다는 거겠지? Modularity같은 애 빼놓곤 disentanglement 가 확실히 정의되어 있지 않는데도 불구하고 (= 그러니까 당연히 그걸 측정할 metric도 확실하게 있는게 아닌데) 대부분 비슷한 결과를 보이더라.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-7.png&quot; alt=&quot;실험결과2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Implications&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Modularity를 제외한 모든 disentaglement metrics가 dataset에 따라 정도는 다르지만, 서로 correlate되어 있다.&lt;/p&gt;

&lt;h3 id=&quot;3-how-important-are-different-models-and-hyperparameters-for-disentaglement&quot;&gt;3) How important are different models and hyperparameters for disentaglement?&lt;/h3&gt;

&lt;p&gt;우리가 실험한 이런 method들을 사용하는 이유는 Disentanglement를 잘하기 위해 하는 거니까, disentanglement 성능이 model choice, hyperparameter selection, randomness(=random seed)에 얼마나 영향을 많이 받나를 실험해봤다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-8.png&quot; alt=&quot;실험결과3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;왼쪽 표를 보면, Model에 따른(hyperparameter, 여기선 regularization strength)을 달리해서 실험한) score 표인데, 같은 모델이어도 엄청 분포가 0.95 상위부터 0.60정도의 하위까지 걸쳐져 있는 것을 볼 수 있음. variance가 크다! hyperparmeter에 엄청 영향을 많이 받는 다는 걸 알 수 있다.&lt;/p&gt;

&lt;p&gt;오른쪽 표는 regularization strength를 다르게 해서 한 model에 대해 random seed를 다르게 하여 실험한 결과. 왼쪽보다는 낫지만 random seed에도 score가 영향을 받는 다는 걸 알 수 있다.&lt;/p&gt;

&lt;p&gt;score로 따졌을때, good run with a bad hyperparmeter &amp;gt; bad run with a good hyperparmeter&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Implication&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;unsupervised learning의 disentanglement score은 randomness(in the form of random seed)와 hyperparmeter choice(in the form of the regularization strength)에 영향을 막대하게 받는다.&lt;/p&gt;

&lt;h3 id=&quot;4-are-there-reliable-recipes-for-model-selection&quot;&gt;4) Are there reliable recipes for model selection?&lt;/h3&gt;

&lt;p&gt;앞서 hyperparameter와 model choice가 엄청 중요한 것을 알았으니, 그럼 어떻게 어떻게 좋은 hyperparameter를 고르나? 이 연구에서는 (1) good learning model과 (2) regularization strength corresponding to that loss function 이 두가지를 집중해서 살펴본다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;General recipes for hyperparameter selection&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-9.png&quot; alt=&quot;ML도표&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 표를 Regularization strength가 달라질 때마다 좋은 성능을 내는 model 이 계속 바뀌는 걸 알 수 있다. (…) 일관되게 좋은 성능을 내는 model이 없고, regularization strength를 고를때도 성능을 최대로 끌어올리는 그런 전략이 없다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Model selection based on unsupervised scores&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다른 방식으로 접근해보자. Hyperparmeter를 unsupervised score(ex, reconstruction error, KL divergence between the prior and the approximate posterior, 등등) 를 보고 고르는 것이다. 그러려면 최종 목표는 disentanglement score를 높이는 것이니 disentanglement metric이랑 unsupervised score랑 correlate하고 있음 unsupervised score기반으로 결정을 내려도 되곘지? 하지만 표를 보면, correlation이 거의 없어서 unsupervised score를 쓰기에 적절치 않다는 결론을 내렸다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ml-research-10.png&quot; alt=&quot;ML도표&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Hyperparameter selection based on transfer&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;🥝 : 여기까지 오다니…&lt;/p&gt;

&lt;p&gt;앞의 두 방법 다 적절치 않아서, 마지막으로 transferring good settings across data sets 이 전략을 취해봤다. 핵심 아이디어는 좋은 hyperparameter setting은 label이 있는 dataset에서 추론되어 새로운 dataset에 적용될 수 있다는 것이다.&lt;/p&gt;

&lt;p&gt;우린 이 transfer based approach to hyperparameter selection vs random model selection 비교를 이렇게 했다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;50개 random seed중 한 sample과 random disentanglement metric, random dataset을 뽑는다. 그리고 요것들로 성능을 최대로 끌어올리는 hyperparameter setting을 찾는다.&lt;/li&gt;
  &lt;li&gt;randomly selected model과 우리가 1)에서 고른 hyperparameter setting을 비교한다.&lt;/li&gt;
  &lt;li&gt;transfer strategy가 random model selection보다 좋은 성능을 보일 때를 10000 trial 중 몇 퍼센트인지 확인할 것이다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;결과&lt;/strong&gt; ⇒ same metric, same dataset (+ different random seed)를 고르면, 80.7% 나온다. same metric을 전체 dataset에 transfer한다고 하면 59.9%, metric과 dataset 전체를 transfer하면 54.9%로 떨어진다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Implications&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unsupervised model selection은 unsolved problem이다. good hyperparameter를 transfer하는 방법은 별로 좋은 방법이 아닌 것 같다. 왜냐하면 good or bad random seed를 구분할 수 있는 방법이 없기 때문이다.&lt;/p&gt;

&lt;h3 id=&quot;5-are-these-disentagled-representations-useful-for-downstream-tasks-in-terms-of-the-sample-complexity-of-learning&quot;&gt;5) Are these disentagled representations useful for downstream tasks in terms of the sample complexity of learning?&lt;/h3&gt;

&lt;p&gt;그러면 진짜로 disentangled representation이 downstream task에 유용하냐?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Implications&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;여기서 우리는 interpretability 나 fairness 의미를 가진 usefulness를 확인하지 않았기 때문에, 이걸 고려하면 다른 결과를 추론할 수도 있다.&lt;/p&gt;

&lt;p&gt;하지만, 여러 different dataset에 disentangeld representation score 와 downstream task performance 의 correlation을 따지면 거의 관계가 없는 걸로 나온다. 그니까, disentanglement때문에 downstream task에 좋은 성능이 나온다고 말할 수 없다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;이 논문에선 이론적으로 unsupervised learning of disentanged representation이 inductive bias 없이는 불가능하다는 걸 증명했다.&lt;/p&gt;

&lt;p&gt;그리고, 대규모 실험 (6 state-of-the-art disentanglement methods와 metric로)을 진행하며 이런 결론을 내렸다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;posterior를 factorizing하는 것이 꼭 representation의 차원이 uncorrelated되어있다는걸 암시하지 않는다.&lt;/li&gt;
  &lt;li&gt;Random seed와 hyperparmeter가 model selection보다 영향을 많이 미치고, tuning할땐 supervision이 필요하다.&lt;/li&gt;
  &lt;li&gt;disentanglement가 잘 된다고 해서 downstream task의 학습이 잘된다는 걸 의미하지 않는다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;그래서 앞으로 있을 연구에는 다음의 것들을 기대한다.&lt;/p&gt;

&lt;p&gt;(i) inductive bias와 supervision의 역할이 명확해야 한다.: model selection이 Key question으로 남겠지.&lt;/p&gt;

&lt;p&gt;(ii) distenglement의 개념이 실질적으로 얼마나 이득을 가져오는지를 증명되어야 한다.&lt;/p&gt;

&lt;p&gt;(iii) 실험들은 모두 재생산할 수 있는 환경에서 시행되어야한다.&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;reference&lt;/h1&gt;

&lt;p&gt;[배경 지식]에서 사용된 이미지를 제외한 모든 이미지는https://arxiv.org/pdf/1811.12359.pdf 에서 가져왔다.&lt;/p&gt;

&lt;p&gt;[배경 지식]의 첫번째 이미지(도표)를 제외하고 내가 그린 그림이다.&lt;/p&gt;</content><author><name>JJIONI</name></author><category term="머신러닝, 논문" /><summary type="html">무시무시한 ML 논문 읽기</summary></entry><entry><title type="html">☁IBM Clouders 활동 후기(20/7 - 20/9)</title><link href="http://localhost:4000/ibm_clouders/ibm-cloud-review/" rel="alternate" type="text/html" title="☁IBM Clouders 활동 후기(20/7 - 20/9)" /><published>2020-09-22T00:00:00+00:00</published><updated>2020-09-22T06:11:00+00:00</updated><id>http://localhost:4000/ibm_clouders/ibm-cloud-review</id><content type="html" xml:base="http://localhost:4000/ibm_clouders/ibm-cloud-review/">&lt;blockquote&gt;
  &lt;p&gt;IBM Clouder로 불성실하게 살아온지(?) 7~9월 어느덧 3개월 정도가 되었습니다. 🤧 그동안의 미션과 활동에 대해 되돌아 보며 주로 반성하는 시간을 갖도록 하겠습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;시작&quot;&gt;시작&lt;/h1&gt;

&lt;p&gt;IBM 에서 주관하는 Call For Code 2020에 참여하며 이 모든 것이 시작되었습니다. (거창..) 할 줄 하는 거 하나 없었는데 친구랑 해커톤 한번 나가보자 했는데 이렇게 스케일 큰 해커톤일 줄이야. 😨 System architecture도 생각하고, 기획도 하고, 셋 밖에 없는 팀이라 처음부터 끝까지 다 달려들어서 해야 했어서 기억에 많이 남는 것 같아요. 결과야 어찌 됐든(😂) 서울 해커톤을 끝내고 Clouders 홍보를 보고 지원했습니다.&lt;/p&gt;

&lt;h1 id=&quot;7월&quot;&gt;7월&lt;/h1&gt;

&lt;p&gt;7월 미션은 개발 블로그 만들기, IBM Cloud와 관련된 기술 글, 자신이 관심있는 기술에 관한 글 하나, IBM Cloud 강의보고 뱃지 획득 이었습니다.&lt;/p&gt;

&lt;p&gt;7월까지 일정 맞춰 진행한 미션은 개발 블로그 만들기네요. 😂&lt;/p&gt;

&lt;p&gt;7월 내내 IBM Call For Code 2020 글로벌 해커톤 때문에 내내 정신이 없었네요.&lt;/p&gt;

&lt;h1 id=&quot;8월&quot;&gt;8월&lt;/h1&gt;

&lt;p&gt;최종 제출하고 미션을 부랴부랴 시작했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://chloejiwon.github.io/%EC%9B%B9,%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/webrtc+tensorflow/&quot;&gt;자신이 관심있는 기술&lt;/a&gt; - WebRTC로 화상채팅을, Tensorflow.js로 object detection을 실시간으로 해보자. (이 와중에 2개나 씀)&lt;/li&gt;
  &lt;li&gt;IBM Cloud Essentials 뱃지 획득&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;제가 직접 구현해보고 고생하고 쓴 글이라 그런지 애틋.. 🥺 이렇게 7월 미션이 마무리 된 줄 알았으나..&lt;/p&gt;

&lt;h1 id=&quot;9월&quot;&gt;9월&lt;/h1&gt;

&lt;p&gt;7월 미션에 IBM 관련 기술 포스팅이 있다는 것을 깨닫고 9월에 7월,8월,9월 미션까지.. 🥶&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Building Cloud native and multi cloud applications 뱃지 턱걸이 획득&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://chloejiwon.github.io/%EC%9B%B9/ibm-cloudfoundry/&quot;&gt;IBM 관련 기술&lt;/a&gt; - IBM Cloud Foundry로 nodejs 간단 배포하기&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://chloejiwon.github.io/python/python-thread-process/&quot;&gt;공부한 분야 관련 포스팅&lt;/a&gt; - Python의 process와 thread&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://chloejiwon.github.io/nlp/ibm-nlp-for-beginners/&quot;&gt;IBM Cloud 관련 기술&lt;/a&gt; - IBM Watson을 이용한 간단한 NLP 실습&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://chloejiwon.github.io/deeplearning/deeplearning-cnn-1/&quot;&gt;관심있는 기술 관련 포스팅&lt;/a&gt; - Deep learning 의 CNN 기초 개념&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://chloejiwon.github.io/ibmcloud/ibmcloud-autoai/&quot;&gt;IBM 관련 기술&lt;/a&gt; - IBM Cloud - AutoAI 실습&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;IBM 관련 기술 포스팅을 준비하며 IBM Cloud의 여러 서비스를 이용해 봤는데 진짜 생각보다 너무 편리하고 재밌었다! cloud foundry랑 auto AI… 🤭 뭐야뭐야. AWS도 사용하긴 했었는데 너무 옛날이라 기억 안나기도 하고 좀 어렵기도 하고.. 해봤으니 혼자 공부하거나, 뭐 만들거나 할때 Cloud Foundry로 편하게 배포하면 좋을 것 같다. (이러려고 Clouders 만든 거겠지..?)&lt;/p&gt;

&lt;h1 id=&quot;후기&quot;&gt;후기&lt;/h1&gt;

&lt;p&gt;미션이야 부랴부랴 끝냈지만,  Clouders 활동하며 여러 사람들 포스팅도 보고, 스터디에도 들어가면서 (활동은 진짜 안했지만.. 죄송합니다. 😩) 세상에 대단한 사람들은 참 많구나 새삼 생각했다. 다들 너무 Eager to learn하고 부지런!!!! 여러 강의들과 세션과 자료 들도 좋았지만 자극을 많이 받은 게 제일 좋았던 듯.&lt;/p&gt;</content><author><name>JJIONI</name></author><category term="ibmcloud" /><summary type="html">IBM Clouders 활동을 돌아보며...</summary></entry><entry><title type="html">🐈 Deep learning - CNN (1)</title><link href="http://localhost:4000/deeplearning/deeplearning-cnn-1/" rel="alternate" type="text/html" title="🐈 Deep learning - CNN (1) " /><published>2020-09-21T00:00:00+00:00</published><updated>2020-09-21T06:11:00+00:00</updated><id>http://localhost:4000/deeplearning/deeplearning-cnn-1</id><content type="html" xml:base="http://localhost:4000/deeplearning/deeplearning-cnn-1/">&lt;blockquote&gt;
  &lt;p&gt;딥 러닝 공부의 기록. CNN 기초를 다져 보자. (언제 RNN, transformer 등…. 하지? 막막🤧) 대부분의 내용은 [밑바닥부터 시작하는 딥러닝] 책을 바탕으로 하며 거의 요약본이라 할 수 있습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;서론&quot;&gt;서론&lt;/h1&gt;

&lt;p&gt;이 글에서는 딥 러닝의 기본적인 개념, Logistic Regression, 활성화 함수, 가중치 업데이트 등을 알고 있다고 가정하고 진행하도록 하겠다.&lt;/p&gt;

&lt;p&gt;CNN(Convolutional Neural Network)는 이미지 인식에 특별히 아주 좋은 성능을 보이는 딥 러닝의 네트워크 중 하나이다. 일단 개념을 정리하는 정도로 이 글을 마무리 짓겠다.(python, pytorch, tensorflow등 기타 framework는 다음 글에서 다루는 걸로)&lt;/p&gt;

&lt;h1 id=&quot;합성곱-신경망cnn&quot;&gt;합성곱 신경망(CNN)&lt;/h1&gt;

&lt;h2 id=&quot;전체-구조&quot;&gt;전체 구조&lt;/h2&gt;

&lt;p&gt;처음에 배웠던 구조는 Fully Connected Neural Net 구조였다. 모든 뉴런이 인접하는 계층의 모든 뉴런과 결합되어 있었다. (책에서는 완전히 연결된 계층을 Affine 계층이라 부른다) 그래서 그 전에서 배운 구조는 Affine 계층 + 활성화 함수 계층이 짝을 이뤄 레고 블록처럼 쌓이는 구조다. 그럼 CNN은 어떻게 다른가? Convolution Layer와 Pooling Layer가 우리가 알고 있는 구조에 새롭게 등장한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cnn-1-1.png&quot; alt=&quot;cnn-1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그래서 한 계층은 Conv - Relu - (Pooling)의 흐름으로 연결된다.&lt;/p&gt;

&lt;h2 id=&quot;convolution-layer&quot;&gt;Convolution Layer&lt;/h2&gt;

&lt;p&gt;Affine 계층로 딥러닝 구조를 구축하면 데이터의 어떤 성질은 무시될 수 밖에 없다. 그게 뭐냐면, &lt;strong&gt;데이터의 형상!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;입력 데이터가 3차원 데이터, 이미지라고 가정하면, Affine계층에 입력하고자 하면 3차원 데이터를 평평한 1차원 데이터로 flatten시켜줘야 한다. 특히 이미지 데이터에는 각각이 공간적 정보를 담고 있는데, 이게 다 무시되는 것이다. 예를 들어 가까운 픽셀은 값이 비슷하거나, 거리가 먼 픽셀은 연관관계가 없거나 등. 데이터에 있는 본질적인 패턴이 있을 것이다. 근데 이걸 다 무시하니, 형상에 담긴 고유한 특성을 다 버리게 되는 것.&lt;/p&gt;

&lt;p&gt;그러면 이 특성을 살리는 딥 러닝 구조를 만들 수 있을까? 이때 등장하는 것이 Convolution Layer. 합성곱(Convolution)이 뭐하냐면, 이런 걸 하는 거다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cnn-1-2.png&quot; alt=&quot;cnn-2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;맨 왼쪽에 있는 입력 데이터에 필터를 적용해서 출력 Feature map을 만들어 낸다. 데이터의 특성 측면에서 살펴보면 입력 데이터도 세로&lt;em&gt;가로 방향의 형상, 필터 역시 세로&lt;/em&gt;가로, 그리고 출력 데이터 또한 가로*세로의 형상을 가진다. 뭔가 공간적 정보가 유지되는 느낌이 들지 않는 가? 🌀(최면 걸기)&lt;/p&gt;

&lt;p&gt;그리고 저 곱해 지는 저 필터를 우리는 &lt;strong&gt;kernel&lt;/strong&gt; 이라 부르기도 한다. 합성 곱 연산은 필터를 stride라 하는 간격을 두고 이동해 가며 입력 데이터에 적용한다. (행렬 곱 하는 거다)&lt;/p&gt;

&lt;p&gt;그럼 우리가 이미 알고 있는 weight와 bias가 뭐지? weight는 바로 필터의 매개 변수다! 저것들을 back propagation을 통해 update하면 되는 거다. 그리고 bias도 있다. (있는 건 다 있는..) bias는 필터를 적용한 후 데이터에 더해진다. 그리고 &lt;strong&gt;bias는 필터 당 하나&lt;/strong&gt;만 존재한다.&lt;/p&gt;

&lt;h2 id=&quot;패딩padding&quot;&gt;패딩(padding)&lt;/h2&gt;

&lt;p&gt;👀 프로그래머라면 패딩 많이 들어보지 않았습니까!? CNN에서의 padding은 직관적으로 알듯이, 없는데 옆에 채워주는 거다. (어휘력이..) 그림 보면 알겠지.&lt;/p&gt;

&lt;p&gt;입력 데이터 주위에 0을 채운다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cnn-1-3.png&quot; alt=&quot;cnn-3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;padding 1을 주면, (4,4)였던 입력 데이터가 (6,6)이 된다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️근데, 패딩 왜 필요해? 패딩은 주로 출력 크기를 조정할 목적으로 사용된다. 예를 들어서 (4,4) 입력 데이터에 (3,3) 필터를 적용하면 출력은 (2,2)가 되어서 입력보다 2만큼 줄어든다. 합성 곱을 여러번 되풀이하는 Deep Neural Net에서는 문제가 될 수 있다. 합성 곱 한번 거칠때마다 크기가 이렇게 줄어들면, 어느 시점에선 출력 크기가 1이 되어 버리고 연산을 적용할 수 없겠지? 그래서 padding을 사용한다. 위 그림을 보면 입력 데이터 (4,4)가 출력 데이터 (4,4)가 되어 공간적 크기가 고정되어 다음 계층에 전달될 수 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;스트라이드stride&quot;&gt;스트라이드(Stride)&lt;/h2&gt;

&lt;p&gt;필터 적용하는 위치 간격을 stride라고 한다. 1이면 한 칸씩 옮겨가며 필터를 적용하고, 2면 두 칸씩 옮겨가며 필터 적용.&lt;/p&gt;

&lt;h2 id=&quot;3차원-데이터의-합성-곱-연산&quot;&gt;3차원 데이터의 합성 곱 연산&lt;/h2&gt;

&lt;p&gt;3차원 데이터에 자주 쓰인다는 CNN. 그럼 3차원 데이터일 땐 어떻게 합성 곱 연산을 하느냐? 입력 데이터가 (4,4,3) = (4,4)&lt;em&gt;3 이면 필터도 (3,3)&lt;/em&gt;3 가 있어 각 차원 마다 합성 곱을 하면 된다. 그 결과를 모두 더해 출력 데이터가 형성 된다. 출력 데이터는 (2,2)*1 이다.&lt;/p&gt;

&lt;h2 id=&quot;pooling-layer&quot;&gt;Pooling Layer&lt;/h2&gt;

&lt;p&gt;풀링은 세로 * 가로 방향의 공간을 줄이는 연산이다. 하나로 집약해서 공간 크기를 줄여 버린다. 특히 많이 쓰이는 것은 Max Pooling. Max pooling은 최대값을 구하는 연산으로, stride가 2라면 2x2 크기의 윈도우에서 가장 큰 값만 취해서 출력 데이터를 형성한다.&lt;/p&gt;

&lt;p&gt;풀링 레이어의 특징은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;학습해야 할 매개변수가 없다 🥺&lt;/li&gt;
  &lt;li&gt;채널 수가 변하지 않는다 (입력된 고대로~ 출력도)&lt;/li&gt;
  &lt;li&gt;입력의 변화에 영향을 적게 받는다&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;마무리&quot;&gt;마무리&lt;/h1&gt;

&lt;p&gt;일단 가장 기초적인 CNN 구조에서 사용 되는 개념을 알아보았으니 다음 글에선 python이나 pytorch로 구현해보도록 한다. 그리고 내가 책에서 가장 시간 투자를 많이 한.. 정말 정말 모르 겠던 개념도 같이. 🤧(하나 이해 안돼서 절대 다음 장으로 넘어 가질 못했다.)&lt;/p&gt;</content><author><name>JJIONI</name></author><category term="cnn" /><summary type="html">딥러닝 CNN 기초 개념 뽀개기</summary></entry><entry><title type="html">python의 process와 thread</title><link href="http://localhost:4000/python/python-thread-process/" rel="alternate" type="text/html" title="python의 process와 thread" /><published>2020-09-20T00:00:00+00:00</published><updated>2020-09-20T06:11:00+00:00</updated><id>http://localhost:4000/python/python-thread-process</id><content type="html" xml:base="http://localhost:4000/python/python-thread-process/">&lt;blockquote&gt;
  &lt;p&gt;python 의 병렬 처리, process와 thread에 대해 알아보자. 그리고 python의 특별한 GIL도.. 🙄&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;process-와-thread&quot;&gt;Process 와 Thread&lt;/h1&gt;

&lt;p&gt;아주 기초적인 process와 thread의 차이점은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;process : process란 메모리에 적재되어(load) 실행되고 있는 프로그램이다.&lt;/li&gt;
  &lt;li&gt;thread : Thread는 process내에서 자원, memory map을 공유하며 동작하는 흐름의 단위이다. 자원을 공유하기 때문에 race condition과 같은 상황(=예를 들어 A thread가 한 전역 변수에 write하려고 하고 B thread도 동일한 전역 변수를 write하려고 할 때. 전역 변수를 예시로 들었으나 여러 가지 모든 자원이 될 수 있다.) 이 생기기 때문에, thread는 lock &amp;amp; unlock, acquire &amp;amp; release하는 동기화가 필수적이다. &lt;em&gt;동기화 잘못하면 abort나면서 그 프로세스가 죽거나, 어쩔 땐 죽어도 release되지 않아 다음 동작이 실행 안되는 등…. 하여튼 이건 해보면 잘 알듯 😨어쨌든 잘 써야 한다.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;process는 메모리를 새로 할당 받기 때문에 (=각자가 고유한 메모리 맵을 가진다) thread와는 달리 메모리를 많이 사용하며, thread는 light하지만 python 구조 때문에 CPU bound task에서는 multi thread을 이용하는 효과를 못 볼 수 도 있다.&lt;/p&gt;

&lt;p&gt;기본적으로 일 할때 pthread를 많이 사용하곤 하는데 (RTOS 어플리케이션이라 1 프로세스의 멀티 쓰레딩 환경이다. 😐) python은 process / thread를 어떻게 사용하는 지 궁금.&lt;/p&gt;

&lt;h1 id=&quot;python의-threading&quot;&gt;python의 threading&lt;/h1&gt;

&lt;p&gt;python의 threading을 말하려면 꼭 짚고 넘어가야 하는 개념이 있다. 바로 &lt;strong&gt;GIL&lt;/strong&gt;이 그것이다. GIL은 Global Interpreter Lock의 줄임말이다.&lt;/p&gt;

&lt;h2 id=&quot;gilglobal-interpreter-lock&quot;&gt;GIL(Global Interpreter Lock)&lt;/h2&gt;

&lt;p&gt;그럼 GIL이 대체 뭐냐! Python interpreter가 한 thread만, 하나의 bytecode만 실행할 수 있도록 하는 것이다. Global하게 적용되는 lock인 것이다. 😶(당황) GIL은 하나의 thread에게 모든 자원의 점유를 허락하여, 다른 thread는 자원을 acquire하기 전에는 아예 실행조차 될 수 없다. 여기서 당연히 드는 의문. ❓아니 그러면 basically 멀티 쓰레딩이 아니잖아!!&lt;/p&gt;

&lt;p&gt;그 의문점을 가지고(워워..), 왜 python이 GIL을 쓰는지 이유나 들어보자. 일단 알아야 할 것은 python의 모든 것은 객체이다. (여기서 C만 쓰는 나로서는 약간 이해 안되기 시작. 애초에 언어 구조가 다르게 생겨서.. ) 각 Python object는 GC를 위해 reference count를 가지고 있다. reference count는 병렬 프로그래밍에서 흔히 접할 수 있는 (아까 내가 얘기한) 공유 자원 접근 시의 race condition문제를 겪을 수 있다. 그러면 python은 각 개체가 reference count를 접근 할 때마다 개별 mutex를 활용해서 thread-safe하도록 만들기에는 성능 상 손해, deadlock의 위험이 있는 것. 그래서 Interpreter를 그냥 한 thread돌 때 잠궈 버렸다. 🔒GIL로 하나의 thread가 interpreter의 모든 것을 가져간다. GIL때문에 다른 언어와 같이 multi threading 프로그래밍은 제한되었으나 single threading환경에서의 성능은 좋다고 한다. (성능 비교는 안 해봐서 모르지만)&lt;/p&gt;

&lt;h2 id=&quot;그럼-왜-python에서-thread를-써&quot;&gt;그럼 왜 python에서 thread를 써?&lt;/h2&gt;

&lt;p&gt;이전까지만 읽으면 python 어차피 single thread 환경이라는 건데 왜 여러 thread를 사용하는 지 의문이 든다.  (나 또한..) 그런데 좀만 더 생각해보면 python에서도 multi thread로 구현하면 좋을 케이스가 있다. 프로그램이 하는 일을 생각해 봤을 때 CPU bound task와 I/O bound task로 나눌 수 있다. CPU bound task는 CPU에게 일을 많이 시키는 일이고, I/O bound task는 I/O와 관련된 일인데 입출력, 소리내기 등이 있을 수 있겠다. CPU bound task를 python에서 multi thread로 구현하면 느려진다. 왜냐하면 thread간 lock acquire, release하는 과정에서 context switching 등 오버헤드가 발생하기 때문이다. 차라리 single thread로 만드는 편이 성능이 좋다. 하지만 I/O bound task는 대부분의 시간을 I/O에 할애하기 때문에 그동안은 CPU가 논다! Thread가 interpreter를 점유하다가 I/O작업이 발생하면 interpreter 점유를 release하기 때문에 I/O bound task에서는 multi thread를 쓰는 의미가 있다.&lt;/p&gt;

&lt;h1 id=&quot;python의-process&quot;&gt;python의 process&lt;/h1&gt;

&lt;p&gt;그럼 이번에는 python의 multi processing에 대해 알아보도록 한다. python은 interpreter 언어로서 순차적으로 동작하기 때문에, 병렬 프로그래밍을 위해선 별도의 모듈인 multi processing모듈을 사용해야 한다. multi processing 모듈을 사용하면 process를 생성하기 때문에 효과적으로 GIL을 피할 수 있다. (python개발자들이 GIL때문에 많이 힘들어 한 듯😂)&lt;/p&gt;

&lt;p&gt;예를 들면 multi processing 모듈을 사용하여 이런 식으로 구현 하면 된다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;multiprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Process&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Queue&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;work&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'__main__'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Queue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Process&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;work&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;# 생략
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;마무리&quot;&gt;마무리&lt;/h1&gt;

&lt;p&gt;아직 까진 python 에서 프로세싱 이나 threading을 사용할 일이 없었는데 머신 러닝이나 딥 러닝 오픈소스에서 데이터 load나 feature abstract 부분에서 사용하는 것을 볼 수 있었다. 앞으로는 많이 사용하게 될테니 연습해둬야지! 👶🏽&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;reference&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://monkey3199.github.io/develop/python/2018/12/04/python-pararrel.html&quot;&gt;[Python] 파이썬 멀티 쓰레드(thread)와 멀티 프로세스(process)&lt;/a&gt;&lt;/p&gt;</content><author><name>JJIONI</name></author><category term="python" /><summary type="html">GIL이 뭐야!</summary></entry><entry><title type="html">IBM Cloud - auto AI 실습</title><link href="http://localhost:4000/ibmcloud/ibmcloud-autoai/" rel="alternate" type="text/html" title="IBM Cloud - auto AI 실습" /><published>2020-09-20T00:00:00+00:00</published><updated>2020-09-20T06:11:00+00:00</updated><id>http://localhost:4000/ibmcloud/ibmcloud-autoai</id><content type="html" xml:base="http://localhost:4000/ibmcloud/ibmcloud-autoai/">&lt;blockquote&gt;
  &lt;p&gt;Auto AI 라고 하는 IBM Cloud watson이 선보이는 AutoML 서비스의 개념에 대해 알아보고, 사용 해서 배포까지 해보도록 합니다. 😎 전체적인 모든 내용은 &lt;a href=&quot;#[]()&quot;&gt;아주 친절한 유투브&lt;/a&gt; 를 참고했습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;준비물&quot;&gt;준비물&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;IBM Cloud 계정&lt;/li&gt;
  &lt;li&gt;Dataset&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ML이니 Dataset이 필요하겠죠? 무료인 kaggle에서 제공된 dataset을 이용해봅니다. 아래 링크에서 insurance.csv 파일을 이용할 겁니다. 여러 feature를 사용해서 보험 비용을 예측하는 Data입니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.kaggle.com/noordeen/insurance-premium-prediction&quot;&gt;Insurance Premium Prediction&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;demo-project&quot;&gt;Demo Project&lt;/h1&gt;

&lt;h2 id=&quot;auto-ai-experiment&quot;&gt;Auto AI Experiment&lt;/h2&gt;

&lt;p&gt;일단 차근차근 또 따라가 봅시다.&lt;/p&gt;

&lt;p&gt;IBM watson studio를 하나 만들고(이전 포스팅에 있어요!), New Project &amp;gt; empty project를 생성합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-1.png&quot; alt=&quot;autoai-1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아까 kaggle 사이트에서 다운받은 Data를 업로드합니다.&lt;/p&gt;

&lt;p&gt;별거 없습니다. Add to project &amp;gt; Data선택해서 다운받은 insurance.csv 파일 업로드 해주면 돼요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-2.png&quot; alt=&quot;autoai-2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 우린 이제 본격적으로 AutoAI experiment를 해볼겁니다. Add to Project에서 Auto AI experiment를 클릭해봅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-3.png&quot; alt=&quot;autoai-3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그러면 이렇게 뭐라 뭐라 뜹니다.&lt;/p&gt;

&lt;p&gt;Watson Machine Learning Instance랑 연결해줘야 하는 모양이니 저 파란색 링크를 타고 들어가서 New Service &amp;gt; Machine Learning 서비스를 만들고 associate해주면 됩니다.  만들고 Reload를 누르세요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-4.png&quot; alt=&quot;autoai-4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그러면 이런 페이지가 뜹니다. 🥳Data에는 Select from project눌러서 우리가 아까 올렸던 insurance data를 선택하세요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-5.png&quot; alt=&quot;autoai-5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그러고 나면 요런 페이지가 뜨면서 configure해달라고 나옵니다. 여기가 중요합니다!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What do you want to predict?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;우리 Dataset은 label되어 있기 때문에 supervised learning이고, correct answer이 있기 때문에 우리가 target으로 삼는 column명을 잘 선택 해줘야 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-6.png&quot; alt=&quot;autoai-6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;target을 expenses로 설정하고 나면 제일 기본적인 머신러닝 모델을 이렇게 보여주는데, experiment setting에서 바꿀 수 있어요. 어떤 feature를 넣고 뺄건지도 정해서 experiment해볼 수 있고요. (좋다..🤭) 물론 metric도 정할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-7.png&quot; alt=&quot;autoai-7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-8.png&quot; alt=&quot;autoai-8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Regression 모델 중에서 이러한 머신러닝 알고리즘을 다 돌려보고 이중 제일 좋은 걸 찾을 수 있다는 거에요. 다 해봅시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-9.png&quot; alt=&quot;autoai-9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;선택하고 싶은 대로 선택하고 Run Experiment를 클릭합니다.&lt;/p&gt;

&lt;p&gt;🥺진짜 너무 세상 좋네요. 한 5분 ~ 10분 정도 걸립니다.&lt;/p&gt;

&lt;p&gt;하고 있는 중간 화면을 캡쳐 해봤어요. 아마 3-fold로 training data를 만들어서 사용한다는 거겠죠?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-10.png&quot; alt=&quot;autoai-10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;좀 더 지나니 pipeline이 만들어진 걸 알 수 있습니다. 아니 이쯤 되니까.. 내가 왜 ML 알고리즘을 공부해야 하는지 생각이 드네요. 😂머신러닝이 알아서 제일 좋은 걸로 골라주는 세상에..&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-11.png&quot; alt=&quot;autoai-11&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오우 👀 밑에 리더보드를 보면 실시간으로 제일 좋은 결과를 내는 ML 알고리즘이 막 랭킹되고 있어요. 실시간으로 막 생겨나는 중입니다. 지금 1위는 Gradient Boosting regression이네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-12.png&quot; alt=&quot;autoai-12&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오 끝났네요. 🎉&lt;/p&gt;

&lt;p&gt;지금 보면 1등이 바꼈어요. 마지막 8번 Pipeline의 Random Forset Regressor이 가장 적은 RSME를 기록했네요. 그럼 제일 성능이 좋았던 이 모델을 deploy해볼까요? 일단 deploy할 모델을 프로젝트에 save 해둡니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-13.png&quot; alt=&quot;autoai-13&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그러면 우리 프로젝트에 Model 이 생기며 이런 화면이 뜰거에요. Deploy를 해야 하는데 Deployment Space가 없네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-14.png&quot; alt=&quot;autoai-14&quot; /&gt;&lt;/p&gt;

&lt;p&gt;메뉴에서 Create Deployment Space를 찾아 생성해줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-15.png&quot; alt=&quot;autoai-15&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 Deployment에 우리가 만든 ML instance를 연결 시켜줘야 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-16.png&quot; alt=&quot;autoai-16&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 다시 모델 페이지로 돌아가서 Promote to Deploy Space를 눌러 연결 해줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-17.png&quot; alt=&quot;autoai-17&quot; /&gt;&lt;/p&gt;

&lt;p&gt;후에 Deployment Space에 들어가 연결되어 있는 모델을 또 Create a deployment를 눌러 줍니다. Online으로 한번 배포를 해볼게요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-18.png&quot; alt=&quot;autoai-18&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hooray~ 제가 만든 모델(?)이 (AutoAI가 만든 모델..) 배포가 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-19.png&quot; alt=&quot;autoai-19&quot; /&gt;&lt;/p&gt;

&lt;p&gt;들어가면 API reference정보도 있고(→나중에 web app으로 만들 수도 있는!), Test할 수도 있게 되어 있네요. 간단하게 테스트를 해보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-20.png&quot; alt=&quot;autoai-20&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오호 요런 결과가 나왔네요. 6246 달러 정도?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/autoai-21.png&quot; alt=&quot;autoai-21&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;마무리&quot;&gt;마무리&lt;/h1&gt;

&lt;p&gt;다음에는 Deploy 된 모델 api 를 활용하여 간단한 flask 기반 web app을 만들어 봐야겠어요. 😎그리고 내가 본능적으로(?) 선택한 모델과 auto AI 가 선택한 모델이 같은 지 보는 식의 테스트도 재밌을 것 같네요.&lt;/p&gt;</content><author><name>JJIONI</name></author><category term="autoML" /><summary type="html">autoAI 를 사용하여 최적의 모델 찾기</summary></entry><entry><title type="html">IBM Watson을 이용한 간단한 NLP 실습</title><link href="http://localhost:4000/nlp/ibm-nlp-for-beginners/" rel="alternate" type="text/html" title="IBM Watson을 이용한 간단한 NLP 실습" /><published>2020-09-18T00:00:00+00:00</published><updated>2020-09-18T06:11:00+00:00</updated><id>http://localhost:4000/nlp/ibm-nlp-for-beginners</id><content type="html" xml:base="http://localhost:4000/nlp/ibm-nlp-for-beginners/">&lt;blockquote&gt;
  &lt;p&gt;IBM Watson Studio 를 이용해서 간단한 NLP 프로그램을 만들어봤습니다. 자연어 처리 한번 해보고 싶었는데, 자연어 처리하면 또 IBM Watson 아니겠어요! NLP에 대한 이해는 아주 적지만 친절한 외쿡 블로그를 따라 하나씩 작성해보았습니다.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;https://medium.com/analytics-vidhya/getting-started-with-notebooks-in-ibm-watson-nlu-part-1-3b0b92894901&quot;&gt;참고한 친절한 외쿡 블로그 가기&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;과연 성공할런지.. 👶🏽&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;-table-of-contents&quot;&gt;📌 Table of Contents&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#뭐-할건가요&quot;&gt;뭐 할건가요?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#실습-시작&quot;&gt;실습 시작&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;뭐-할건가요&quot;&gt;뭐 할건가요?&lt;/h1&gt;

&lt;p&gt;Review DataSet을 IBM Cloud Storage에 올리고, Data를 내가 구현한 노트북에서 좀 변형하고, Watson’s API를 사용하여 Review Dataset에서 감정을 추출해 낼 것입니다. Positive review인지, Negative Review인지 말이죠!&lt;/p&gt;

&lt;h1 id=&quot;실습-시작&quot;&gt;실습 시작&lt;/h1&gt;

&lt;p&gt;이번 실습은 &lt;a href=&quot;https://www.ibm.com/cloud/watson-studio&quot;&gt;IBM Watson Studio&lt;/a&gt;를 사용할 것이기 때문에, IBM Cloud 및 Watson Studio에 가입되어 있다고 전제하고 시작합니다.&lt;/p&gt;

&lt;p&gt;IBM Cloud ID로 로그인하니 뭔가 멋진 그림이 보입니다. 제가 아무 서비스도 사용하지 않아서 뜨는 것 같군요. 🐣&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918223858039.png&quot; alt=&quot;image-20200918223858039&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Service Catalog에서 Natural Language Understanding을 누르고, 서비스를 만들어 봅니다.&lt;/p&gt;

&lt;p&gt;참고로 Lite는 한 계정 하나만 사용할 수 있어서, Lite 플랜을 사용하고 있는 앱은 중지해야 합니다. 😨&lt;/p&gt;

&lt;p&gt;짠! 잘 만들면, 요렇게 뜹니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918224924851.png&quot; alt=&quot;image-20200918224924851&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 우리는 Watson NLU 서비스의 API 를 호출할 때 필요한 credentials이 필요해요. 우리가 만든 서비스에 들어가 New Credential을 누릅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918225343790.png&quot; alt=&quot;image-20200918225343790&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 credential key를 메모장이라든지 잘 보관해두세요. 이름 쓰라고 뜨는데 원하는 이름 쓰면, api key같은 것들이 잔뜩 써진 credential이 생성됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918225811817.png&quot; alt=&quot;image-20200918225811817&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 우리는 api_key 와 url endpoint를 사용할 거에요.&lt;/p&gt;

&lt;p&gt;이제 우리 서비스가 만들어졌으니, NLP Project를 빌드해봅시다!&lt;/p&gt;

&lt;p&gt;New project &amp;gt; Create an empty project를 눌러 빈 프로젝트를 생성합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918230057367.png&quot; alt=&quot;image-20200918230057367&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그러고 나면 디테일한 정보들을 입력하라고 하고, Storage를 추가하라고 할거에요. Cloud Object Storage를 사용할건데, IBM Cloud object storage를 Lite플랜으로 만들면 됩니다. 👏🏻&lt;/p&gt;

&lt;p&gt;그러면 이제 project를 생성할 수 있어요.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;…. 프로젝트 만드는 중….&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918230251238.png&quot; alt=&quot;image-20200918230251238&quot; /&gt;&lt;/p&gt;

&lt;p&gt;뭔가 만들어지긴 했습니다. 이 대시보드에서 Add to Project를 클릭합니다. 이제 우리 notebook을 여기에 연결할 거에요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918230323216.png&quot; alt=&quot;image-20200918230323216&quot; /&gt;&lt;/p&gt;

&lt;p&gt;⛔️ 아니.. 왜 저는… Available asset types에 Notebook 이 없죠..? 난관 봉착.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918233008279.png&quot; alt=&quot;image-20200918233008279&quot; /&gt;&lt;/p&gt;

&lt;p&gt;⚠️ watson studio 앱이 생성되어 있지 않아서 였습니다. 아래 페이지에서 상세하게 설명해주고 있으니, 확인해보세요! 서비스 카탈로그에서 watson studio free tier로 생성만 하면 바로 해결되긴 합니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.ibm.com/support/pages/options-notebook-model-and-so-are-missing-add-project-watson-studio&quot;&gt;Notebook is missing. How to fix ?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;휴, 이제 다시 노트북을 생성할 수 있으니 생성해봅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918234751425.png&quot; alt=&quot;image-20200918234751425&quot; /&gt;&lt;/p&gt;

&lt;p&gt;노트북이 정말 더럽게 안만들어지네요. 공지 보니 9/18일 Dallas 지역 뭐가 안된다고 써있는 것 같긴 한데…🤬 계속 Start 를 실패하고, Load실패하고… 난리 부르스.&lt;/p&gt;

&lt;p&gt;드디어 성공. 하여튼 이제부터 코드를 작성하면 되는데, 일단 이번엔 watson nlu api를 써보는게 목적이므로 친절한 외국 블로그의 코드를 따라 해보겠습니다.&lt;/p&gt;

&lt;p&gt;앗, 일단 코딩 하기전에 Data Set을 업로드 합니다. Add to Project &amp;gt; Data를 누르고, 파일을 drag&amp;amp;drop하면 됩니다.&lt;/p&gt;

&lt;p&gt;당연히 Dataset은 저 외국블로그에서 다운받으면 되겠죠~🙄&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918230905876.png&quot; alt=&quot;image-20200918230905876&quot; /&gt;&lt;/p&gt;

&lt;p&gt;자 이제 다시 코딩 해보겠습니다.&lt;/p&gt;

&lt;p&gt;노트북에서 우리가 넣은 Dataset을 불러오고, transform하고 해야되는데 노트북안에서 Data에 우리가 넣은 tokenised_dataset.csv 파일을 쉽게 자동으로 insert할 수 있습니다. pandas Dataframe으로 넣어볼게요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200919015356075.png&quot; alt=&quot;image-20200919015356075&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그러면 빈 셀에 자동이로 이런 코드가 작성됩니다. 정말 편리한 세상…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200919015609159.png&quot; alt=&quot;image-20200919015609159&quot; /&gt;&lt;/p&gt;

&lt;p&gt;실행하면 우리가 넣은 dataset을 확인할 수 있어요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200919015641406.png&quot; alt=&quot;image-20200919015641406&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 이 Dataset 문장에서 sentiment 를 추론해야겠죠? 이때 Watson의 &lt;strong&gt;NLU API&lt;/strong&gt;를 활용할 겁니다.&lt;/p&gt;

&lt;p&gt;사용자 리뷰 데이터에서 Watson이 제공하는 요런 옵션들을 사용할 수 있습니다. Features, EntitiesOptions, KeywordsOptions, SentimentOptions.&lt;/p&gt;

&lt;p&gt;우리는 EntitiesOptions와 SentimentOptions를 활용합니다. 이때 우리가 초반에 만들었던 Credential의 API key와 url endpoint를 사용합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ibm_watson&lt;/span&gt; 
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;ibm_watson&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NaturalLanguageUnderstandingV1&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;ibm_watson.natural_language_understanding_v1&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EntitiesOptions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KeywordsOptions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SentimentOptions&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;ibm_cloud_sdk_core.authenticators&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IAMAuthenticator&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;authenticator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IAMAuthenticator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Authentication via external config like VCAP_SERVICES
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;service&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NaturalLanguageUnderstandingV1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'2018-03-16'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;authenticator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;authenticator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# service.set_disable_ssl_verification(True)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_service_url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'XXXXXXXXXXXXXXXXXXXXXXXX'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이제 service를 불러왔으니, sentiment와 keyword 추출하는 함수를 만들어봅니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sentiment_and_keyword&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;analyze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keywords&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;KeywordsOptions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentiment&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;emotion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;limit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentiment&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SentimentOptions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
  
&lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sentiment_keyword_json'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentiment_and_keyword&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sentences'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;확인해보면 이런식으로 동작합니다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Absolutely wonderful - silky and sexy and comfortable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;요 sentence를 watson service에 analyze를 부탁하면,  이런 대답이 나옵니다. 99%가 positive라는 것입니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{'usage': {'text_units': 1, 'text_characters': 53, 'features': 2},
 'sentiment': {'document': {'score': 0.998302, 'label': 'positive'}},
 'language': 'en',
 'keywords': []}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;중간에 클라우드 메뉴 같은 것들이 익숙치 않아 헤매긴 했으나 성공적으로 실습을 마쳤습니다. 😎 (뿌듯 뿌듯)&lt;/p&gt;

&lt;p&gt;Watson의 노트북으로 여러가지 API 를 활용해서 서비스할 수도 있겠네요… (아주 먼 훗날에..)&lt;/p&gt;

&lt;p&gt;일단 다음에는 AutoAI 쪽 공부를 해보고 싶습니다. 후후&lt;/p&gt;</content><author><name>JJIONI</name></author><category term="NLP, ibmcloud" /><summary type="html">초보자를 위한 NLU 튜토리얼 따라하기</summary></entry><entry><title type="html">IBM Cloud Foundry 로 node js 간단 배포하기</title><link href="http://localhost:4000/%EC%9B%B9/ibm-cloudfoundry/" rel="alternate" type="text/html" title="IBM Cloud Foundry 로 node js 간단 배포하기" /><published>2020-09-17T00:00:00+00:00</published><updated>2020-09-17T06:11:00+00:00</updated><id>http://localhost:4000/%EC%9B%B9/ibm-cloudfoundry</id><content type="html" xml:base="http://localhost:4000/%EC%9B%B9/ibm-cloudfoundry/">&lt;blockquote&gt;
  &lt;p&gt;webRTC 프로젝트 시작하기 전, node js로 간단한 webrtc 프로그램을 만들고 ibm cloud foundry로 배포해봤습니다. cli로 뚝딱 되어버려 너무 신기해요.🤔&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;-table-of-contents&quot;&gt;📌 Table of Contents&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#서론&quot;&gt;서론&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ibm-cloud-foundry란&quot;&gt;IBM Cloud Foundry란?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ibm-cloud-foundry-로-배포&quot;&gt;IBM Cloud Foundry 로 배포&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reference&quot;&gt;reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;서론&quot;&gt;서론&lt;/h1&gt;

&lt;p&gt;이 글은 간단한 WebRTC Web App을 IBM Cloud Foundry를 사용하여 배포해보고자 한다. 생각보다 매우 간단!&lt;/p&gt;

&lt;p&gt;그리고 WebRTC Web App은 기존에 있는 오픈소스를 활용하겠다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/agilityfeat/webrtc-video-conference-tutorial/tree/webrtc&quot;&gt;WebRTC Basic App&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;로컬 컴퓨터에서 테스트하면 아래처럼 동작한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200917233853509.png&quot; alt=&quot;image-20200917233853509&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Room Number를 써서 화상 채팅방 만들면 된다. 나는 1을 넣어보겠다.&lt;/p&gt;

&lt;p&gt;숫자 넣고 Go하면 내 화면이 뜬다.  짜잔! 방을 만든 썬구리 얼굴이 뜬다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918004209456.png&quot; alt=&quot;image-20200918004209456&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 내가 여기서 다시 localhost:3000을 들어가서 내가 입력했던 채팅방 room number 1로 들어가면, 이제 화상 채팅이 가능해진다.&lt;/p&gt;

&lt;p&gt;요렇게!&lt;/p&gt;

&lt;p&gt;썬구리와 윙키가 이제 화상채팅에 성공한 것.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200917234747302.png&quot; alt=&quot;image-20200917234747302&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사실은 이게 중요한게 아니라 오늘의 글은 이 간단한 프로그램을 IBM Cloud Foundry를 사용하여 손쉽게 배포해보도록 하는 것이다.&lt;/p&gt;

&lt;h1 id=&quot;ibm-cloud-foundry란&quot;&gt;IBM Cloud Foundry란?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://cloud.ibm.com/docs/cloud-foundry-public?topic=cloud-foundry-public-getting-started&quot;&gt;IBM Cloud Foundry&lt;/a&gt;는 클라우드 네이티브 앱을 쉽고 빠르며 안정되게 배치할 수 있는 PaaS(Platform-as-a-Service)이다. Cloud Foundry를 사용하면 코딩의 빌드 및 배치 요소가 연결된 서비스에 맞게 계속 신중하게 조정되므로, 애플리케이션을 빠르고 일관되며 안정되게 반복할 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;ibm-cloud-foundry-로-배포&quot;&gt;IBM Cloud Foundry 로 배포&lt;/h1&gt;

&lt;p&gt;그럼 IBM Cloud Foundry 로 내가 만든 node js 서버를 간단하게 배포해보고자 한다. 따끈따끈하게 새로 진행했으므로 아래 내용을 따라하면 바로 될 것이다. (IBM Cloud에서 lite계정을 사용하면 무료이므로 비용은 걱정하지 않아도 된다.)&lt;/p&gt;

&lt;p&gt;자, 일단 IBM Cloud 대시보드에서 Node JS 리소스를 만든다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;리소스 작성 &amp;gt; Cloud Foundry 앱 &amp;gt; Node js 선택&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200917235511936.png&quot; alt=&quot;image-20200917235511936&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이름 대충 정해서 만들면, 실행 중으로 변하면서 뭔가 시작하는 느낌이 든다. 내 대시보드에 가도 내가 만든 앱이 실행 중인 걸 알 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200917235955171.png&quot; alt=&quot;image-20200917235955171&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cloud Foundry 앱을 사용하기 위해서는 앱의 root 경로에 manifest.yml파일을 만들어 설정해줘야 한다. 아래처럼 하면 되겠다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# manifest.yml&lt;/span&gt;
applications:
  - name: WebRTC Basic App
    random-route: &lt;span class=&quot;nb&quot;&gt;true
    &lt;/span&gt;memory: 64M
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;package.json에는 실행할 명령어를 적는다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# ...
&quot;scripts&quot;: {
    &quot;start&quot;: &quot;node server.js&quot;   
  }
# ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;이제 배포하자&lt;/p&gt;

    &lt;p&gt;Cloud Foundry cli를 사용해야 하기 때문에 IBM Cloud CLI를 설치한다. 이미 내 맥에는 깔아버렸기 때문에 자세한 설명은 &lt;a href=&quot;https://cloud.ibm.com/docs/cli?topic=cli-getting-started&quot;&gt;공식홈페이지&lt;/a&gt;를 참고하세요.&lt;/p&gt;

    &lt;p&gt;ibmcloud login을 CLI로 입력하고, 로그인하면 된다.  지역은 아까 생성한 리소스와 일치시킨다. 나는 댈러스로 선택했으므로 us-south 선택&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ibmcloud login
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918001207345.png&quot; alt=&quot;image-20200918001207345&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ibmcloud target –cf로 해당 directory와 클라우드를 연결하고, ibmcloud cf push로 리소스를 클라우드에 입력시킨다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918001455261.png&quot; alt=&quot;image-20200918001455261&quot; /&gt;&lt;/p&gt;

&lt;p&gt;😅 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ibmcloud cf push&lt;/code&gt;는 생각보다 오래걸리므로 좀 기다립시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918002320370.png&quot; alt=&quot;image-20200918002320370&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918002222784.png&quot; alt=&quot;image-20200918002222784&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아.. 이런.. package.json에 오류가 있어서 실패했네. 로컬에 그래도 한번 돌려보고 push해야하는데.. 😤 어쨌든 오류 수정하고, 다시 ibmcloud cf push하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918002756205.png&quot; alt=&quot;image-20200918002756205&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오우 성공! 이제 그럼 ibm cloud 대시보드를 확인해볼까?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;배포한 앱을 실행해보자&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다시 내 리소스를 들어가서 앱URL방문을 클릭해본다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918002900132.png&quot; alt=&quot;image-20200918002900132&quot; /&gt;&lt;/p&gt;

&lt;p&gt;로컬 컴퓨터에서 돌렸던 것과 똑같이 뜬다! 성공!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918003026418.png&quot; alt=&quot;image-20200918003026418&quot; /&gt;&lt;/p&gt;

&lt;p&gt;화상 채팅까지 성공하는 지 확인해봅시다.&lt;/p&gt;

&lt;p&gt;1번 방을 만들고, 입장&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918003110003.png&quot; alt=&quot;image-20200918003110003&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다시 썬구리가 등장한 것을 볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918003302577.png&quot; alt=&quot;image-20200918003302577&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 다시 1번방을 들어가서 화상 채팅 기능을 테스트해보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200918003516244.png&quot; alt=&quot;image-20200918003516244&quot; /&gt;&lt;/p&gt;

&lt;p&gt;로컬에서 테스트했을 때와 동일하게 동작하는 것을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;🚦 이제 내 로컬 컴퓨터에서 기능 수정하고, 버그 수정하고, 테스트한 다음 ibmcloud cf push만 하면 배포가 가능하다.&lt;/p&gt;

&lt;h1 id=&quot;refrerence&quot;&gt;refrerence&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;https://lovemewithoutall.github.io/it/deploy-on-ibm-cloud/&lt;/li&gt;
&lt;/ul&gt;</content><author><name>JJIONI</name></author><category term="node js, ibm cloud" /><summary type="html">IBM Cloud Foundry로 간단한 node js 배포까지</summary></entry><entry><title type="html">WebRTC로 실시간 영상 통화를 하며 Object Detection을 해보자 - 2탄</title><link href="http://localhost:4000/%EC%9B%B9,%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/webrtc+tensorflow2/" rel="alternate" type="text/html" title="WebRTC로 실시간 영상 통화를 하며 Object Detection을 해보자 - 2탄" /><published>2020-08-20T00:00:00+00:00</published><updated>2020-08-23T06:11:00+00:00</updated><id>http://localhost:4000/%EC%9B%B9,%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/webrtc+tensorflow2</id><content type="html" xml:base="http://localhost:4000/%EC%9B%B9,%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/webrtc+tensorflow2/">&lt;blockquote&gt;
  &lt;p&gt;프로젝트 진행하면서 처음 접한 webrtc.. 실시간으로 영상 채팅하며tensorflow js 모델과 함께 객체 인식을 하는 기능을 구현해보는데… 👀&lt;/p&gt;

  &lt;p&gt;2탄. 대망의 tensorflow js 모델로 object detection 성공해보자&lt;/p&gt;

  &lt;p&gt;왕왕 초보라 틀린 정보가 있을 수 있음을 미리 고지합니다&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;-table-of-contents&quot;&gt;📌 Table of Contents&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#tensorflow-js-모델을-심자&quot;&gt;Tensorflow js모델을 심자&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#tensorflow-js&quot;&gt;Tensorflow js&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#webrtc-+-tensorflow.js&quot;&gt;WebRTC + Tensorflow.JS&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#tensorflowjs-모듈-설치&quot;&gt;Tensorflowjs 모듈 설치&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reference&quot;&gt;reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;tensorflow-js-모델을-심자&quot;&gt;Tensorflow js 모델을 심자&lt;/h1&gt;

&lt;h2 id=&quot;tensorflow-js&quot;&gt;Tensorflow js&lt;/h2&gt;

&lt;p&gt;Tensorflow.js는 자바스크립트 머신러닝 라이브러리다. JS로 ML모델을 개발하고 브라우저 / node.js에서 바로 ML 모델을 사용해볼 수 있다. 🎉 ML관련 웹 서비스에 대한 아이디어가 있다면 바로 활용하기 좋을듯 ! 나름대로 Pretrained된 모델 및 API도 많이 제공해주고 있으니 공식 홈페이지 구경해보면 재밌을 거 같다. 데모도 제공하는데 귀여운 아이디어와 UI로 구성된 페이지들이 많다.&lt;/p&gt;

&lt;p&gt;바로 사용할 수 있는 JS 모델을 node.js에 같은 데 넣어서 사용해도 되고(내가 했던 부분) 혹은 직접 모델을 만들었다면 Python Tensorflow모델을 변환하기도 된다는데, 이건 안해봐서 모르겠다.&lt;/p&gt;

&lt;p&gt;(아래는 Demo 페이지들. 구경해보세요 👀)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/js/demos?hl=ko&quot;&gt;https://www.tensorflow.org/js/demos?hl=ko&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;webrtc--tensorflowjs&quot;&gt;WebRTC + Tensorflow.JS&lt;/h2&gt;

&lt;p&gt;그럼 나는 뭘 하고 싶냐면, 내가 1탄에서 만들었던 WebRTC기반 화상채팅 시스템에다가 남들 웹 캠 stream말고 내 웹 캠에서 나오는 stream에서 object detection을 해 볼 것이다. 추가로 더 하고자하는 건 object detection해서 나오는 정보를 peer에 data를 실어 보내면(?) peer connection을 맺고 있는 나머지 사람들도 내 정보를 받게끔 하고 싶다.&lt;/p&gt;

&lt;p&gt;나는  TensorFlow.JS 측에서 제공해주고 있는 &lt;strong&gt;객체 감지 모델(CoCo SSD)&lt;/strong&gt;을 이용할 것이다. CoCo SSD 가 어떻게 train 됐고 어쩌고 저쩌고는 사실 관심 없고(이미지 관련 머신러닝은 왜케 땡기지가 않는지 모르겠다🤔.. 궁금한 사람은 TensorflowJS 공식 홈페이지나 github repo가면 나름 설명되어 있는것 같으니 참조하세요), 이용해서 내 웹캠에 어떻게 띄우냐가 중요한 부분이니까 빠르게 skip한다.&lt;/p&gt;

&lt;h3 id=&quot;tensorflowjs-모듈-설치&quot;&gt;Tensorflowjs 모듈 설치&lt;/h3&gt;

&lt;p&gt;1탄에서 한거에서 package.json의 dependencies에다가 tensorflow-model cocossd 넣어준다.&lt;/p&gt;

&lt;p&gt;⛔️ 요기서 잠깐 스탑!!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;@tensorflow/tfjs&lt;/strong&gt; 요거 버전 원래 2.0.1 사용하니까, 화면 띄울때 (=React실행되면서..) 텐서플로우 라이브러리가 import되면서 페이지를 몇 번씩 불러오는(=compDidMount 가 두번 이상 불려!!) 거가 아니겠는가 🤬 그래도 기능은 돌아가야 하는데 문제는 webRTC로 인해서 p2p connection 맺어야하니까 자꾸 시그널을 몇번씩 보내는거다.  아주 성가신… 하여튼, 버전을 다시 1.7.4로 하니까 이런 일은 없어졌다. 별 이슈 없다면 1.7.4를 사용하시길~&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;dependencies&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;@tensorflow-models/coco-ssd&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;^2.1.0&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;@tensorflow/tfjs&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;^1.7.4&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그 담에 우리 화면 띄워줄 js 파일에 import 해준다.&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;cocoSsd&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;@tensorflow-models/coco-ssd&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;@tensorflow/tfjs&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이렇게 dependency 추가해주고 npm install 한번 해주면 우리가 사용할 Tensorflow library는 import 완료.&lt;/p&gt;

&lt;h2 id=&quot;모델-활용&quot;&gt;모델 활용&lt;/h2&gt;

&lt;p&gt;이제 내 웹캠 화면을 하나씩 뽑아서 object detect를 해보자. 나는 promise를 사용했음. CompDidMount()내에서 Promise로 내 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;navigator.mediaDevices.GetUserMedia(this.streamConstraints)&lt;/code&gt;가지고 Model를 load해서 처리하는 함수들을 만들었다.&lt;/p&gt;

&lt;p&gt;모델 load는 이렇게 하면 되고,&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;loadlModelPromise&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;cocoSsd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Promise를 사용해서 로드한 cocoSSD 와 mediaDevices의 localVideo stream을 detect하는 함수에 넘겼다.&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;c1&quot;&gt;// resolve all the Promises&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;Promise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;loadlModelPromise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;webcamPromise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;detectFromVideoFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;localVideo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;current&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그리고 실제 모델 가지고 화면으로 predict하는 부분은 여기!&lt;/p&gt;

&lt;p&gt;알다시피… requestAnimationFrame() 이게 1 frame마다 불려서 너무 성능이 구려지기에 120 frame마다 돌아가게 했다. 여기다 timing넣는 방법은 못찾고 구글링하니 다들 저렇게 쓴다 그러길래 그냥 나도 따라 씀.&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;detectFromVideoFrame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;video&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;120&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;120&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;===&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;detect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;video&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;nx&quot;&gt;showDetections&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;Couldn&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;t start the webcam&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;requestAnimationFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;detectFromVideoFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;video&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;model.detect(video)&lt;/strong&gt;하면 predictions 값을 return한다. 어떤 형식 이냐면 아래와 같다. bbox의 화면의 위치들이 나오고, class로 predict한 object의 분류?값이 나온다. 참고로, 얼굴이 있어야 사람으로 잡는 것 같고(당연한가?🙂) 기본 제공되는 모델이다 보니 성능이 미친듯이 좋지는 않다.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;bbox:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;class:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;person&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;score:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8380282521247864&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;bbox:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;class:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;kite&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;score:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.74644153267145157&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그리고 나는 model.detect해서 나오는 predictions 값을 확인하는 로직을 세워서 다른 peer에게 전송해야 되는 값이라면, peer에 data를 실어서 보내줬다. 내가 사용한 simple peer에선 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;peer.send(JSON.stringify(data))&lt;/code&gt; 해주면 되더라!&lt;/p&gt;

&lt;p&gt;받는 peer 쪽에서는 ‘data’로 받아주면 된다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;peer.on('data', data =&amp;gt; { 어쩌고저쩌고 &lt;/code&gt;&lt;/p&gt;

&lt;p&gt;어떤 사람은 네모 박스 만들어서 Video 위 Canvas를 띄워주기도 하는데 나는 tutorial따라하다가 실제로는 안쓰긴 했다. 후후.&lt;/p&gt;

&lt;p&gt;어쨌든 결론은 1:N 화상채팅은 WebRTC로 구현 (1탄에선 N:N이긴 했으나… 1:N은 p2p connection을 1 - N (N끼리는 안하고) 으로만 해주면 되는 심플한 일.. ), 그다음 각 local video를 object detection해서 몇가지 특정 값은 peer에 보내주는 것 까지 구현 성공했다. 자기 webcam stream을 object detection하는 튜토리얼은 많다! 나는 여기 참고했다. 전체 구조랑 다 배웠다;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://nanonets.com/blog/object-detection-tensorflow-js/&quot;&gt;https://nanonets.com/blog/object-detection-tensorflow-js/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;우리 팀이 프로젝트에서 한거는 요거! frontend랑, signaling-server참고하면 된다. 🤟🏼&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/CheatingBusters&quot;&gt;https://github.com/CheatingBusters&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;reference&quot;&gt;reference&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tensorflow/tfjs-models/tree/master/coco-ssd&quot;&gt;https://github.com/tensorflow/tfjs-models/tree/master/coco-ssd&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.tensorflow.org/js/models?hl=ko&quot;&gt;https://www.tensorflow.org/js/models?hl=ko&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://webrtchacks.com/webrtc-cv-tensorflow/&quot;&gt;https://webrtchacks.com/webrtc-cv-tensorflow/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nanonets.com/blog/object-detection-tensorflow-js/&quot;&gt;https://nanonets.com/blog/object-detection-tensorflow-js/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>JJIONI</name></author><category term="tensorflow, javascript" /><summary type="html">webrtc 와 tensorflow js 모델로 object detection해보기</summary></entry><entry><title type="html">IBM Cloud Essentials Badge 획득하기</title><link href="http://localhost:4000/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C/ibm-clouders-notetaking/" rel="alternate" type="text/html" title="IBM Cloud Essentials Badge 획득하기" /><published>2020-08-09T00:00:00+00:00</published><updated>2020-08-09T06:11:00+00:00</updated><id>http://localhost:4000/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C/ibm-clouders-notetaking</id><content type="html" xml:base="http://localhost:4000/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C/ibm-clouders-notetaking/">&lt;blockquote&gt;
  &lt;p&gt;ibm CLouders로 활동하면서 ibm CLoud Essentials 강의 들으며 note taking한 거 공유하는 글이다. 클라우드의 개략적인 개념을 이해할 수 있어서 좋았다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;iaas--paas--saas&quot;&gt;Iaas / Paas / Saas&lt;/h1&gt;

&lt;p&gt;aaS = as a Service, how you consume&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Iaas : Infrastructure. For example, my computer, some random other person’s computer running somewhere else …
    &lt;ul&gt;
      &lt;li&gt;persona of Iaas is “System admin”&lt;/li&gt;
      &lt;li&gt;It’s like a “leasing a car”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Paas : Platform.
    &lt;ul&gt;
      &lt;li&gt;persona is “Developer”&lt;/li&gt;
      &lt;li&gt;renting a car&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Saas : Software
    &lt;ul&gt;
      &lt;li&gt;taking taxi / Uber&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200809084416216.png&quot; alt=&quot;image-20200809084416216&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;cloud-native&quot;&gt;Cloud Native&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://d1fto35gcfffzn.cloudfront.net/images/topics/cloudnative/diagram-cloud-native.png&quot; alt=&quot;Cloud-Native&quot; style=&quot;zoom:33%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Microservice, Container를 Cloud native application으로 개발&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;infrastructure-as-a-service&quot;&gt;Infrastructure as a Service&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Bare Metal Server ?
    &lt;ul&gt;
      &lt;li&gt;A dedicated physical server which is yours to use and manage from the ‘metal up’&lt;/li&gt;
      &lt;li&gt;No sharing of underlying hardware&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;virtual-private-cloud-vpc&quot;&gt;Virtual Private Cloud (VPC)&lt;/h2&gt;

&lt;p&gt;traditional cloud works like this. Network engineers have to do all network system things. routers, VPN, …&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200809092240180.png&quot; alt=&quot;image-20200809092240180&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However «Virtual Networking» , all of these capabilities is given as a service. User can create these functions and the segmentation with a UI or CLI or API without knowing any proprietary interfaces.&lt;/p&gt;

&lt;p&gt;VPC is isolated logical network that you create. this includes Multi Region Zone(MZR), Sequrity Groups , Connectivity. Other services are provided to support VPC like Load Balancing.&lt;/p&gt;

&lt;h2 id=&quot;vmware&quot;&gt;VMWare&lt;/h2&gt;

&lt;p&gt;What IBM cloud gives:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data sovereignty compliance - Geo-Fencing for workloads,  Data doeson’t cross borders&lt;/li&gt;
  &lt;li&gt;Compliance and regulatory control&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;platform-as-a-service&quot;&gt;Platform as a Service&lt;/h1&gt;

&lt;p&gt;There’s a lot of technologies for doing cloud-native transformation.&lt;/p&gt;

&lt;p&gt;Let’s see from the bottom with bare-bones physical infrasturcture.&lt;/p&gt;

&lt;p&gt;legacy applications &amp;gt; VMs &amp;gt; Kubernates &amp;gt; &lt;strong&gt;Cloud Foundry&lt;/strong&gt; &amp;gt; Serverless&lt;/p&gt;

&lt;h2 id=&quot;cloud-foundry&quot;&gt;Cloud Foundry&lt;/h2&gt;

&lt;p&gt;very developer focused approach. increased speed &amp;amp; ease. For example, what developer would do with cloud foundry is like this. Let’s say you developed front application with react. You can deploy app with CLI tool CF(cloud foundry). Cloud Foundry takes your app and runs your app with the environment of cloud. You don’t need to care about security, infrastructure. Then if you are to change your backend application into container, you can use CLI tool &lt;strong&gt;kubectl&lt;/strong&gt; to deploy to cloud. Backend container &amp;amp; front app are running in the same environnment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-20200809224240804.png&quot; alt=&quot;image-20200809224240804&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;container-orchestration&quot;&gt;Container Orchestration&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Before starting, Let’s look at quick explanation of “Container”. A container is a unit of software that contains all the components required to run and application, including all its dependencies, libraries, other binaries and so on.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let’s look at what container orchestration platform like k8s does. It uses “Worker node” just like vm, computer.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Deploy&lt;/li&gt;
  &lt;li&gt;Scaling&lt;/li&gt;
  &lt;li&gt;Network (ex. with scaling, platform should do load-balancing and connect other microservices)&lt;/li&gt;
  &lt;li&gt;Insight (logging, anaylitics, …)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;red-hat-openshift&quot;&gt;Red Hat OpenShift&lt;/h2&gt;

&lt;p&gt;OpenShift is a platform as a service offering from Red Hat, built on Kubernates technology. OpenShift simplifies the use of Kubernetes by taking away many of the decisions that normally need to be made when using Kubernetes. As an example, a developer, can concentrate on creating application code and let OpenShift create all the Kubernetes configuration by using the Source to Image workflow, which takes your code from a git repository, builds the container then deploys it to OpenShift autogenerating all the deployment configuration needed.&lt;/p&gt;

&lt;p&gt;When application or project was created, OpenShift in the backend will create a Jenkins job and pipeline. So all developers need to do is push their code changes to repository(ex. Github).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Developer push changes to repository&lt;/li&gt;
  &lt;li&gt;Webhook is triggered, then kicks off a Jenkins job.&lt;/li&gt;
  &lt;li&gt;Jenkins job will create “Sourced image”, which means creating a Docker image out of that source code.&lt;/li&gt;
  &lt;li&gt;Take this and put it into a private restiry.&lt;/li&gt;
  &lt;li&gt;OpenShift will push this into a cluster.&lt;/li&gt;
  &lt;li&gt;Bring down the old version, and start the new version.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;knatvie&quot;&gt;KNatvie&lt;/h2&gt;

&lt;p&gt;It is the platform that on top of kubernetes, supports function serverless. We have three components on top of kubernetes, BUILD &amp;amp; SERVE &amp;amp; EVENT. With Knative, we can make all build process into one kubernetes cluster.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;https://tanzu.vmware.com/kr/cloud-native&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>JJIONI</name></author><category term="cloud" /><summary type="html">IBM CLOUD essentials 무료강의 듣고 badge 따자!</summary></entry></feed>