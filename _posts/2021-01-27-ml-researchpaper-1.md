---
title:  "Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations"
excerpt: "ë¬´ì‹œë¬´ì‹œí•œ ML ë…¼ë¬¸ ì½ê¸°"

categories:
  - MachineLearning
tags:
  - ë¨¸ì‹ ëŸ¬ë‹, ë…¼ë¬¸
last_modified_at: 2021-01-27T17:42:00-05:00

---

> 2019ë…„ Top 10 ë…¼ë¬¸ì— ë½‘íŒ Common Assumptions in the unsupervised learning of disentangled representations ë…¼ë¬¸ì„ ì‚´í´ë³´ì. ì´ë²ˆ ë…¼ë¬¸ì€ ì œëª© ê·¸ëŒ€ë¡œ Common Assumptionì— ëŒ€í•´ ë„ì „í•˜ëŠ” ë…¼ë¬¸ì´ë‹¤. ê·¸ë˜ì„œ ì´ê±¸ ì´í•´í•˜ë ¤ë©´ ì• ì´ˆì— ê·¸ëŸ¼ ë‚´ê°€ Common Assumptionì´ ìˆì„ ì •ë„ë¡œ ë°°ê²½ ì§€ì‹ì´ ìˆì–´ì•¼ í–ˆê¸° ë•Œë¬¸ì—(...) ë°°ê²½ ì§€ì‹ì„ (ë…¼ë¬¸ì„ ì´í•´í•  ë§Œí¼ë§Œ) ê°„ë‹¨íˆ ì •ë¦¬í•´ë³´ì•˜ë‹¤. ì—¬ê¸°ì„œ ğŸ¥ ë¡œ ì¤‘ê°„ ì¤‘ê°„  ë‚´ ì˜ê²¬, ì°¸ê²¬ë“¤ì„ ì ì–´ë‘ì—ˆë‹¤. ì‹œë„ëŸ¬ìš°ë©´ ì‚´í¬ì‹œ ë¬´ì‹œí•´ì£¼ì„¸ìš”.

# ë°°ê²½ ì§€ì‹

### representation learning

representationì€ ì–´ë–¤ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ë‹¤ë¥¸ ì°¨ì›ì˜ ë°ì´í„° í¬ì¸íŠ¸ë¡œ ë°”ê¾¸ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.

(x1, x2 ..)  â†’ (z1, z2, ...)

ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë‹¤ë¥¸ ì°¨ì›ì— **í‘œí˜„** í•œë‹¤ëŠ” ëœ»ìœ¼ë¡œ ë°›ì•„ë“¤ì´ë©´ ëœë‹¤. ê·¸ë¦¬ê³  `learning`ì´ë‹ˆê¹Œ ê·¸ í‘œí˜„í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë‹¤ë¥¸ ë°ì´í„° í¬ì¸íŠ¸ë¡œ mappingí•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµí•œë‹¤ê³  ë³´ë©´ ë˜ê² ë‹¤.

![MLë„í‘œ](/assets/images/ml-research-1.png)

### Disentanglement

Transforming from an uninterpretable space with entagled features to eigen spaces where features are independent.

*í–‰ë ¬ì˜ ê³ ìœ  ë²¡í„°(eigen vector)ë“¤ì´ í˜•ì„±í•˜ëŠ” ë¶€ë¶„ ê³µê°„ = ê³ ìœ  ê³µê°„(eigenspace)*

ì¡°ê¸ˆ ë” í’€ì–´ ì„¤ëª…í•˜ìë©´, ì•„ë˜ ê·¸ë¦¼ì—ì„œ ë³´ë“¯ì´ xë¥¼ ë§Œë“¤ë•Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë…ë¦½ì ì¸ ìš”ì¸ì¸ z1, z2, .... ê°€ ìˆë‹¤ê³  ì¹˜ì. ê·¸ë˜ì„œ xê°€ ì£¼ì–´ì¡Œì„ë•Œ ê·¸ì˜ í‘œí˜„ì¸ r(x)ë¥¼ ë§Œë“¤ë•Œ (â† r(x)ë§Œë“¤ë•Œ zë“¤ì´ ì˜í–¥ì„ ë¼ì¹˜ëŠ”ê±´ë°) í•œ ìš”ì†Œê°€ ë°”ë€Œë©´ ë”± í•œ representation vectorë§Œ ë°”ë€ŒëŠ” ê±¸ ì˜ë¯¸í•œë‹¤.

ë‹¤ë¥¸ representation vectorê¹Œì§€ ë°”ê¿”ë²„ë¦¬ë©´, zê°€ entagledë˜ì–´ ìˆë‹¤ëŠ” ê±¸ ì˜ë¯¸í•œë‹¤.

![disentanglement](/assets/images/ml-research-2.png)

### Disentagled Representation

ì–´ë–¤ data(ì´ë¯¸ì§€)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” latent variableì´ ì—¬ëŸ¬ ê°œë¡œ ë¶„ë¦¬ ë˜ì–´ ê°ê° ë‹¤ë¥¸ ë°ì´í„°(ì´ë¯¸ì§€)ì˜ íŠ¹ì„±ì— ê´€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” ê²ƒì„ ì˜ë¯¸

### latent variable

that describes original data

ì ì¬ ë³€ìˆ˜. Factor Analysis ë¥¼ í•  ë•Œ ë§ì€ ìˆ˜ì˜ ë³€ìˆ˜ë“¤ì„ ì†Œìˆ˜ì˜ ëª‡ ê°œì˜ ì ì¬ëœ ë³€ìˆ˜ë¡œ ì°¾ì•„ë‚´ëŠ”ë°, ê·¸ ë³€ìˆ˜ê°€ latent variable.

### Autoencoder

for (Nonlinear) Dimensionality Reduction, Feature extraction, Representation learning

![autoencoder](/assets/images/ml-research-3.png)

### VAE (variational auto encoder)

ìƒì„±ëª¨ë¸ì„ ë‹¤ë£¨ëŠ” ê¸°ìˆ  ì¤‘ í•˜ë‚˜. GANê³¼ ê°™ì´ ê°ê´‘ë°›ê³  ìˆìŒ.

![vae](/assets/images/ml-research-4.png)

### downstream task

ì‹¤ì œë¡œ, êµ¬ì²´ì ìœ¼ë¡œ í’€ê³  ì‹¶ì€ ë¬¸ì œ(task)

ìš”ìƒˆëŠ” ëŒ€ë¶€ë¶„ pretrain í•˜ê³  ê·¸ ëª¨ë¸ì— ìš°ë¦¬ê°€ í’€ê³  ì‹¶ì–´í•˜ëŠ” downstream task ë¥¼ ë„£ì–´ì„œ fine tuningì„ í•œë‹¤.

------

# Abstract

unsupervised learning of disentagled representationì˜ key idea :  real-worldì˜ ë°ì´í„°ê°€ ì†Œìˆ˜ì˜ explanatory factor(unsupervised learning ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë§Œë“¤ì–´ì§€ëŠ”)ë¡œ ìƒì„±ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ.

ì—¬ê¸°ì„œ ë„˜ì–´ê°€ê¸° ì „, priorê³¼ posteriorì˜ ê°œë…ì— ëŒ€í•´ ì§šê³  ë„˜ì–´ê°€ì.

- **prior** : z
- **posterior(=likelihood)** : P(x|z)



**prior**ëŠ” ì–´ë–¤ ì¤‘ìš”í•œ factor(autoencoder ì…ì¥ì—ì„œ ë³´ë©´ latent variable), **posterior**ëŠ” prior(=z)ë“¤ì´ ì£¼ì–´ì¡Œì„ë•Œ, xë¥¼ zì˜ ê²°í•©ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ë‹¤.

in this paper, ì´ ë¶„ì•¼ì˜ ìµœê·¼ ì§„í–‰ ìƒí™©ì„ ì‚´í´ë³´ê³ , ì¼ë°˜ì ì¸ ê°€ì •ì— challengeí•˜ê² ë‹¤.

# Introduction

In representation learning, real-world observations xëŠ” ì´ë ‡ê²Œ 2-stepìœ¼ë¡œ ìƒì„±ëœë‹¤ê³  ìƒê°í•œë‹¤.

1. multivariate latent random variable zê°€ P(z) ë¶„í¬ë¡œë¶€í„° ìƒ˜í”Œëœë‹¤.
2. P(x|z)ë¡œë¶€í„° xê°€ ìƒ˜í”Œëœë‹¤.

ê·¸ë‹ˆê¹Œ, real-worldì—ì„œëŠ” ìš°ë¦¬ê°€ ëª¨ë¥´ëŠ” ìš”ì¸ì¸ zê°€ ì´ë¯¸ ìˆëŠ”ë°, ê·¸ zë¡œë¶€í„° x(ì‹¤ì„¸ê³„ì— ì¡´ì¬í•˜ëŠ” ê³ ì°¨ì›ì˜ ë°ì´í„°)ë¥¼ generateí•  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•˜ëŠ” ê²ƒì´ë‹¤.

â‡’ ì¦‰, ê³ ì°¨ì›ì˜ ë°ì´í„° xê°€ ì €ì°¨ì›ì˜ ë°ì´í„° z(semantically meaningful latent variable)ë¡œ í‘œí˜„ì´ ëœë‹¤.

ê·¸ëŸ¬ë©´, ì—¬ê¸°ì„œ ëª©í‘œì™€ ì™œë¥¼ í•œë²ˆ ë” ì§šì–´ë³´ì.

- **Representation learningì˜ ëª©í‘œ** : useful transformation r(x)ë¥¼ ì°¾ëŠ” ê²ƒ. ì™œ? Classifierë‚˜ ë‹¤ë¥¸ ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“¤ ë•Œ, ì–´ë–¤ xì— ëŒ€í•´ ìœ ìš©í•œ informationì„ ì˜ ì¶”ì¶œí•˜ê¸° ìœ„í•´ì„œ.
- **ê·¸ë¦¬ê³  disentaglementê°€ ì—¬ê¸°ì„œ ì™œ ì¤‘ìš”í•˜ëƒ?** dataì—ì„œ informativeí•˜ê³  êµ¬ë¶„ë˜ëŠ” factorë¥¼ ë‚˜ëˆ ì•¼ ì¢€ ë” ì˜ ìƒì„±ë  ìˆ˜ ìˆì–´ ë³´ì¸ë‹¤.

### ì´ ë…¼ë¬¸ì—ì„œ ë§í•˜ê³ ì í•˜ëŠ” ì£¼ì¥ë“¤ì˜ summary (ì‚¬ì‹¤ ì´ê²Œ ì „ë¶€ë‹¤...)

- Unsupervised learning of disentangled representationì€ learning approach(ëª¨ë¸,..)ì™€ datasetì—  inductive biasì—†ì´ëŠ” ë¶ˆê°€ëŠ¥í•˜ë‹¤. (ì´ê±¸ ì´ë¡ ì ìœ¼ë¡œ ì¦ëª…í•  ê²ƒì´ë‹¤.)

- ê·¸ë˜ì„œ current approachë“¤ê³¼ inductive biasì— ëŒ€í•´ ì¡°ì‚¬í•˜ê³  ì‹¤í—˜í–ˆë‹¤.

- ìš°ë¦¬ê°€ í•œ disentaglement_lib ë„ ë°°í¬í•˜ê³ , trained modelë„ ì—¬ëŸ¬ê°œ ë°°í¬í–ˆë‹¤.

- ìš°ë¦¬ëŠ” ì´ë ‡ê²Œ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¶„ì„í–ˆë‹¤.

  (i) ëª¨ë“  methodê°€

  (ii) random seedì™€ hyperparameterê°€ model choiceë³´ë‹¤ ë” ì¤‘ìš”í•˜ë‹¤. ì¦‰, reliableí•œ modelì´ ì—†ë‹¤.

  (iii) disentaglementê°€ downstream tasksì— ìœ ìš©í•œì§€ ê²€ì¦í•  ê¸¸ì´ ì—†ë‹¤.

- ê·¸ë˜ì„œ further researchì—ê²ŒëŠ” followingsê°€ ìˆìŒ ì¢‹ê² ë‹¤.

  (i) inductive biasì™€ supervisionì˜ ì—­í• ì´ ëª…í™•í•´ì•¼ í•œë‹¤.: model selectionì´ Key questionìœ¼ë¡œ ë‚¨ê² ì§€.

  (ii) distenglementì˜ ê°œë…ì´ ì‹¤ì§ˆì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì´ë“ì„ ê°€ì ¸ì˜¤ëŠ”ì§€ë¥¼ ì¦ëª…ë˜ì–´ì•¼ í•œë‹¤.

  (iii) ì‹¤í—˜ë“¤ì€ ëª¨ë‘ ì¬ìƒì‚°í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì—ì„œ ì‹œí–‰ë˜ì–´ì•¼í•œë‹¤.

# Impossibility result

ì–´ë–¤ generative modelë„ unsupervised disentanglement learningì´ ê°€ëŠ¥í•œê°€? ë¥¼ ì¦ëª…í•´ë³´ì.

â‡’ ë‹µì€ Noì´ê³  ì•„ë˜ theoremì„ ì´ìš©í•  ê²ƒì´ë‹¤.

![Theorem](/assets/images/ml-research-5.png)

(ì¦ëª…) P(x|z) ê°€ generative modelì´ë¼ê³  í•˜ì. ê·¸ë¦¬ê³  zì— ëŒ€í•´ ì™„ë²½í•˜ê²Œ disentagledëœ representation r(x)ë¥¼ ì°¾ì•„ëƒˆë‹¤ê³  ê°€ì •í•˜ì.

ê·¼ë° Theorem1ì— ì˜í•´ì„œ, equivalentí•œ generative modelì¸ z_hat = f(z) ê°€ ìˆì„ ê²ƒì´ë‹¤.

ì´ z_hatì€ ê·¼ë° z, r(x)ì™€ ì™„ì „íˆ entagledë˜ì–´ ìˆë‹¤.

ì™œëƒí•˜ë©´, Theorem 1ì—ì„œ ë§í–ˆë“¯ì´, fì˜ ë¯¸ë¶„ê°’ì´ ëŒ€ì²´ë¡œ 0ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— (=ì¦‰,  ujê°€ fiì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ëœ» = disentagleë˜ë ¤ë©´ ujëŠ” fiì— ì˜í–¥ì„ ë¯¸ì¹˜ë©´ ì•ˆë¨).

ê·¸ëŸ¬ë‹ˆê¹Œ, zì˜ í•œ ì°¨ì›ì„ ë³€í™”ì‹œí‚¤ë©´ z_hatì˜ ëª¨ë“  ì°¨ì›ì´ ë‹¤ ì˜í–¥ì„ ë°›ì•„ì„œ ë³€í™”í•œë‹¤. (=entangled) ê·¸ë¦¬ê³  p(z) = p(z_hat) ì´ë‹ˆê¹Œ ë‘ generative modelì€ xì— ëŒ€í•´ ê°™ì€ marginal distributionì„ ê°€ì§„ë‹¤.

distenglement methodëŠ” ê´€ì¸¡ê°’ì¸ xì—ë§Œ ì ‘ê·¼í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ë‘ ìƒì„± ëª¨ë¸ ì¤‘ ì–´ë–¤ê²Œ ì œëŒ€ë¡œ disentagledëœ ì•„ì´ì¸ì§€ êµ¬ë³„ í•  ìˆ˜ ê°€ ì—†ë‹¤. (ë‘˜ ì¤‘ í•˜ë‚˜ëŠ” ë¬´ì¡°ê±´ entagledë˜ì–´ ìˆë‹¤.)

â‡’ fê°€ infinite familyì´ë‹ˆê¹Œ ê·¸ ë¬´í•œëŒ€ì¸ entaglement ì‚¬ì´ì—ì„œ ì§„ì§œ disentagledëœ ìƒì„±ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤.

â†’ **ì¦‰, inductive bias ì™€ supervision ì—†ì´ëŠ” unsupervised disentaglement learningì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤.**

# experiment design

ê¸°ë³¸ì ìœ¼ë¡œ VAE ë¥¼ ì‚¬ìš©í•˜ê³ , loss ëŠ” VAE loss + regularizer

Gaussian encoder, Bernoulli decoder, latent dimensionì€ 10ìœ¼ë¡œ ê³ ì •í•´ì„œ êµ¬ì„±

ê·¸ë¦¬ê³  inductive biasê°€ í•„ìš”í•˜ë‹¤ëŠ” ê±¸ ë³´ì—¬ì£¼ê¸° ìœ„í•´, ì—¬ëŸ¬ hyperparmeter setì„ ì¨ì„œ ì‹¤í—˜ì„ í•  ê²ƒì´ë‹¤.

# Key experimental results

### 1) Can current methods enforce a uncorrelated aggregated posterior and representation?

ì§ˆë¬¸ì˜ ëœ»ì€, í˜„ì¬ ë°©ì‹ë“¤ì´ aggregated posteriorì™€ representationì´ ê°ê° uncorrelate í•˜ê²Œ í•´ì£¼ëƒ?

ë³´í†µì˜ methodì—ì„œ, Gaussian Encoderì—ì„œ ë‚˜ì˜¨ valueë¥¼ sampleí•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼ mean vectorë¥¼ representationìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.

mean vectorë¡œ í•™ìŠµ ì§„í–‰í–ˆì„ ë•Œì™€, ê·¸ ë¶„í¬ì—ì„œ sampleì„ ì–»ì–´ ì§„í–‰í–ˆì„ë•Œì˜ ê²°ê³¼ë¥¼ ì‚´í´ë³´ë©´ ì•„ë˜ í‘œì™€ ê°™ë‹¤. ì—¬ê¸°ì„œ valueëŠ” total correlationì´ë‹¤.

ì™¼ìª½(sample)ì€ regularization strengthê°€ ê°•í•´ì§ˆìˆ˜ë¡(=ì¼ë°˜í™”ì˜ ì¤‘ìš”ë„ê°€ ê°•í•´ì§ˆìˆ˜ë¡) valueê°€ ê°ì†Œí•˜ê³  ì˜¤ë¥¸ìª½(mean)ì€ ë°˜ëŒ€ë‹¤.

ğŸ¥ : ì—¬ê¸°ì„œ regularization strength â†” value ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì–´ë–»ê²Œ í•´ì„í•˜ëŠ” ê±°ì§€? ì¼ë°˜í™”ê°€ ì˜ ë˜ë©´, ë‹¹ì—°íˆ correlation scoreë„ ë‚®ì•„ì§€ëŠ” ê²Œ ì¼ë°˜ì ì´ë¼ê³  ë³´ëŠ”ê±´ê°€?

â‡’ ì•„! ê·¸ê²Œ ì•„ë‹ˆë¼, regularzation strengthëŠ” hyper parameter ì´ë‹ˆê¹Œ hyperparam choiceì— ë”°ë¼ correlation scoreê°€ ë‹¬ë¼ì§„ë‹¤ê³  ë³´ëŠ”ê²Œ ë§ë‹¤.

![ì‹¤í—˜ê²°ê³¼1](/assets/images/ml-research-6.png)

- **Implications**

í˜„ ë°©ì‹ë“¤ì€ aggregated posteriorê°€ uncorrelateí•œ ë°©í–¥ìœ¼ë¡œ ì‹œí–‰ë˜ëŠ”ê±´ ë§ì§€ë§Œ, ê·¸ë ‡ë‹¤ê³  í•´ì„œ mean representationì˜ dimensionì´ uncorrelateí•˜ë‹¤ëŠ” ëœ»ì€ ì•„ë‹ˆë‹¤.

### 2) How much do the disentanglement metrics agree?

disentanglementì˜ í™•ì‹¤í•œ ì •ì˜ê°€ ì—†ê¸° ë•Œë¬¸ì— disentaglementì˜ metric scoreê°€ ì–¼ë§ˆë‚˜ í†µì¼ëœ ê²°ê³¼ë¥¼ ë³´ì´ëŠ”ê°€ë¥¼ ì‹¤í—˜í•´ë´¤ë‹¤.

ì•„ë˜ figureëŠ” Noisy-dSprites datasetì— ëŒ€í•œ ê° metricì˜ correlationì„ matrixë¡œ ë³´ì¸ê²ƒì´ë‹¤.

Modularityì •ë„ ë¹¼ë†“ê³ ëŠ” metricë“¤ì´ ì„œë¡œ ì—„ì²­ correlateë˜ì–´ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.

ğŸ¥ : ê·¸ë‹ˆê¹Œ correlateë˜ì–´ ìˆë‹¤ëŠ” ê±°ëŠ”, ëŒ€ì¶© metric scoreê°€ í†µì¼ëœ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ê³  ìˆë‹¤ëŠ” ê±°ê² ì§€? Modularityê°™ì€ ì•  ë¹¼ë†“ê³¤ disentanglement ê°€ í™•ì‹¤íˆ ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠëŠ”ë°ë„ ë¶ˆêµ¬í•˜ê³  (= ê·¸ëŸ¬ë‹ˆê¹Œ ë‹¹ì—°íˆ ê·¸ê±¸ ì¸¡ì •í•  metricë„ í™•ì‹¤í•˜ê²Œ ìˆëŠ”ê²Œ ì•„ë‹Œë°) ëŒ€ë¶€ë¶„ ë¹„ìŠ·í•œ ê²°ê³¼ë¥¼ ë³´ì´ë”ë¼.

![ì‹¤í—˜ê²°ê³¼2](/assets/images/ml-research-7.png)

- **Implications**

Modularityë¥¼ ì œì™¸í•œ ëª¨ë“  disentaglement metricsê°€ datasetì— ë”°ë¼ ì •ë„ëŠ” ë‹¤ë¥´ì§€ë§Œ, ì„œë¡œ correlateë˜ì–´ ìˆë‹¤.

### 3) How important are different models and hyperparameters for disentaglement?

ìš°ë¦¬ê°€ ì‹¤í—˜í•œ ì´ëŸ° methodë“¤ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” Disentanglementë¥¼ ì˜í•˜ê¸° ìœ„í•´ í•˜ëŠ” ê±°ë‹ˆê¹Œ, disentanglement ì„±ëŠ¥ì´ model choice, hyperparameter selection, randomness(=random seed)ì— ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ë§ì´ ë°›ë‚˜ë¥¼ ì‹¤í—˜í•´ë´¤ë‹¤.

![ì‹¤í—˜ê²°ê³¼3](/assets/images/ml-research-8.png)

ì™¼ìª½ í‘œë¥¼ ë³´ë©´, Modelì— ë”°ë¥¸(hyperparameter, ì—¬ê¸°ì„  regularization strength)ì„ ë‹¬ë¦¬í•´ì„œ ì‹¤í—˜í•œ) score í‘œì¸ë°, ê°™ì€ ëª¨ë¸ì´ì–´ë„ ì—„ì²­ ë¶„í¬ê°€ 0.95 ìƒìœ„ë¶€í„° 0.60ì •ë„ì˜ í•˜ìœ„ê¹Œì§€ ê±¸ì³ì ¸ ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ. varianceê°€ í¬ë‹¤! hyperparmeterì— ì—„ì²­ ì˜í–¥ì„ ë§ì´ ë°›ëŠ” ë‹¤ëŠ” ê±¸ ì•Œ ìˆ˜ ìˆë‹¤.

ì˜¤ë¥¸ìª½ í‘œëŠ” regularization strengthë¥¼ ë‹¤ë¥´ê²Œ í•´ì„œ í•œ modelì— ëŒ€í•´ random seedë¥¼ ë‹¤ë¥´ê²Œ í•˜ì—¬ ì‹¤í—˜í•œ ê²°ê³¼. ì™¼ìª½ë³´ë‹¤ëŠ” ë‚«ì§€ë§Œ random seedì—ë„ scoreê°€ ì˜í–¥ì„ ë°›ëŠ” ë‹¤ëŠ” ê±¸ ì•Œ ìˆ˜ ìˆë‹¤.

scoreë¡œ ë”°ì¡Œì„ë•Œ, good run with a bad hyperparmeter > bad run with a good hyperparmeter

- **Implication**

unsupervised learningì˜ disentanglement scoreì€ randomness(in the form of random seed)ì™€ hyperparmeter choice(in the form of the regularization strength)ì— ì˜í–¥ì„ ë§‰ëŒ€í•˜ê²Œ ë°›ëŠ”ë‹¤.

### 4) Are there reliable recipes for model selection?

ì•ì„œ hyperparameterì™€ model choiceê°€ ì—„ì²­ ì¤‘ìš”í•œ ê²ƒì„ ì•Œì•˜ìœ¼ë‹ˆ, ê·¸ëŸ¼ ì–´ë–»ê²Œ ì–´ë–»ê²Œ ì¢‹ì€ hyperparameterë¥¼ ê³ ë¥´ë‚˜? ì´ ì—°êµ¬ì—ì„œëŠ” (1) good learning modelê³¼ (2) regularization strength corresponding to that loss function ì´ ë‘ê°€ì§€ë¥¼ ì§‘ì¤‘í•´ì„œ ì‚´í´ë³¸ë‹¤.

- **General recipes for hyperparameter selection**

![MLë„í‘œ](/assets/images/ml-research-9.png)

ì´ í‘œë¥¼ Regularization strengthê°€ ë‹¬ë¼ì§ˆ ë•Œë§ˆë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” model ì´ ê³„ì† ë°”ë€ŒëŠ” ê±¸ ì•Œ ìˆ˜ ìˆë‹¤. (...) ì¼ê´€ë˜ê²Œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” modelì´ ì—†ê³ , regularization strengthë¥¼ ê³ ë¥¼ë•Œë„ ì„±ëŠ¥ì„ ìµœëŒ€ë¡œ ëŒì–´ì˜¬ë¦¬ëŠ” ê·¸ëŸ° ì „ëµì´ ì—†ë‹¤.

- **Model selection based on unsupervised scores**

ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•´ë³´ì. Hyperparmeterë¥¼ unsupervised score(ex, reconstruction error, KL divergence between the prior and the approximate posterior, ë“±ë“±) ë¥¼ ë³´ê³  ê³ ë¥´ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¬ë ¤ë©´ ìµœì¢… ëª©í‘œëŠ” disentanglement scoreë¥¼ ë†’ì´ëŠ” ê²ƒì´ë‹ˆ disentanglement metricì´ë‘ unsupervised scoreë‘ correlateí•˜ê³  ìˆìŒ unsupervised scoreê¸°ë°˜ìœ¼ë¡œ ê²°ì •ì„ ë‚´ë ¤ë„ ë˜ê³˜ì§€? í•˜ì§€ë§Œ í‘œë¥¼ ë³´ë©´, correlationì´ ê±°ì˜ ì—†ì–´ì„œ unsupervised scoreë¥¼ ì“°ê¸°ì— ì ì ˆì¹˜ ì•Šë‹¤ëŠ” ê²°ë¡ ì„ ë‚´ë ¸ë‹¤.

![MLë„í‘œ](/assets/images/ml-research-10.png)

- **Hyperparameter selection based on transfer**

ğŸ¥ : ì—¬ê¸°ê¹Œì§€ ì˜¤ë‹¤ë‹ˆ...

ì•ì˜ ë‘ ë°©ë²• ë‹¤ ì ì ˆì¹˜ ì•Šì•„ì„œ, ë§ˆì§€ë§‰ìœ¼ë¡œ transferring good settings across data sets ì´ ì „ëµì„ ì·¨í•´ë´¤ë‹¤. í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ì¢‹ì€ hyperparameter settingì€ labelì´ ìˆëŠ” datasetì—ì„œ ì¶”ë¡ ë˜ì–´ ìƒˆë¡œìš´ datasetì— ì ìš©ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

ìš°ë¦° ì´ transfer based approach to hyperparameter selection vs random model selection ë¹„êµë¥¼ ì´ë ‡ê²Œ í–ˆë‹¤.

1. 50ê°œ random seedì¤‘ í•œ sampleê³¼ random disentanglement metric, random datasetì„ ë½‘ëŠ”ë‹¤. ê·¸ë¦¬ê³  ìš”ê²ƒë“¤ë¡œ ì„±ëŠ¥ì„ ìµœëŒ€ë¡œ ëŒì–´ì˜¬ë¦¬ëŠ” hyperparameter settingì„ ì°¾ëŠ”ë‹¤.
2. randomly selected modelê³¼ ìš°ë¦¬ê°€ 1)ì—ì„œ ê³ ë¥¸ hyperparameter settingì„ ë¹„êµí•œë‹¤.
3. transfer strategyê°€ random model selectionë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¼ ë•Œë¥¼ 10000 trial ì¤‘ ëª‡ í¼ì„¼íŠ¸ì¸ì§€ í™•ì¸í•  ê²ƒì´ë‹¤.

**ê²°ê³¼** â‡’ same metric, same dataset (+ different random seed)ë¥¼ ê³ ë¥´ë©´, 80.7% ë‚˜ì˜¨ë‹¤. same metricì„ ì „ì²´ datasetì— transferí•œë‹¤ê³  í•˜ë©´ 59.9%, metricê³¼ dataset ì „ì²´ë¥¼ transferí•˜ë©´ 54.9%ë¡œ ë–¨ì–´ì§„ë‹¤.

- **Implications**

Unsupervised model selectionì€ unsolved problemì´ë‹¤. good hyperparameterë¥¼ transferí•˜ëŠ” ë°©ë²•ì€ ë³„ë¡œ ì¢‹ì€ ë°©ë²•ì´ ì•„ë‹Œ ê²ƒ ê°™ë‹¤. ì™œëƒí•˜ë©´ good or bad random seedë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ì—†ê¸° ë•Œë¬¸ì´ë‹¤.

### 5) Are these disentagled representations useful for downstream tasks in terms of the sample complexity of learning?

ê·¸ëŸ¬ë©´ ì§„ì§œë¡œ disentangled representationì´ downstream taskì— ìœ ìš©í•˜ëƒ?

- **Implications**

ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” interpretability ë‚˜ fairness ì˜ë¯¸ë¥¼ ê°€ì§„ usefulnessë¥¼ í™•ì¸í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì—, ì´ê±¸ ê³ ë ¤í•˜ë©´ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ì¶”ë¡ í•  ìˆ˜ë„ ìˆë‹¤.

í•˜ì§€ë§Œ, ì—¬ëŸ¬ different datasetì— disentangeld representation score ì™€ downstream task performance ì˜ correlationì„ ë”°ì§€ë©´ ê±°ì˜ ê´€ê³„ê°€ ì—†ëŠ” ê±¸ë¡œ ë‚˜ì˜¨ë‹¤. ê·¸ë‹ˆê¹Œ, disentanglementë•Œë¬¸ì— downstream taskì— ì¢‹ì€ ì„±ëŠ¥ì´ ë‚˜ì˜¨ë‹¤ê³  ë§í•  ìˆ˜ ì—†ë‹¤.

# Conclusion

ì´ ë…¼ë¬¸ì—ì„  ì´ë¡ ì ìœ¼ë¡œ unsupervised learning of disentanged representationì´ inductive bias ì—†ì´ëŠ” ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ê±¸ ì¦ëª…í–ˆë‹¤.

ê·¸ë¦¬ê³ , ëŒ€ê·œëª¨ ì‹¤í—˜ (6 state-of-the-art disentanglement methodsì™€ metricë¡œ)ì„ ì§„í–‰í•˜ë©° ì´ëŸ° ê²°ë¡ ì„ ë‚´ë ¸ë‹¤.

1. posteriorë¥¼ factorizingí•˜ëŠ” ê²ƒì´ ê¼­ representationì˜ ì°¨ì›ì´ uncorrelatedë˜ì–´ìˆë‹¤ëŠ”ê±¸ ì•”ì‹œí•˜ì§€ ì•ŠëŠ”ë‹¤.
2. Random seedì™€ hyperparmeterê°€ model selectionë³´ë‹¤ ì˜í–¥ì„ ë§ì´ ë¯¸ì¹˜ê³ , tuningí• ë• supervisionì´ í•„ìš”í•˜ë‹¤.
3. disentanglementê°€ ì˜ ëœë‹¤ê³  í•´ì„œ downstream taskì˜ í•™ìŠµì´ ì˜ëœë‹¤ëŠ” ê±¸ ì˜ë¯¸í•˜ì§€ ì•ŠëŠ”ë‹¤.

ê·¸ë˜ì„œ ì•ìœ¼ë¡œ ìˆì„ ì—°êµ¬ì—ëŠ” ë‹¤ìŒì˜ ê²ƒë“¤ì„ ê¸°ëŒ€í•œë‹¤.

(i) inductive biasì™€ supervisionì˜ ì—­í• ì´ ëª…í™•í•´ì•¼ í•œë‹¤.: model selectionì´ Key questionìœ¼ë¡œ ë‚¨ê² ì§€.

(ii) distenglementì˜ ê°œë…ì´ ì‹¤ì§ˆì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì´ë“ì„ ê°€ì ¸ì˜¤ëŠ”ì§€ë¥¼ ì¦ëª…ë˜ì–´ì•¼ í•œë‹¤.

(iii) ì‹¤í—˜ë“¤ì€ ëª¨ë‘ ì¬ìƒì‚°í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì—ì„œ ì‹œí–‰ë˜ì–´ì•¼í•œë‹¤.

# reference

[ë°°ê²½ ì§€ì‹]ì—ì„œ ì‚¬ìš©ëœ ì´ë¯¸ì§€ë¥¼ ì œì™¸í•œ ëª¨ë“  ì´ë¯¸ì§€ëŠ”https://arxiv.org/pdf/1811.12359.pdf ì—ì„œ ê°€ì ¸ì™”ë‹¤.



[ë°°ê²½ ì§€ì‹]ì˜ ì²«ë²ˆì§¸ ì´ë¯¸ì§€(ë„í‘œ)ë¥¼ ì œì™¸í•˜ê³  ë‚´ê°€ ê·¸ë¦° ê·¸ë¦¼ì´ë‹¤.