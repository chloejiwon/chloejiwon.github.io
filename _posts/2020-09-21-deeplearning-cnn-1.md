---
title: "🐈 Deep learning - CNN (1) "
excerpt: "딥러닝 CNN 기초 개념 뽀개기"

categories:
  - deeplearning
tags:
  - cnn
last_modified_at: 2020-09-21T01:11:00-05:00
---



> 딥 러닝 공부의 기록. CNN 기초를 다져 보자. (언제 RNN, transformer 등.... 하지? 막막🤧) 대부분의 내용은 <밑바닥부터 시작하는 딥 러닝> 책을 바탕으로 하며 거의 요약본이라 할 수 있습니다.

# 서론

이 글에서는 딥 러닝의 기본적인 개념, Logistic Regression, 활성화 함수, 가중치 업데이트 등을 알고 있다고 가정하고 진행하도록 하겠다.

CNN(Convolutional Neural Network)는 이미지 인식에 특별히 아주 좋은 성능을 보이는 딥 러닝의 네트워크 중 하나이다. 일단 개념을 정리하는 정도로 이 글을 마무리 짓겠다.(python, pytorch, tensorflow등 기타 framework는 다음 글에서 다루는 걸로)

# 합성곱 신경망(CNN)

## 전체 구조

처음에 배웠던 구조는 Fully Connected Neural Net 구조였다. 모든 뉴런이 인접하는 계층의 모든 뉴런과 결합되어 있었다. (책에서는 완전히 연결된 계층을 Affine 계층이라 부른다) 그래서 그 전에서 배운 구조는 Affine 계층 + 활성화 함수 계층이 짝을 이뤄 레고 블록처럼 쌓이는 구조다. 그럼 CNN은 어떻게 다른가? Convolution Layer와 Pooling Layer가 우리가 알고 있는 구조에 새롭게 등장한다.

![cnn-1](/assets/images/cnn-1-1.png)



그래서 한 계층은 Conv - Relu - (Pooling)의 흐름으로 연결된다.

## Convolution Layer

Affine 계층로 딥러닝 구조를 구축하면 데이터의 어떤 성질은 무시될 수 밖에 없다. 그게 뭐냐면, **데이터의 형상!**

입력 데이터가 3차원 데이터, 이미지라고 가정하면, Affine계층에 입력하고자 하면 3차원 데이터를 평평한 1차원 데이터로 flatten시켜줘야 한다. 특히 이미지 데이터에는 각각이 공간적 정보를 담고 있는데, 이게 다 무시되는 것이다. 예를 들어 가까운 픽셀은 값이 비슷하거나, 거리가 먼 픽셀은 연관관계가 없거나 등. 데이터에 있는 본질적인 패턴이 있을 것이다. 근데 이걸 다 무시하니, 형상에 담긴 고유한 특성을 다 버리게 되는 것.

그러면 이 특성을 살리는 딥 러닝 구조를 만들 수 있을까? 이때 등장하는 것이 Convolution Layer. 합성곱(Convolution)이 뭐하냐면, 이런 걸 하는 거다.

![cnn-2](/assets/images/cnn-1-2.png)

맨 왼쪽에 있는 입력 데이터에 필터를 적용해서 출력 Feature map을 만들어 낸다. 데이터의 특성 측면에서 살펴보면 입력 데이터도 세로*가로 방향의 형상, 필터 역시 세로*가로, 그리고 출력 데이터 또한 가로*세로의 형상을 가진다. 뭔가 공간적 정보가 유지되는 느낌이 들지 않는 가? 🌀(최면 걸기)

그리고 저 곱해 지는 저 필터를 우리는 **kernel** 이라 부르기도 한다. 합성 곱 연산은 필터를 stride라 하는 간격을 두고 이동해 가며 입력 데이터에 적용한다. (행렬 곱 하는 거다)

그럼 우리가 이미 알고 있는 weight와 bias가 뭐지? weight는 바로 필터의 매개 변수다! 저것들을 back propagation을 통해 update하면 되는 거다. 그리고 bias도 있다. (있는 건 다 있는..) bias는 필터를 적용한 후 데이터에 더해진다. 그리고 **bias는 필터 당 하나**만 존재한다.

## 패딩(padding)

👀 프로그래머라면 패딩 많이 들어보지 않았습니까!? CNN에서의 padding은 직관적으로 알듯이, 없는데 옆에 채워주는 거다. (어휘력이..) 그림 보면 알겠지.

입력 데이터 주위에 0을 채운다.

![cnn-3](/assets/images/cnn-1-3.png)

padding 1을 주면, (4,4)였던 입력 데이터가 (6,6)이 된다.

> ⚠️근데, 패딩 왜 필요해? 패딩은 주로 출력 크기를 조정할 목적으로 사용된다. 예를 들어서 (4,4) 입력 데이터에 (3,3) 필터를 적용하면 출력은 (2,2)가 되어서 입력보다 2만큼 줄어든다. 합성 곱을 여러번 되풀이하는 Deep Neural Net에서는 문제가 될 수 있다. 합성 곱 한번 거칠때마다 크기가 이렇게 줄어들면, 어느 시점에선 출력 크기가 1이 되어 버리고 연산을 적용할 수 없겠지? 그래서 padding을 사용한다. 위 그림을 보면 입력 데이터 (4,4)가 출력 데이터 (4,4)가 되어 공간적 크기가 고정되어 다음 계층에 전달될 수 있다.

## 스트라이드(Stride)

필터 적용하는 위치 간격을 stride라고 한다. 1이면 한 칸씩 옮겨가며 필터를 적용하고, 2면 두 칸씩 옮겨가며 필터 적용.

## 3차원 데이터의 합성 곱 연산

3차원 데이터에 자주 쓰인다는 CNN. 그럼 3차원 데이터일 땐 어떻게 합성 곱 연산을 하느냐? 입력 데이터가 (4,4,3) = (4,4)*3 이면 필터도 (3,3)*3 가 있어 각 차원 마다 합성 곱을 하면 된다. 그 결과를 모두 더해 출력 데이터가 형성 된다. 출력 데이터는 (2,2)*1 이다.

## Pooling Layer

풀링은 세로 * 가로 방향의 공간을 줄이는 연산이다. 하나로 집약해서 공간 크기를 줄여 버린다. 특히 많이 쓰이는 것은 Max Pooling. Max pooling은 최대값을 구하는 연산으로, stride가 2라면 2x2 크기의 윈도우에서 가장 큰 값만 취해서 출력 데이터를 형성한다.

풀링 레이어의 특징은 다음과 같다.

- 학습해야 할 매개변수가 없다 🥺
- 채널 수가 변하지 않는다 (입력된 고대로~ 출력도)
- 입력의 변화에 영향을 적게 받는다

# 마무리

일단 가장 기초적인 CNN 구조에서 사용 되는 개념을 알아보았으니 다음 글에선 python이나 pytorch로 구현해보도록 한다. 그리고 내가 책에서 가장 시간 투자를 많이 한.. 정말 정말 모르 겠던 개념도 같이. 🤧(하나 이해 안돼서 절대 다음 장으로 넘어 가질 못했다.)